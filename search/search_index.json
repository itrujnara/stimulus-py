{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"STIMULUS","text":""},{"location":"#stochastic-testing-with-input-modification-for-unbiased-learning-systems","title":"Stochastic Testing with Input Modification for Unbiased Learning Systems.","text":"<p>Warning</p> <p> This package is in active development and breaking changes may occur. The API is not yet stable and features might be added, modified, or removed without notice. Use in production environments is not recommended at this stage.</p> <p>We encourage you to:</p> <ul> <li> <p>\ud83d\udcdd Report bugs and issues on our GitHub Issues page</p> </li> <li> <p>\ud83d\udca1 Suggest features and improvements through GitHub Discussions</p> </li> <li> <p>\ud83e\udd1d Contribute by submitting pull requests</p> </li> </ul> <p>We are actively working towards release 1.0.0 (see milestone), check the slack channel by clicking on the badge above where we are actively discussing. Build with us every wednesday at 14:00 CET until 18:00 CET on the nf-core gathertown (see slack for calendar updates i.e. some weeks open dev hours are not possible)</p>"},{"location":"#introduction","title":"Introduction","text":"<p>Most (if not all) quality software is thouroughly tested. Deep neural networks seem to have escaped this paradigm. </p> <p>In the age of large-scale deep learning, it is critical that early-stage dl models (prototypes) are tested to ensure costly bugs do not happen at scale.</p> <p>Here, we attempt at solving the testing problem by proposing an extensive library to test deep neural networks beyond test-set performance. </p> <p>Stimulus provides those functionalities</p> <ol> <li> <p>Data Perturbation Testing:    Modify training data to test model's robustness to perturbations and uncover which pre-processing steps increase performance</p> </li> <li> <p>Hyperparameter Optimization:    Perform tuning on model architecture with user-defined search spaces using Ray[tune] to ensure comparable performance across data transformations</p> </li> <li> <p>Comprehensive Analysis:    Generate all-against-all model report to guide data pre-processing decisions</p> </li> </ol> <p>For large scale experiments, we recommend our nf-core deepmodeloptim pipeline which is still under development and will be released alongside stimulus v1.0.0.</p> <p>\ud83d\udcf9 Stimulus was featured at the nextflow summit 2024 in Barcelona, which is a nice intoduction to current package capabilities, you can watch the talk here</p> <p>Stimulus aims at providing those functionalities in a near future, stay tuned for updates!</p> <ol> <li> <p>Model Architecture Testing:    Run routine checks on model architecture and training process including type-checking, model execution, and weight updates</p> </li> <li> <p>Post-Training Validation:    Perform comprehensive model validation including overfitting detection and out-of-distribution performance testing</p> </li> <li> <p>Informed Hyperparameter Tuning:    Encourage tuning strategies that follow Google's Deep Learning Tuning Playbook <sup>1</sup></p> </li> <li> <p>Scaling Analysis:    Generate scaling law reports to understand prototype model behavior at different scales</p> </li> </ol>"},{"location":"#user-guide","title":"User guide","text":""},{"location":"#repository-organization","title":"Repository organization","text":"<p>Stimulus is organized as follows, we will reference to this structure in the following sections</p> <pre><code>src/stimulus/ \ud83e\uddea\n\u251c\u2500\u2500 analysis/ \ud83d\udcca\n\u2502   \u2514\u2500\u2500 analysis_default.py\n\u251c\u2500\u2500 cli/ \ud83d\udda5\ufe0f\n\u2502   \u251c\u2500\u2500 analysis_default.py\n\u2502   \u251c\u2500\u2500 check_model.py\n\u2502   \u251c\u2500\u2500 interpret_json.py\n\u2502   \u251c\u2500\u2500 predict.py\n\u2502   \u251c\u2500\u2500 shuffle_csv.py\n\u2502   \u251c\u2500\u2500 split_csv.py\n\u2502   \u251c\u2500\u2500 split_yaml.py\n\u2502   \u251c\u2500\u2500 transform_csv.py\n\u2502   \u2514\u2500\u2500 tuning.py\n\u251c\u2500\u2500 data/ \ud83d\udcc1\n\u2502   \u251c\u2500\u2500 csv.py\n\u2502   \u251c\u2500\u2500 experiments.py\n\u2502   \u251c\u2500\u2500 handlertorch.py\n\u2502   \u251c\u2500\u2500 encoding/ \ud83d\udd10\n\u2502   \u2502   \u2514\u2500\u2500 encoders.py\n\u2502   \u251c\u2500\u2500 splitters/ \u2702\ufe0f\n\u2502   \u2502   \u2514\u2500\u2500 splitters.py\n\u2502   \u2514\u2500\u2500 transform/ \ud83d\udd04\n\u2502       \u2514\u2500\u2500 data_transformation_generators.py\n\u251c\u2500\u2500 learner/ \ud83e\udde0\n\u2502   \u251c\u2500\u2500 predict.py\n\u2502   \u251c\u2500\u2500 raytune_learner.py\n\u2502   \u2514\u2500\u2500 raytune_parser.py\n\u2514\u2500\u2500 utils/ \ud83d\udee0\ufe0f\n    \u251c\u2500\u2500 json_schema.py\n    \u251c\u2500\u2500 launch_utils.py\n    \u251c\u2500\u2500 performance.py\n    \u2514\u2500\u2500 yaml_model_schema.py\n</code></pre>"},{"location":"#data-encoding","title":"Data encoding","text":"<p>Data in stimulus can take many forms (files, text, images, networks...) in order to support this diversity, stimulus relies on the encoding module. List of available encoders can be found here.</p> <p>If the provided encoders do not support the type of data you are working with, you can write your own encoder by inheriting from the <code>AbstractEncoder</code> class and implementing the <code>encode</code>, <code>decode</code> and <code>encode_all</code> methods. </p> <ul> <li><code>encode</code> is currently optional, can return a <code>NotImplementedError</code> if the encoder does not support encoding a single data point</li> <li><code>decode</code> is currently optional, can return a <code>NotImplementedError</code> if the encoder does not support decoding</li> <li><code>encode_all</code> is called by other stimulus functions, and is expected to return a <code>np.array</code> . </li> </ul>"},{"location":"#expected-data-format","title":"Expected data format","text":"<p>Data is expected to be presented in a csv samplesheet file with the following format: </p> input1:input:input_type input2:input:input_type meta1:meta:meta_type label1:label:label_type label2:label:label_type sample1 input1 sample1 input2 sample1 meta1 sample1 label1 sample1 label2 sample2 input1 sample2 input2 sample2 meta1 sample2 label1 sample2 label2 sample3 input1 sample3 input2 sample3 meta1 sample3 label1 sample3 label2 <p>Columns are expected to follow this name convention : <code>name:type:data_type</code></p> <ul> <li> <p>name corresponds to the column name, this should be the same as input names in model batch definition (see model section for more details)</p> </li> <li> <p>type is either input, meta or label, typically models predict the labels from the input, and meta is used to perform downstream analysis</p> </li> <li> <p>data_type is the column data type.</p> </li> </ul> <p>Note</p> <p> This rigid data format is expected to change once we move to release v1.0.0, data types and information will be defined in a yaml config and only column names will be required in the data, see this github issue</p>"},{"location":"#connecting-encoders-and-datasets","title":"Connecting encoders and datasets","text":"<p>Once we have our data formated and our encoders ready, we need to explicitly state which encoder is used for which data type. This is done through an experiment class. </p> <p>To understand how experiment classes are used to connect data types and encoders, let's have a look at a minimal DnaToFloat example : </p> <pre><code>class DnaToFloat(AbstractExperiment):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.dna = {\n            \"encoder\": encoders.TextOneHotEncoder(alphabet=\"acgt\"),\n        }\n        self.float = {\n            \"encoder\": encoders.FloatEncoder(),\n        }\n</code></pre> <p>Here we define the <code>data_type</code> for the dna and float types, note that those <code>data_type</code> are the same as the ones defined in the samplesheet dataset above, for example, a dataset on which this experiment would run could look like this: </p> mouse_dna:input:dna mouse_rnaseq:label:float ACTAGGCATGCTAGTCG 0.53 ACTGGGGCTAGTCGAA 0.23 GATGTTCTGATGCT 0.98 <p>Note how the <code>data_type</code> for the mouse_dna and mouse_rnaseq columns match exactly the attribute names defined in the <code>DnaToFloat</code> minimal class above. </p> <p>stimulus-py ships with a few basic experiment classes, if you need to write your own experiment class, simply inherit from the base <code>AbstractExperiment</code> class and overwrite the class <code>__init__</code> method like shown above.</p> <p>Note</p> <p> This has the drawback of requiring a build of the experiment class each time a new task is defined (for instance, let's say we want to use dna and protein sequences to predict rna).</p> <p>Once we move to release v1.0.0, <code>type</code> (i.e. input, meta, label) and <code>data_type</code> will be defined in the data yaml config, and the relevant experiment class will be automatically built.</p>"},{"location":"#loading-the-data","title":"Loading the data","text":"<p>Finally, once we have defined our encoders, the experiment class and the samplesheet, stimulus will transparently load the data using the csv.py module</p> <p>csv.py contains two important classes, <code>CsvLoader</code> and <code>CsvProcessing</code></p> <p><code>CsvLoader</code> is responsible for na\u00efvely loading the data (without changing anything), it works by performing a couple of checks on the dataset to ensure it is correctly formated, and then uses the experiment class in conjunction with the column names to call the proper encoders and output inputs, labels, and meta dictionary objects. </p> <p><code>CsvLoader</code> is used by the <code>handlertorch</code> module to load data into pytorch tensors. </p> <p>Tip</p> <p> So, to recap, when you load a dataset into a torch tensor, </p> <ol> <li> <p><code>handlertorch</code> will call <code>CsvLoader</code> with the csv samplesheet and the experiment class</p> </li> <li> <p><code>CsvLoader</code> will use the experiment class to fetch the proper encoder <code>encode_all</code> method for each data column</p> </li> <li> <p><code>CsvLoader</code> will use the <code>encode_all</code> method to encode the data and output dictionary objects for inputs, labels and meta data</p> </li> <li> <p><code>handlertorch</code> will convert the contents to torch tensors</p> </li> <li> <p><code>handlertorch</code> will feed the <code>input</code> torch tensor to the model, use the <code>label</code> torch tensor for loss computation and will store the <code>meta</code> tensor for downstream analysis</p> </li> </ol> <p>Great, now you know how stimulus transparently loads your data into your pytorch model! While this seems complicated, the only thing you really have to do, is to format your data correctly in a csv samplesheet and define your experiment class with the proper encoders (either by using the provided encoders or by writing your own).</p>"},{"location":"#data-transformation","title":"Data transformation","text":"<p>Measuring the impact of data transformations (noising, down/upsampling, augmentation...) on models at training time is a major feature of stimulus.</p> <p>Data transformations materialize as <code>DataTransformer</code> classes, and should inherit from the <code>AbstractDataTransformer</code> class (see docs)</p> <p>Note</p> <p> Writing your own <code>DataTransformer</code> class is the same as writing your own <code>Encoder</code> class, you should overwrite the <code>transform</code> and <code>transform_all</code> methods</p> <p>Warning</p> <p> Every <code>DataTransformer</code> class has to have <code>seed</code> in <code>transform</code> and <code>transform_all</code> methods parameters, and <code>np.random.seed(seed)</code> should be called in those methods.</p> <p>Warning</p> <p> Every <code>DataTransformer</code> class should have an <code>add_row</code> argument set to either <code>True</code> or <code>False</code> depending on if it is augmenting the data (adding rows) or not.</p>"},{"location":"#connecting-transformations-and-dataset","title":"Connecting transformations and dataset","text":"<p>Just like encoders, data transformations are defined in the <code>Experiment</code> class alongside encoders. Let's upgrade our <code>DnaToFloat</code> minimal class defined above to reflect this.</p> <pre><code>class DnaToFloat(AbstractExperiment):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.dna = {\n            \"encoder\": encoders.TextOneHotEncoder(alphabet=\"acgt\"),\n            \"data_transformation_generators\": {\n                \"UniformTextMasker\": data_transformation_generators.UniformTextMasker(mask=\"N\"),\n                \"ReverseComplement\": data_transformation_generators.ReverseComplement(),\n                \"GaussianChunk\": data_transformation_generators.GaussianChunk(),\n            },\n        }\n        self.float = {\n            \"encoder\": encoders.FloatEncoder(),\n            \"data_transformation_generators\": {\"GaussianNoise\": data_transformation_generators.GaussianNoise()},\n        }\n</code></pre> <p>As you can see, our <code>data_type</code> arguments get an other field, <code>\"data_transformation_generators\"</code>, there we can initialize the <code>DataTransformer</code> classes with their relevant parameters. </p> <p>In the <code>csv</code> module, the <code>CsvProcessing</code> class will call the <code>transform_all</code> methods from the classes contained in <code>\"data_transformation_generators\"</code> based on the column type and a list of transformations. </p> <p>i.e., if we give the <code>[\"ReverseComplement\",\"GaussianChunk\"]</code> list to the <code>CsvProcessing</code> class <code>transform</code> method the data contained in the <code>mouse_dna:input:dna</code> column in our minimal example above will be first reverse complemented and then chunked. </p> <p>Tip</p> <p> Recap : To transform your dataset,</p> <ul> <li> <p>define your own <code>DataTransformer</code> class or use one we provide</p> </li> <li> <p>add it to your experiment class</p> </li> <li> <p>load your data through <code>CsvProcessing</code> </p> </li> <li> <p>set a list of transforms</p> </li> <li> <p>call <code>CsvProcessing.transform(transform_list)</code></p> </li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>stimulus is still under development, you can install it from test-pypi by running the following command:</p> <pre><code>pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple stimulus-py==0.0.10\n</code></pre>"},{"location":"#citations","title":"citations","text":"<ol> <li> <p>Godbole, V., Dahl, G. E., Gilmer, J., Shallue, C. J., &amp; Nado, Z. (2023). Deep Learning Tuning Playbook (Version 1.0) [Computer software]. http://github.com/google-research/tuning_playbook \u21a9</p> </li> </ol>"},{"location":"changelog/","title":"Changelog","text":"<p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#v001","title":"v0.0.1","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>pyproject.toml</li> <li>init.py files in most folders</li> <li>this changelog</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>moved launchers from </li> <li>directory structure to follow PEP8 principle for packaging</li> <li>safetensors in requirements</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>important path issues related to project structure change </li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li>all nextflow specific code c226ef</li> </ul>"},{"location":"code_of_conduct/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"code_of_conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"code_of_conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall   community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of   any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address,   without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"code_of_conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"code_of_conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"code_of_conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at mathysgrapotte@gmail.com. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"code_of_conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"code_of_conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"code_of_conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"code_of_conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"code_of_conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"code_of_conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p>"},{"location":"contributing/#environment-setup","title":"Environment setup","text":"<p>Nothing easier!</p> <p>Fork and clone the repository, then:</p> <pre><code>cd stimulus-py\nmake setup\n</code></pre> <p>Note</p> <p> If it fails for some reason, you'll need to install uv manually.</p> <p>You can install it with:</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>Now you can try running <code>make setup</code> again, or simply <code>uv sync</code>.</p> <p>You now have the dependencies installed.</p> <p>Run <code>make help</code> to see all the available actions!</p>"},{"location":"contributing/#tasks","title":"Tasks","text":"<p>The entry-point to run commands and tasks is the <code>make</code> Python script, located in the <code>scripts</code> directory. Try running <code>make</code> to show the available commands and tasks. The commands do not need the Python dependencies to be installed, while the tasks do. The cross-platform tasks are written in Python, thanks to duty.</p> <p>If you work in VSCode, we provide an action to configure VSCode for the project.</p>"},{"location":"contributing/#development","title":"Development","text":"<p>As usual:</p> <ol> <li>create a new branch: <code>git switch -c feature-or-bugfix-name</code></li> <li>edit the code and/or the documentation</li> </ol> <p>Before committing:</p> <ol> <li>run <code>make format</code> to auto-format the code</li> <li>run <code>make check</code> to check everything (fix any warning)</li> <li>run <code>make test</code> to run the tests (fix any issue)</li> <li>if you updated the documentation or the project dependencies:<ol> <li>run <code>make docs</code></li> <li>go to http://localhost:8000 and check that everything looks good</li> </ol> </li> </ol> <p>Then you can pull request and we will review. Make sure you join our slack hosted on nf-core to talk and build with us!</p>"},{"location":"credits/","title":"Credits","text":""},{"location":"credits/#exec-1--credits","title":"Credits","text":"<p>These projects were used to build stimulus-py. Thank you!</p> <p>Python | uv | copier-uv</p>"},{"location":"credits/#exec-1--runtime-dependencies","title":"Runtime dependencies","text":"Project Summary Version (accepted) Version (last resolved) License aiohappyeyeballs Happy Eyeballs for asyncio <code>&gt;=2.3.0</code> <code>2.4.4</code> PSF-2.0 aiohttp Async http client/server framework (asyncio) <code>&gt;=3.7</code> <code>3.11.11</code> Apache-2.0 aiohttp-cors CORS support for aiohttp <code>0.7.0</code> Apache License, Version 2.0 aiosignal aiosignal: a list of registered asynchronous callbacks <code>1.3.2</code> Apache 2.0 annotated-types Reusable constraint types to use with typing.Annotated <code>&gt;=0.6.0</code> <code>0.7.0</code> MIT License attrs Classes Without Boilerplate <code>&gt;=22.2.0</code> <code>25.1.0</code> MIT cachetools Extensible memoizing collections and decorators <code>&gt;=2.0.0, &lt;6.0</code> <code>5.5.1</code> MIT certifi Python package for providing Mozilla's CA Bundle. <code>&gt;=2017.4.17</code> <code>2024.12.14</code> MPL-2.0 charset-normalizer The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet. <code>&gt;=2, &lt;4</code> <code>3.4.1</code> MIT click Composable command line interface toolkit <code>&gt;=7.0</code> <code>8.1.8</code> BSD License colorama Cross-platform colored terminal text. <code>&gt;=0.4</code> <code>0.4.6</code> BSD License colorful Terminal string styling done right, in Python. <code>0.5.6</code> MIT License contourpy Python library for calculating contours of 2D quadrilateral grids <code>&gt;=1.0.1</code> <code>1.3.1</code> BSD License cycler Composable style cycles <code>&gt;=0.10</code> <code>0.12.1</code> BSD License dill serialize all of Python <code>&gt;=0.3.9</code> <code>0.3.9</code> BSD-3-Clause distlib Distribution utilities <code>&gt;=0.3.7, &lt;1</code> <code>0.3.9</code> PSF-2.0 filelock A platform independent file lock. <code>3.17.0</code> Unlicense fonttools Tools to manipulate font files <code>&gt;=4.22.0</code> <code>4.55.8</code> MIT frozenlist A list-like structure which implements collections.abc.MutableSequence <code>1.5.0</code> Apache 2 fsspec File-system specification <code>2024.12.0</code> BSD License google-api-core Google API client core library <code>&gt;=1.0.0, &lt;2.0.0</code> <code>2.24.1</code> Apache 2.0 google-auth Google Authentication Library <code>&gt;=2.14.1, &lt;3.0.dev0</code> <code>2.38.0</code> Apache 2.0 googleapis-common-protos Common protobufs used in Google APIs <code>&gt;=1.56.2, &lt;2.0.dev0</code> <code>1.66.0</code> Apache-2.0 grpcio HTTP/2-based RPC framework <code>&gt;=1.32.0</code> <code>1.70.0</code> Apache License 2.0 idna Internationalized Domain Names in Applications (IDNA) <code>&gt;=2.5, &lt;4</code> <code>3.10</code> BSD License importlib_metadata Read metadata from Python packages <code>&gt;=4.4</code> <code>8.6.1</code> Apache Software License iniconfig brain-dead simple config-ini parsing <code>2.0.0</code> MIT Jinja2 A very fast and expressive template engine. <code>&gt;=2.11.1</code> <code>3.1.5</code> BSD License joblib Lightweight pipelining with Python functions <code>&gt;=1.2.0</code> <code>1.4.2</code> BSD 3-Clause jsonschema An implementation of JSON Schema validation for Python <code>4.23.0</code> MIT jsonschema-specifications The JSON Schema meta-schemas and vocabularies, exposed as a Registry <code>&gt;=2023.03.6</code> <code>2024.10.1</code> MIT License kiwisolver A fast implementation of the Cassowary constraint solver <code>&gt;=1.3.1</code> <code>1.4.8</code> BSD License MarkupSafe Safely add untrusted strings to HTML/XML markup. <code>&gt;=2.0.1, &gt;=2.0</code> <code>3.0.2</code> BSD License matplotlib Python plotting package <code>&gt;=3.9.0</code> <code>3.10.0</code> Python Software Foundation License mpmath Python library for arbitrary-precision floating-point arithmetic <code>&gt;=1.1.0, &lt;1.4</code> <code>1.3.0</code> BSD msgpack MessagePack serializer <code>&gt;=1.0.0, &lt;2.0.0</code> <code>1.1.0</code> Apache 2.0 multidict multidict implementation <code>&gt;=4.5, &lt;7.0</code> <code>6.1.0</code> Apache 2 multiprocess better multiprocessing and multithreading in Python <code>==0.70.17</code> <code>0.70.17</code> BSD-3-Clause networkx Python package for creating and manipulating graphs and networks <code>3.4.2</code> BSD License numpy Fundamental package for array computing in Python <code>&gt;=1.26.0, &lt;2.0.0</code> <code>1.26.4</code> BSD License nvidia-cublas-cu12 CUBLAS native runtime libraries <code>==12.1.3.1</code> <code>12.1.3.1</code> NVIDIA Proprietary Software nvidia-cuda-cupti-cu12 CUDA profiling tools runtime libs. <code>==12.1.105</code> <code>12.1.105</code> NVIDIA Proprietary Software nvidia-cuda-nvrtc-cu12 NVRTC native runtime libraries <code>==12.1.105</code> <code>12.1.105</code> NVIDIA Proprietary Software nvidia-cuda-runtime-cu12 CUDA Runtime native Libraries <code>==12.1.105</code> <code>12.1.105</code> NVIDIA Proprietary Software nvidia-cudnn-cu12 cuDNN runtime libraries <code>==8.9.2.26</code> <code>8.9.2.26</code> NVIDIA Proprietary Software nvidia-cufft-cu12 CUFFT native runtime libraries <code>==11.0.2.54</code> <code>11.0.2.54</code> NVIDIA Proprietary Software nvidia-curand-cu12 CURAND native runtime libraries <code>==10.3.2.106</code> <code>10.3.2.106</code> NVIDIA Proprietary Software nvidia-cusolver-cu12 CUDA solver native runtime libraries <code>==11.4.5.107</code> <code>11.4.5.107</code> NVIDIA Proprietary Software nvidia-cusparse-cu12 CUSPARSE native runtime libraries <code>==12.1.0.106</code> <code>12.1.0.106</code> NVIDIA Proprietary Software nvidia-nccl-cu12 NVIDIA Collective Communication Library (NCCL) Runtime <code>==2.19.3</code> <code>2.19.3</code> NVIDIA Proprietary Software nvidia-nvjitlink-cu12 Nvidia JIT LTO Library <code>12.8.61</code> NVIDIA Proprietary Software nvidia-nvtx-cu12 NVIDIA Tools Extension <code>==12.1.105</code> <code>12.1.105</code> NVIDIA Proprietary Software opencensus A stats collection and distributed tracing framework <code>0.11.4</code> Apache-2.0 opencensus-context OpenCensus Runtime Context <code>&gt;=0.1.3</code> <code>0.1.3</code> Apache-2.0 packaging Core utilities for Python packages <code>&gt;=20.5</code> <code>24.2</code> Apache Software License + BSD License pandas Powerful data structures for data analysis, time series, and statistics <code>&gt;=2.2.0</code> <code>2.2.3</code> BSD License pillow Python Imaging Library (Fork) <code>&gt;=8</code> <code>11.1.0</code> MIT-CMU platformdirs A small Python package for determining appropriate platform-specific dirs, e.g. a <code>user data dir</code>. <code>&gt;=3.9.1, &gt;=2.2.0, &lt;5</code> <code>4.3.6</code> MIT pluggy plugin and hook calling mechanisms for python <code>&gt;=1.5, &lt;2</code> <code>1.5.0</code> MIT polars-lts-cpu Blazingly fast DataFrame library <code>&gt;=0.20.30, &lt;1.12.0</code> <code>1.11.0</code> MIT License prometheus_client Python client for the Prometheus monitoring system. <code>&gt;=0.7.1</code> <code>0.21.1</code> Apache Software License 2.0 propcache Accelerated property cache <code>&gt;=0.2.0</code> <code>0.2.1</code> Apache-2.0 proto-plus Beautiful, Pythonic protocol buffers <code>&gt;=1.22.3, &lt;2.0.0dev</code> <code>1.26.0</code> Apache 2.0 protobuf <code>&gt;=3.15.3, !=3.19.5</code> <code>5.29.3</code> 3-Clause BSD License py-spy Sampling profiler for Python programs <code>&gt;=0.2.0</code> <code>0.4.0</code> MIT pyarrow Python library for Apache Arrow <code>&gt;=9.0.0</code> <code>17.0.0</code> Apache Software License pyasn1 Pure-Python implementation of ASN.1 types and DER/BER/CER codecs (X.208) <code>&gt;=0.1.3</code> <code>0.6.1</code> BSD-2-Clause pyasn1_modules A collection of ASN.1-based protocols modules <code>&gt;=0.2.1</code> <code>0.4.1</code> BSD pydantic Data validation using Python type hints <code>&gt;=2.0.0</code> <code>2.10.6</code> MIT pydantic_core Core functionality for Pydantic validation and serialization <code>==2.27.2</code> <code>2.27.2</code> MIT pyparsing pyparsing module - Classes and methods to define and execute parsing grammars <code>&gt;=2.3.1</code> <code>3.2.1</code> MIT License pytest pytest: simple powerful testing with Python <code>&gt;=8.2, &gt;=7.0.0, &lt;9.0.0</code> <code>8.3.4</code> MIT python-dateutil Extensions to the standard Python datetime module <code>&gt;=2.8.2, &gt;=2.8.1</code> <code>2.9.0.post0</code> BSD License + Apache Software License pytz World timezone definitions, modern and historical <code>&gt;=2020.1</code> <code>2024.2</code> MIT PyYAML YAML parser and emitter for Python <code>&gt;=5.1</code> <code>6.0.2</code> MIT ray Ray provides a simple, universal API for building distributed applications. <code>&gt;=2.38.0</code> <code>2.41.0</code> Apache 2.0 referencing JSON Referencing + Python <code>&gt;=0.28.4</code> <code>0.36.2</code> MIT requests Python HTTP for Humans. <code>~=2.26</code> <code>2.32.3</code> Apache-2.0 rpds-py Python bindings to Rust's persistent data structures (rpds) <code>&gt;=0.7.1</code> <code>0.22.3</code> MIT License rsa Pure-Python RSA implementation <code>&gt;=3.1.4, &lt;5</code> <code>4.9</code> Apache-2.0 safetensors <code>&gt;=0.4.5</code> <code>0.5.2</code> Apache Software License scikit-learn A set of python modules for machine learning and data mining <code>&gt;=1.5.0</code> <code>1.6.1</code> BSD License scipy Fundamental algorithms for scientific computing in Python <code>==1.14.1</code> <code>1.14.1</code> BSD License six Python 2 and 3 compatibility utilities <code>&gt;=1.5</code> <code>1.17.0</code> MIT smart-open Utils for streaming large files (S3, HDFS, GCS, Azure Blob Storage, gzip, bz2...) <code>7.1.0</code> MIT sympy Computer algebra system (CAS) in Python <code>1.13.3</code> BSD syrupy Pytest Snapshot Test Utility <code>&gt;=4.8.0</code> <code>4.8.1</code> Apache-2.0 tensorboardX TensorBoardX lets you watch Tensors Flow without Tensorflow <code>&gt;=1.9</code> <code>2.6.2.2</code> MIT license threadpoolctl threadpoolctl <code>&gt;=3.1.0</code> <code>3.5.0</code> BSD-3-Clause torch Tensors and Dynamic neural networks in Python with strong GPU acceleration <code>==2.2.2</code> <code>2.2.2</code> BSD-3 triton A language and compiler for custom Deep Learning operations <code>==2.2.0</code> <code>2.2.0</code> MIT License typing_extensions Backported and Experimental Type Hints for Python 3.8+ <code>&gt;=4.6.0, &gt;=4.12.2</code> <code>4.12.2</code> Python Software Foundation License tzdata Provider of IANA time zone data <code>&gt;=2022.7</code> <code>2025.1</code> Apache-2.0 urllib3 HTTP library with thread-safe connection pooling, file post, and more. <code>&gt;=1.21.1, &lt;3</code> <code>2.3.0</code> MIT License virtualenv Virtual Python Environment builder <code>&gt;=20.0.24, !=20.21.1</code> <code>20.29.1</code> MIT wrapt Module for decorators, wrappers and monkey patching. <code>1.17.2</code> BSD yarl Yet another URL library <code>&gt;=1.17.0, &lt;2.0</code> <code>1.18.3</code> Apache-2.0 zipp Backport of pathlib-compatible object wrapper for zip files <code>&gt;=3.20</code> <code>3.21.0</code> MIT License"},{"location":"credits/#exec-1--development-dependencies","title":"Development dependencies","text":"Project Summary Version (accepted) Version (last resolved) License ansimarkup Produce colored terminal text with an xml-like markup <code>~=1.4</code> <code>1.5.0</code> Revised BSD License appdirs A small Python module for determining appropriate platform-specific dirs, e.g. a \"user data dir\". <code>&gt;=1.4</code> <code>1.4.4</code> MIT babel Internationalization utilities <code>~=2.10</code> <code>2.16.0</code> BSD-3-Clause backports.tarfile Backport of CPython tarfile module <code>1.2.0</code> MIT License black The uncompromising code formatter. <code>&gt;=24.4</code> <code>25.1.0</code> MIT build A simple, correct Python build frontend <code>&gt;=1.2</code> <code>1.2.2.post1</code> MIT License certifi Python package for providing Mozilla's CA Bundle. <code>&gt;=2017.4.17</code> <code>2024.12.14</code> MPL-2.0 cffi Foreign Function Interface for Python calling C code. <code>&gt;=1.12</code> <code>1.17.1</code> MIT charset-normalizer The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet. <code>&gt;=2, &lt;4</code> <code>3.4.1</code> MIT click Composable command line interface toolkit <code>&gt;=7.0</code> <code>8.1.8</code> BSD License colorama Cross-platform colored terminal text. <code>&gt;=0.4</code> <code>0.4.6</code> BSD License coverage Code coverage measurement for Python <code>&gt;=7.5</code> <code>7.6.10</code> Apache-2.0 cryptography cryptography is a package which provides cryptographic recipes and primitives to Python developers. <code>&gt;=2.0</code> <code>44.0.0</code> Apache-2.0 OR BSD-3-Clause csscompressor A python port of YUI CSS Compressor <code>&gt;=0.9.5</code> <code>0.9.5</code> BSD docutils Docutils -- Python Documentation Utilities <code>&gt;=0.21.2</code> <code>0.21.2</code> Public Domain + Python Software Foundation License + BSD License + GNU General Public License (GPL) duty A simple task runner. <code>&gt;=1.4</code> <code>1.4.3</code> ISC editables Editable installations <code>&gt;=0.5</code> <code>0.5</code> MIT License execnet execnet: rapid multi-Python deployment <code>&gt;=2.1</code> <code>2.1.1</code> MIT failprint Run a command, print its output only if it fails. <code>&gt;=0.11, !=1.0.0</code> <code>1.0.3</code> ISC ghp-import Copy your docs directly to the gh-pages branch. <code>&gt;=1.0</code> <code>2.1.0</code> Apache Software License git-changelog Automatic Changelog generator using Jinja2 templates. <code>&gt;=2.5</code> <code>2.5.3</code> ISC gitdb Git Object Database <code>&gt;=4.0.1, &lt;5</code> <code>4.0.12</code> BSD License GitPython GitPython is a Python library used to interact with Git repositories <code>3.1.44</code> BSD-3-Clause griffe Signatures for entire Python programs. Extract the structure, the frame, the skeleton of your project, to generate API documentation or find breaking changes in your API. <code>&gt;=0.49</code> <code>1.5.6</code> ISC htmlmin2 An HTML Minifier <code>&gt;=0.1.13</code> <code>0.1.13</code> BSD id A tool for generating OIDC identities <code>1.5.0</code> Apache Software License idna Internationalized Domain Names in Applications (IDNA) <code>&gt;=2.5, &lt;4</code> <code>3.10</code> BSD License importlib_metadata Read metadata from Python packages <code>&gt;=4.4</code> <code>8.6.1</code> Apache Software License iniconfig brain-dead simple config-ini parsing <code>2.0.0</code> MIT jaraco.classes Utility functions for Python class constructs <code>3.4.0</code> MIT License jaraco.context Useful decorators and context managers <code>6.0.1</code> MIT License jaraco.functools Functools like those found in stdlib <code>4.1.0</code> MIT License jeepney Low-level, pure Python DBus protocol wrapper. <code>&gt;=0.4.2</code> <code>0.8.0</code> MIT License Jinja2 A very fast and expressive template engine. <code>&gt;=2.11.1</code> <code>3.1.5</code> BSD License jsmin JavaScript minifier. <code>&gt;=3.0.1</code> <code>3.0.1</code> MIT License keyring Store and access your passwords safely. <code>&gt;=15.1</code> <code>25.6.0</code> MIT License Markdown Python implementation of John Gruber's Markdown. <code>&gt;=3.3.6</code> <code>3.7</code> BSD License markdown-callouts Markdown extension: a classier syntax for admonitions <code>&gt;=0.4</code> <code>0.4.0</code> MIT markdown-exec Utilities to execute code blocks in Markdown files. <code>&gt;=1.8</code> <code>1.10.0</code> ISC markdown-it-py Python port of markdown-it. Markdown parsing, done right! <code>&gt;=2.2.0</code> <code>3.0.0</code> MIT License MarkupSafe Safely add untrusted strings to HTML/XML markup. <code>&gt;=2.0.1, &gt;=2.0</code> <code>3.0.2</code> BSD License mdurl Markdown URL utilities <code>~=0.1</code> <code>0.1.2</code> MIT License mergedeep A deep merge function for \ud83d\udc0d. <code>&gt;=1.3.4</code> <code>1.3.4</code> MIT License mkdocs Project documentation with Markdown. <code>&gt;=1.6</code> <code>1.6.1</code> BSD-2-Clause mkdocs-autorefs Automatically link across pages in MkDocs. <code>&gt;=1.2</code> <code>1.3.0</code> ISC mkdocs-coverage MkDocs plugin to integrate your coverage HTML report into your site. <code>&gt;=1.0</code> <code>1.1.0</code> ISC mkdocs-gen-files MkDocs plugin to programmatically generate documentation pages during the build <code>&gt;=0.5</code> <code>0.5.0</code> MIT mkdocs-get-deps MkDocs extension that lists all dependencies according to a mkdocs.yml file <code>&gt;=0.2.0</code> <code>0.2.0</code> MIT mkdocs-git-revision-date-localized-plugin Mkdocs plugin that enables displaying the localized date of the last git modification of a markdown file. <code>&gt;=1.2</code> <code>1.3.0</code> MIT mkdocs-literate-nav MkDocs plugin to specify the navigation in Markdown instead of YAML <code>&gt;=0.6</code> <code>0.6.1</code> MIT mkdocs-material Documentation that simply works <code>&gt;=9.5</code> <code>9.5.50</code> MIT mkdocs-material-extensions Extension pack for Python Markdown and MkDocs Material. <code>~=1.3</code> <code>1.3.1</code> MIT mkdocs-minify-plugin An MkDocs plugin to minify HTML, JS or CSS files prior to being written to disk <code>&gt;=0.8</code> <code>0.8.0</code> MIT mkdocstrings Automatic documentation from sources, for MkDocs. <code>&gt;=0.25</code> <code>0.27.0</code> ISC mkdocstrings-python A Python handler for mkdocstrings. <code>&gt;=0.5.2</code> <code>1.13.0</code> ISC more-itertools More routines for operating on iterables, beyond itertools <code>10.6.0</code> MIT License mypy Optional static typing for Python <code>&gt;=1.10</code> <code>1.14.1</code> MIT mypy-extensions Type system extensions for programs checked with the mypy type checker. <code>&gt;=1.0.0</code> <code>1.0.0</code> MIT License nh3 Python binding to Ammonia HTML sanitizer Rust crate <code>&gt;=0.2.14</code> <code>0.2.20</code> MIT packaging Core utilities for Python packages <code>&gt;=20.5</code> <code>24.2</code> Apache Software License + BSD License paginate Divides large result sets into pages for easier browsing <code>~=0.5</code> <code>0.5.7</code> MIT pathspec Utility library for gitignore style pattern matching of file paths. <code>&gt;=0.11.1</code> <code>0.12.1</code> Mozilla Public License 2.0 (MPL 2.0) platformdirs A small Python package for determining appropriate platform-specific dirs, e.g. a <code>user data dir</code>. <code>&gt;=3.9.1, &gt;=2.2.0, &lt;5</code> <code>4.3.6</code> MIT pluggy plugin and hook calling mechanisms for python <code>&gt;=1.5, &lt;2</code> <code>1.5.0</code> MIT ptyprocess Run a subprocess in a pseudo terminal <code>~=0.6</code> <code>0.7.0</code> ISC License (ISCL) pycparser C parser in Python <code>2.22</code> BSD-3-Clause Pygments Pygments is a syntax highlighting package written in Python. <code>~=2.16</code> <code>2.19.1</code> BSD-2-Clause pymdown-extensions Extension pack for Python Markdown. <code>~=10.2</code> <code>10.14.2</code> MIT pyproject_hooks Wrappers to call pyproject.toml-based build backend hooks. <code>1.2.0</code> MIT License pytest pytest: simple powerful testing with Python <code>&gt;=8.2, &gt;=7.0.0, &lt;9.0.0</code> <code>8.3.4</code> MIT pytest-cov Pytest plugin for measuring coverage. <code>&gt;=5.0</code> <code>6.0.0</code> MIT pytest-randomly Pytest plugin to randomly order tests and control random.seed. <code>&gt;=3.15</code> <code>3.16.0</code> MIT License pytest-xdist pytest xdist plugin for distributed testing, most importantly across multiple CPUs <code>&gt;=3.6</code> <code>3.6.1</code> MIT License python-dateutil Extensions to the standard Python datetime module <code>&gt;=2.8.2, &gt;=2.8.1</code> <code>2.9.0.post0</code> BSD License + Apache Software License pytz World timezone definitions, modern and historical <code>&gt;=2020.1</code> <code>2024.2</code> MIT PyYAML YAML parser and emitter for Python <code>&gt;=5.1</code> <code>6.0.2</code> MIT pyyaml_env_tag A custom YAML tag for referencing environment variables in YAML files. <code>&gt;=0.1</code> <code>0.1</code> MIT License readme_renderer readme_renderer is a library for rendering readme descriptions for Warehouse <code>&gt;=35.0</code> <code>44.0</code> Apache License, Version 2.0 regex Alternative regular expression module, to replace re. <code>&gt;=2022.4</code> <code>2024.11.6</code> Apache Software License requests Python HTTP for Humans. <code>~=2.26</code> <code>2.32.3</code> Apache-2.0 requests-toolbelt A utility belt for advanced users of python-requests <code>&gt;=0.8.0, !=0.9.0</code> <code>1.0.0</code> Apache 2.0 rfc3986 Validating URI References per RFC 3986 <code>&gt;=1.4.0</code> <code>2.0.0</code> Apache 2.0 rich Render rich text, tables, progress bars, syntax highlighting, markdown and more to the terminal <code>&gt;=12.0.0</code> <code>13.9.4</code> MIT ruff An extremely fast Python linter and code formatter, written in Rust. <code>&gt;=0.4</code> <code>0.9.3</code> MIT SecretStorage Python bindings to FreeDesktop.org Secret Service API <code>&gt;=3.2</code> <code>3.3.3</code> BSD 3-Clause License semver Python helper for Semantic Versioning (https://semver.org) <code>&gt;=2.13</code> <code>3.0.4</code> BSD License six Python 2 and 3 compatibility utilities <code>&gt;=1.5</code> <code>1.17.0</code> MIT smmap A pure Python implementation of a sliding window memory map manager <code>&gt;=3.0.1, &lt;6</code> <code>5.0.2</code> BSD-3-Clause twine Collection of utilities for publishing packages on PyPI <code>&gt;=5.1</code> <code>6.1.0</code> Apache Software License types-Markdown Typing stubs for Markdown <code>&gt;=3.6</code> <code>3.7.0.20241204</code> Apache-2.0 types-PyYAML Typing stubs for PyYAML <code>&gt;=6.0</code> <code>6.0.12.20241230</code> Apache-2.0 typing_extensions Backported and Experimental Type Hints for Python 3.8+ <code>&gt;=4.6.0, &gt;=4.12.2</code> <code>4.12.2</code> Python Software Foundation License urllib3 HTTP library with thread-safe connection pooling, file post, and more. <code>&gt;=1.21.1, &lt;3</code> <code>2.3.0</code> MIT License watchdog Filesystem events monitoring <code>&gt;=2.0</code> <code>6.0.0</code> Apache-2.0 zipp Backport of pathlib-compatible object wrapper for zip files <code>&gt;=3.20</code> <code>3.21.0</code> MIT License"},{"location":"license/","title":"License","text":"<pre><code>MIT License\n\nCopyright (c) 2024 Mathys Grapotte\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li> stimulus<ul> <li> analysis<ul> <li> analysis_default</li> </ul> </li> <li> cli<ul> <li> analysis_default</li> <li> check_model</li> <li> predict</li> <li> shuffle_csv</li> <li> split_csv</li> <li> split_yaml</li> <li> transform_csv</li> <li> tuning</li> </ul> </li> <li> data<ul> <li> data_handlers</li> <li> encoding<ul> <li> encoders</li> </ul> </li> <li> handlertorch</li> <li> loaders</li> <li> splitters<ul> <li> splitters</li> </ul> </li> <li> transform<ul> <li> data_transformation_generators</li> </ul> </li> </ul> </li> <li> debug</li> <li> learner<ul> <li> predict</li> <li> raytune_learner</li> <li> raytune_parser</li> </ul> </li> <li> typing</li> <li> utils<ul> <li> generic_utils</li> <li> launch_utils</li> <li> performance</li> <li> yaml_data</li> <li> yaml_model_schema</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/stimulus/","title":"stimulus","text":""},{"location":"reference/stimulus/#stimulus","title":"stimulus","text":"<p>stimulus-py package.</p> <p>Modules:</p> <ul> <li> <code>analysis</code>           \u2013            <p>Analysis package for stimulus, analysis_default is to be refactored, see git issues.</p> </li> <li> <code>cli</code>           \u2013            <p>Command line interface package for the stimulus library.</p> </li> <li> <code>data</code>           \u2013            <p>Data handling and processing module.</p> </li> <li> <code>debug</code>           \u2013            <p>Debugging utilities.</p> </li> <li> <code>learner</code>           \u2013            <p>Learner package for model training and evaluation.</p> </li> <li> <code>typing</code>           \u2013            <p>Typing for Stimulus Python API.</p> </li> <li> <code>utils</code>           \u2013            <p>Utility functions package.</p> </li> </ul>"},{"location":"reference/stimulus/debug/","title":"stimulus.debug","text":""},{"location":"reference/stimulus/debug/#stimulus.debug","title":"debug","text":"<p>Debugging utilities.</p> <p>Classes:</p> <ul> <li> <code>Environment</code>           \u2013            <p>Dataclass to store environment information.</p> </li> <li> <code>Package</code>           \u2013            <p>Dataclass describing a Python package.</p> </li> <li> <code>Variable</code>           \u2013            <p>Dataclass describing an environment variable.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>get_debug_info</code>             \u2013              <p>Get debug/environment information.</p> </li> <li> <code>get_version</code>             \u2013              <p>Get version of the given distribution.</p> </li> <li> <code>print_debug_info</code>             \u2013              <p>Print debug/environment information.</p> </li> </ul>"},{"location":"reference/stimulus/debug/#stimulus.debug.Environment","title":"Environment  <code>dataclass</code>","text":"<pre><code>Environment(\n    interpreter_name: str,\n    interpreter_version: str,\n    interpreter_path: str,\n    platform: str,\n    packages: list[Package],\n    variables: list[Variable],\n)\n</code></pre> <p>Dataclass to store environment information.</p> <p>Attributes:</p> <ul> <li> <code>interpreter_name</code>               (<code>str</code>)           \u2013            <p>Python interpreter name.</p> </li> <li> <code>interpreter_path</code>               (<code>str</code>)           \u2013            <p>Path to Python executable.</p> </li> <li> <code>interpreter_version</code>               (<code>str</code>)           \u2013            <p>Python interpreter version.</p> </li> <li> <code>packages</code>               (<code>list[Package]</code>)           \u2013            <p>Installed packages.</p> </li> <li> <code>platform</code>               (<code>str</code>)           \u2013            <p>Operating System.</p> </li> <li> <code>variables</code>               (<code>list[Variable]</code>)           \u2013            <p>Environment variables.</p> </li> </ul>"},{"location":"reference/stimulus/debug/#stimulus.debug.Environment.interpreter_name","title":"interpreter_name  <code>instance-attribute</code>","text":"<pre><code>interpreter_name: str\n</code></pre> <p>Python interpreter name.</p>"},{"location":"reference/stimulus/debug/#stimulus.debug.Environment.interpreter_path","title":"interpreter_path  <code>instance-attribute</code>","text":"<pre><code>interpreter_path: str\n</code></pre> <p>Path to Python executable.</p>"},{"location":"reference/stimulus/debug/#stimulus.debug.Environment.interpreter_version","title":"interpreter_version  <code>instance-attribute</code>","text":"<pre><code>interpreter_version: str\n</code></pre> <p>Python interpreter version.</p>"},{"location":"reference/stimulus/debug/#stimulus.debug.Environment.packages","title":"packages  <code>instance-attribute</code>","text":"<pre><code>packages: list[Package]\n</code></pre> <p>Installed packages.</p>"},{"location":"reference/stimulus/debug/#stimulus.debug.Environment.platform","title":"platform  <code>instance-attribute</code>","text":"<pre><code>platform: str\n</code></pre> <p>Operating System.</p>"},{"location":"reference/stimulus/debug/#stimulus.debug.Environment.variables","title":"variables  <code>instance-attribute</code>","text":"<pre><code>variables: list[Variable]\n</code></pre> <p>Environment variables.</p>"},{"location":"reference/stimulus/debug/#stimulus.debug.Package","title":"Package  <code>dataclass</code>","text":"<pre><code>Package(name: str, version: str)\n</code></pre> <p>Dataclass describing a Python package.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Package name.</p> </li> <li> <code>version</code>               (<code>str</code>)           \u2013            <p>Package version.</p> </li> </ul>"},{"location":"reference/stimulus/debug/#stimulus.debug.Package.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>Package name.</p>"},{"location":"reference/stimulus/debug/#stimulus.debug.Package.version","title":"version  <code>instance-attribute</code>","text":"<pre><code>version: str\n</code></pre> <p>Package version.</p>"},{"location":"reference/stimulus/debug/#stimulus.debug.Variable","title":"Variable  <code>dataclass</code>","text":"<pre><code>Variable(name: str, value: str)\n</code></pre> <p>Dataclass describing an environment variable.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Variable name.</p> </li> <li> <code>value</code>               (<code>str</code>)           \u2013            <p>Variable value.</p> </li> </ul>"},{"location":"reference/stimulus/debug/#stimulus.debug.Variable.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>Variable name.</p>"},{"location":"reference/stimulus/debug/#stimulus.debug.Variable.value","title":"value  <code>instance-attribute</code>","text":"<pre><code>value: str\n</code></pre> <p>Variable value.</p>"},{"location":"reference/stimulus/debug/#stimulus.debug.get_debug_info","title":"get_debug_info","text":"<pre><code>get_debug_info() -&gt; Environment\n</code></pre> <p>Get debug/environment information.</p> <p>Returns:</p> <ul> <li> <code>Environment</code>           \u2013            <p>Environment information.</p> </li> </ul> Source code in <code>src/stimulus/debug.py</code> <pre><code>def get_debug_info() -&gt; Environment:\n    \"\"\"Get debug/environment information.\n\n    Returns:\n        Environment information.\n    \"\"\"\n    py_name, py_version = _interpreter_name_version()\n    packages = [\"stimulus-py\"]\n    variables = [\"PYTHONPATH\", *[var for var in os.environ if var.startswith(\"STIMULUS_PY\")]]\n    return Environment(\n        interpreter_name=py_name,\n        interpreter_version=py_version,\n        interpreter_path=sys.executable,\n        platform=platform.platform(),\n        variables=[Variable(var, val) for var in variables if (val := os.getenv(var))],\n        packages=[Package(pkg, get_version(pkg)) for pkg in packages],\n    )\n</code></pre>"},{"location":"reference/stimulus/debug/#stimulus.debug.get_version","title":"get_version","text":"<pre><code>get_version(dist: str = 'stimulus-py') -&gt; str\n</code></pre> <p>Get version of the given distribution.</p> <p>Parameters:</p> <ul> <li> <code>dist</code>               (<code>str</code>, default:                   <code>'stimulus-py'</code> )           \u2013            <p>A distribution name.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>A version number.</p> </li> </ul> Source code in <code>src/stimulus/debug.py</code> <pre><code>def get_version(dist: str = \"stimulus-py\") -&gt; str:\n    \"\"\"Get version of the given distribution.\n\n    Parameters:\n        dist: A distribution name.\n\n    Returns:\n        A version number.\n    \"\"\"\n    try:\n        return metadata.version(dist)\n    except metadata.PackageNotFoundError:\n        return \"0.0.0\"\n</code></pre>"},{"location":"reference/stimulus/debug/#stimulus.debug.print_debug_info","title":"print_debug_info","text":"<pre><code>print_debug_info() -&gt; None\n</code></pre> <p>Print debug/environment information.</p> Source code in <code>src/stimulus/debug.py</code> <pre><code>def print_debug_info() -&gt; None:\n    \"\"\"Print debug/environment information.\"\"\"\n    info = get_debug_info()\n    print(f\"- __System__: {info.platform}\")\n    print(f\"- __Python__: {info.interpreter_name} {info.interpreter_version} ({info.interpreter_path})\")\n    print(\"- __Environment variables__:\")\n    for var in info.variables:\n        print(f\"  - `{var.name}`: `{var.value}`\")\n    print(\"- __Installed packages__:\")\n    for pkg in info.packages:\n        print(f\"  - `{pkg.name}` v{pkg.version}\")\n</code></pre>"},{"location":"reference/stimulus/analysis/","title":"stimulus.analysis","text":""},{"location":"reference/stimulus/analysis/#stimulus.analysis","title":"analysis","text":"<p>Analysis package for stimulus, analysis_default is to be refactored, see git issues.</p> <p>Modules:</p> <ul> <li> <code>analysis_default</code>           \u2013            <p>Default analysis module for stimulus package.</p> </li> </ul>"},{"location":"reference/stimulus/analysis/analysis_default/","title":"stimulus.analysis.analysis_default","text":""},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default","title":"analysis_default","text":"<p>Default analysis module for stimulus package.</p> <p>Classes:</p> <ul> <li> <code>Analysis</code>           \u2013            <p>General functions for analysis and plotting.</p> </li> <li> <code>AnalysisPerformanceTune</code>           \u2013            <p>Report the performance during tuning.</p> </li> <li> <code>AnalysisRobustness</code>           \u2013            <p>Report the robustness of the models.</p> </li> </ul>"},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default.Analysis","title":"Analysis","text":"<p>General functions for analysis and plotting.</p> <p>TODO automatically set up proper figsize depends on the number of subplots, etc</p> <p>Methods:</p> <ul> <li> <code>annotate_heatmap</code>             \u2013              <p>A function to annotate a heatmap.</p> </li> <li> <code>get_grid_shape</code>             \u2013              <p>Calculates rows and columns for a rectangle layout (flexible).</p> </li> <li> <code>heatmap</code>             \u2013              <p>Create a heatmap from a numpy array and two lists of labels.</p> </li> </ul>"},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default.Analysis.annotate_heatmap","title":"annotate_heatmap  <code>staticmethod</code>","text":"<pre><code>annotate_heatmap(\n    im: Any,\n    data: ndarray | None = None,\n    valfmt: Union[str, StrMethodFormatter] = \"{x:.2f}\",\n    textcolors: tuple[str, str] = (\"black\", \"white\"),\n    threshold: float | None = None,\n    **textkw: Any\n) -&gt; list[Any]\n</code></pre> <p>A function to annotate a heatmap.</p>"},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default.Analysis.annotate_heatmap--parameters","title":"Parameters","text":"<p>im     The AxesImage to be labeled. data     Data used to annotate.  If None, the image's data is used.  Optional. valfmt     The format of the annotations inside the heatmap.  This should either     use the string format method, e.g. \"$ {x:.2f}\", or be a     <code>matplotlib.ticker.Formatter</code>.  Optional. textcolors     A pair of colors.  The first is used for values below a threshold,     the second for those above.  Optional. threshold     Value in data units according to which the colors from textcolors are     applied.  If None (the default) uses the middle of the colormap as     separation.  Optional. **kwargs     All other arguments are forwarded to each call to <code>text</code> used to create     the text labels.</p> Source code in <code>src/stimulus/analysis/analysis_default.py</code> <pre><code>@staticmethod\ndef annotate_heatmap(\n    im: Any,\n    data: np.ndarray | None = None,\n    valfmt: Union[str, StrMethodFormatter] = \"{x:.2f}\",\n    textcolors: tuple[str, str] = (\"black\", \"white\"),\n    threshold: float | None = None,\n    **textkw: Any,\n) -&gt; list[Any]:\n    \"\"\"A function to annotate a heatmap.\n\n    Parameters\n    ----------\n    im\n        The AxesImage to be labeled.\n    data\n        Data used to annotate.  If None, the image's data is used.  Optional.\n    valfmt\n        The format of the annotations inside the heatmap.  This should either\n        use the string format method, e.g. \"$ {x:.2f}\", or be a\n        `matplotlib.ticker.Formatter`.  Optional.\n    textcolors\n        A pair of colors.  The first is used for values below a threshold,\n        the second for those above.  Optional.\n    threshold\n        Value in data units according to which the colors from textcolors are\n        applied.  If None (the default) uses the middle of the colormap as\n        separation.  Optional.\n    **kwargs\n        All other arguments are forwarded to each call to `text` used to create\n        the text labels.\n    \"\"\"\n    if not isinstance(data, (list, np.ndarray)):\n        data = im.get_array()\n\n    # Normalize the threshold to the images color range.\n    threshold = im.norm(threshold) if threshold is not None else im.norm(data.max()) / 2.0\n\n    # Set default alignment to center, but allow it to be\n    # overwritten by textkw.\n    kw = {\"horizontalalignment\": \"center\", \"verticalalignment\": \"center\"}\n    kw.update(textkw)\n\n    # Get the formatter in case a string is supplied\n    if isinstance(valfmt, str):\n        valfmt = StrMethodFormatter(valfmt)\n\n    # Loop over the data and create a `Text` for each \"pixel\".\n    # Change the text's color depending on the data.\n    texts = []\n    for i in range(data.shape[0]):\n        for j in range(data.shape[1]):\n            kw.update(color=textcolors[int(im.norm(data[i, j]) &gt; threshold)])\n            text = im.axes.text(j, i, valfmt(data[i, j]), **kw)\n            texts.append(text)\n\n    return texts\n</code></pre>"},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default.Analysis.get_grid_shape","title":"get_grid_shape  <code>staticmethod</code>","text":"<pre><code>get_grid_shape(n: int) -&gt; tuple[int, int]\n</code></pre> <p>Calculates rows and columns for a rectangle layout (flexible).</p> Source code in <code>src/stimulus/analysis/analysis_default.py</code> <pre><code>@staticmethod\ndef get_grid_shape(n: int) -&gt; tuple[int, int]:\n    \"\"\"Calculates rows and columns for a rectangle layout (flexible).\"\"\"\n    rows = int(math.ceil(math.sqrt(n)))  # Round up the square root for rows\n    cols = int(math.ceil(n / rows))  # Calculate columns based on rows\n    return rows, cols\n</code></pre>"},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default.Analysis.heatmap","title":"heatmap  <code>staticmethod</code>","text":"<pre><code>heatmap(\n    data: ndarray,\n    row_labels: list[str],\n    col_labels: list[str],\n    ax: Any | None = None,\n    cbar_kw: dict | None = None,\n    cbarlabel: str = \"\",\n    **kwargs: Any\n) -&gt; tuple[Any, Any]\n</code></pre> <p>Create a heatmap from a numpy array and two lists of labels.</p>"},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default.Analysis.heatmap--parameters","title":"Parameters","text":"<p>data     A 2D numpy array of shape (M, N). row_labels     A list or array of length M with the labels for the rows. col_labels     A list or array of length N with the labels for the columns. ax     A <code>matplotlib.axes.Axes</code> instance to which the heatmap is plotted.  If     not provided, use current axes or create a new one.  Optional. cbar_kw     A dictionary with arguments to <code>matplotlib.Figure.colorbar</code>.  Optional. cbarlabel     The label for the colorbar.  Optional. **kwargs     All other arguments are forwarded to <code>imshow</code>.</p> Source code in <code>src/stimulus/analysis/analysis_default.py</code> <pre><code>@staticmethod\ndef heatmap(\n    data: np.ndarray,\n    row_labels: list[str],\n    col_labels: list[str],\n    ax: Any | None = None,\n    cbar_kw: dict | None = None,\n    cbarlabel: str = \"\",\n    **kwargs: Any,\n) -&gt; tuple[Any, Any]:\n    \"\"\"Create a heatmap from a numpy array and two lists of labels.\n\n    Parameters\n    ----------\n    data\n        A 2D numpy array of shape (M, N).\n    row_labels\n        A list or array of length M with the labels for the rows.\n    col_labels\n        A list or array of length N with the labels for the columns.\n    ax\n        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n        not provided, use current axes or create a new one.  Optional.\n    cbar_kw\n        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n    cbarlabel\n        The label for the colorbar.  Optional.\n    **kwargs\n        All other arguments are forwarded to `imshow`.\n    \"\"\"\n    if ax is None:\n        ax = plt.gca()\n\n    if cbar_kw is None:\n        cbar_kw = {}\n\n    # Plot the heatmap\n    im = ax.imshow(data, **kwargs)\n\n    # Create colorbar\n    if ax.figure is not None and hasattr(ax.figure, \"colorbar\"):\n        cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n        cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n    else:\n        cbar = None\n\n    # Show all ticks and label them with the respective list entries.\n    ax.set_xticks(np.arange(data.shape[1]), labels=col_labels)\n    ax.set_yticks(np.arange(data.shape[0]), labels=row_labels)\n\n    # Let the horizontal axes labeling appear on top.\n    ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\", rotation_mode=\"anchor\")\n\n    # Turn spines off and create white grid.\n    ax.spines[:].set_visible(False)\n\n    ax.set_xticks(np.arange(data.shape[1] + 1) - 0.5, minor=True)\n    ax.set_yticks(np.arange(data.shape[0] + 1) - 0.5, minor=True)\n    ax.grid(which=\"minor\", color=\"w\", linestyle=\"-\", linewidth=3)\n    ax.tick_params(which=\"minor\", bottom=False, left=False)\n\n    return im, cbar\n</code></pre>"},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default.AnalysisPerformanceTune","title":"AnalysisPerformanceTune","text":"<pre><code>AnalysisPerformanceTune(results_path: str)\n</code></pre> <p>               Bases: <code>Analysis</code></p> <p>Report the performance during tuning.</p> <p>TODO maybe instead of reporting one pdf for one model with all metrics, report one pdf for all models with one metric. TODO or maybe one pdf for all models with all metrics, colored by model. One for train, one for val.</p> <p>Methods:</p> <ul> <li> <code>annotate_heatmap</code>             \u2013              <p>A function to annotate a heatmap.</p> </li> <li> <code>get_grid_shape</code>             \u2013              <p>Calculates rows and columns for a rectangle layout (flexible).</p> </li> <li> <code>heatmap</code>             \u2013              <p>Create a heatmap from a numpy array and two lists of labels.</p> </li> <li> <code>plot_metric_vs_iteration</code>             \u2013              <p>Plot metrics vs iteration for training and validation.</p> </li> <li> <code>plot_metric_vs_iteration_per_metric</code>             \u2013              <p>Plot the metric vs the iteration.</p> </li> </ul> Source code in <code>src/stimulus/analysis/analysis_default.py</code> <pre><code>def __init__(self, results_path: str) -&gt; None:\n    \"\"\"Initialize the AnalysisPerformanceTune class.\"\"\"\n    super().__init__()\n    self.results = pd.read_csv(results_path)\n</code></pre>"},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default.AnalysisPerformanceTune.annotate_heatmap","title":"annotate_heatmap  <code>staticmethod</code>","text":"<pre><code>annotate_heatmap(\n    im: Any,\n    data: ndarray | None = None,\n    valfmt: Union[str, StrMethodFormatter] = \"{x:.2f}\",\n    textcolors: tuple[str, str] = (\"black\", \"white\"),\n    threshold: float | None = None,\n    **textkw: Any\n) -&gt; list[Any]\n</code></pre> <p>A function to annotate a heatmap.</p>"},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default.AnalysisPerformanceTune.annotate_heatmap--parameters","title":"Parameters","text":"<p>im     The AxesImage to be labeled. data     Data used to annotate.  If None, the image's data is used.  Optional. valfmt     The format of the annotations inside the heatmap.  This should either     use the string format method, e.g. \"$ {x:.2f}\", or be a     <code>matplotlib.ticker.Formatter</code>.  Optional. textcolors     A pair of colors.  The first is used for values below a threshold,     the second for those above.  Optional. threshold     Value in data units according to which the colors from textcolors are     applied.  If None (the default) uses the middle of the colormap as     separation.  Optional. **kwargs     All other arguments are forwarded to each call to <code>text</code> used to create     the text labels.</p> Source code in <code>src/stimulus/analysis/analysis_default.py</code> <pre><code>@staticmethod\ndef annotate_heatmap(\n    im: Any,\n    data: np.ndarray | None = None,\n    valfmt: Union[str, StrMethodFormatter] = \"{x:.2f}\",\n    textcolors: tuple[str, str] = (\"black\", \"white\"),\n    threshold: float | None = None,\n    **textkw: Any,\n) -&gt; list[Any]:\n    \"\"\"A function to annotate a heatmap.\n\n    Parameters\n    ----------\n    im\n        The AxesImage to be labeled.\n    data\n        Data used to annotate.  If None, the image's data is used.  Optional.\n    valfmt\n        The format of the annotations inside the heatmap.  This should either\n        use the string format method, e.g. \"$ {x:.2f}\", or be a\n        `matplotlib.ticker.Formatter`.  Optional.\n    textcolors\n        A pair of colors.  The first is used for values below a threshold,\n        the second for those above.  Optional.\n    threshold\n        Value in data units according to which the colors from textcolors are\n        applied.  If None (the default) uses the middle of the colormap as\n        separation.  Optional.\n    **kwargs\n        All other arguments are forwarded to each call to `text` used to create\n        the text labels.\n    \"\"\"\n    if not isinstance(data, (list, np.ndarray)):\n        data = im.get_array()\n\n    # Normalize the threshold to the images color range.\n    threshold = im.norm(threshold) if threshold is not None else im.norm(data.max()) / 2.0\n\n    # Set default alignment to center, but allow it to be\n    # overwritten by textkw.\n    kw = {\"horizontalalignment\": \"center\", \"verticalalignment\": \"center\"}\n    kw.update(textkw)\n\n    # Get the formatter in case a string is supplied\n    if isinstance(valfmt, str):\n        valfmt = StrMethodFormatter(valfmt)\n\n    # Loop over the data and create a `Text` for each \"pixel\".\n    # Change the text's color depending on the data.\n    texts = []\n    for i in range(data.shape[0]):\n        for j in range(data.shape[1]):\n            kw.update(color=textcolors[int(im.norm(data[i, j]) &gt; threshold)])\n            text = im.axes.text(j, i, valfmt(data[i, j]), **kw)\n            texts.append(text)\n\n    return texts\n</code></pre>"},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default.AnalysisPerformanceTune.get_grid_shape","title":"get_grid_shape  <code>staticmethod</code>","text":"<pre><code>get_grid_shape(n: int) -&gt; tuple[int, int]\n</code></pre> <p>Calculates rows and columns for a rectangle layout (flexible).</p> Source code in <code>src/stimulus/analysis/analysis_default.py</code> <pre><code>@staticmethod\ndef get_grid_shape(n: int) -&gt; tuple[int, int]:\n    \"\"\"Calculates rows and columns for a rectangle layout (flexible).\"\"\"\n    rows = int(math.ceil(math.sqrt(n)))  # Round up the square root for rows\n    cols = int(math.ceil(n / rows))  # Calculate columns based on rows\n    return rows, cols\n</code></pre>"},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default.AnalysisPerformanceTune.heatmap","title":"heatmap  <code>staticmethod</code>","text":"<pre><code>heatmap(\n    data: ndarray,\n    row_labels: list[str],\n    col_labels: list[str],\n    ax: Any | None = None,\n    cbar_kw: dict | None = None,\n    cbarlabel: str = \"\",\n    **kwargs: Any\n) -&gt; tuple[Any, Any]\n</code></pre> <p>Create a heatmap from a numpy array and two lists of labels.</p>"},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default.AnalysisPerformanceTune.heatmap--parameters","title":"Parameters","text":"<p>data     A 2D numpy array of shape (M, N). row_labels     A list or array of length M with the labels for the rows. col_labels     A list or array of length N with the labels for the columns. ax     A <code>matplotlib.axes.Axes</code> instance to which the heatmap is plotted.  If     not provided, use current axes or create a new one.  Optional. cbar_kw     A dictionary with arguments to <code>matplotlib.Figure.colorbar</code>.  Optional. cbarlabel     The label for the colorbar.  Optional. **kwargs     All other arguments are forwarded to <code>imshow</code>.</p> Source code in <code>src/stimulus/analysis/analysis_default.py</code> <pre><code>@staticmethod\ndef heatmap(\n    data: np.ndarray,\n    row_labels: list[str],\n    col_labels: list[str],\n    ax: Any | None = None,\n    cbar_kw: dict | None = None,\n    cbarlabel: str = \"\",\n    **kwargs: Any,\n) -&gt; tuple[Any, Any]:\n    \"\"\"Create a heatmap from a numpy array and two lists of labels.\n\n    Parameters\n    ----------\n    data\n        A 2D numpy array of shape (M, N).\n    row_labels\n        A list or array of length M with the labels for the rows.\n    col_labels\n        A list or array of length N with the labels for the columns.\n    ax\n        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n        not provided, use current axes or create a new one.  Optional.\n    cbar_kw\n        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n    cbarlabel\n        The label for the colorbar.  Optional.\n    **kwargs\n        All other arguments are forwarded to `imshow`.\n    \"\"\"\n    if ax is None:\n        ax = plt.gca()\n\n    if cbar_kw is None:\n        cbar_kw = {}\n\n    # Plot the heatmap\n    im = ax.imshow(data, **kwargs)\n\n    # Create colorbar\n    if ax.figure is not None and hasattr(ax.figure, \"colorbar\"):\n        cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n        cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n    else:\n        cbar = None\n\n    # Show all ticks and label them with the respective list entries.\n    ax.set_xticks(np.arange(data.shape[1]), labels=col_labels)\n    ax.set_yticks(np.arange(data.shape[0]), labels=row_labels)\n\n    # Let the horizontal axes labeling appear on top.\n    ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\", rotation_mode=\"anchor\")\n\n    # Turn spines off and create white grid.\n    ax.spines[:].set_visible(False)\n\n    ax.set_xticks(np.arange(data.shape[1] + 1) - 0.5, minor=True)\n    ax.set_yticks(np.arange(data.shape[0] + 1) - 0.5, minor=True)\n    ax.grid(which=\"minor\", color=\"w\", linestyle=\"-\", linewidth=3)\n    ax.tick_params(which=\"minor\", bottom=False, left=False)\n\n    return im, cbar\n</code></pre>"},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default.AnalysisPerformanceTune.plot_metric_vs_iteration","title":"plot_metric_vs_iteration","text":"<pre><code>plot_metric_vs_iteration(\n    metrics: list,\n    figsize: tuple = (10, 10),\n    output: str | None = None,\n) -&gt; None\n</code></pre> <p>Plot metrics vs iteration for training and validation.</p> Source code in <code>src/stimulus/analysis/analysis_default.py</code> <pre><code>def plot_metric_vs_iteration(\n    self,\n    metrics: list,\n    figsize: tuple = (10, 10),\n    output: str | None = None,\n) -&gt; None:\n    \"\"\"Plot metrics vs iteration for training and validation.\"\"\"\n    # create figure\n    rows, cols = self.get_grid_shape(len(metrics))\n    fig, axs = plt.subplots(rows, cols, figsize=figsize)\n\n    # plot each metric\n    for i, ax in enumerate(axs.flat):\n        if i &gt;= len(metrics):\n            ax.axis(\"off\")\n            continue\n        self.plot_metric_vs_iteration_per_metric(axs.flat[i], metrics[i])\n\n    # add legend\n    # axs.flat[0].legend()\n    handles, labels = axs[0, 0].get_legend_handles_labels()  # Get handles and labels from one subplot\n    plt.legend(handles, labels, loc=\"upper left\")  # Adjust location as needed\n\n    # save plot\n    plt.tight_layout()\n    if output:\n        plt.savefig(output)\n    plt.show()\n</code></pre>"},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default.AnalysisPerformanceTune.plot_metric_vs_iteration_per_metric","title":"plot_metric_vs_iteration_per_metric","text":"<pre><code>plot_metric_vs_iteration_per_metric(\n    ax: Any, metric: str\n) -&gt; Any\n</code></pre> <p>Plot the metric vs the iteration.</p> Source code in <code>src/stimulus/analysis/analysis_default.py</code> <pre><code>def plot_metric_vs_iteration_per_metric(self, ax: Any, metric: str) -&gt; Any:\n    \"\"\"Plot the metric vs the iteration.\"\"\"\n    # plot training performance\n    ax.plot(\n        self.results.training_iteration,\n        self.results[\"train_\" + metric],\n        c=\"blue\",\n        label=\"train\",\n    )\n\n    # plot validation performance\n    ax.plot(\n        self.results.training_iteration,\n        self.results[\"val_\" + metric],\n        c=\"orange\",\n        label=\"val\",\n    )\n\n    # TODO set x-axis labels into integer\n    # plt.xticks(range(min(self.results.training_iteration), max(self.results.training_iteration)))\n\n    # add labels\n    ax.set_xlabel(\"epoch\")\n    ax.set_ylabel(metric)\n\n    return ax\n</code></pre>"},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default.AnalysisRobustness","title":"AnalysisRobustness","text":"<pre><code>AnalysisRobustness(\n    metrics: list, experiment: object, batch_size: int\n)\n</code></pre> <p>               Bases: <code>Analysis</code></p> <p>Report the robustness of the models.</p> <p>Methods:</p> <ul> <li> <code>annotate_heatmap</code>             \u2013              <p>A function to annotate a heatmap.</p> </li> <li> <code>get_average_performance_table</code>             \u2013              <p>Compute the average performance of each model on each dataset.</p> </li> <li> <code>get_grid_shape</code>             \u2013              <p>Calculates rows and columns for a rectangle layout (flexible).</p> </li> <li> <code>get_performance_table</code>             \u2013              <p>Compute the performance metrics of each model on each dataset.</p> </li> <li> <code>get_performance_table_for_one_model</code>             \u2013              <p>Compute the performance table of one model on each dataset.</p> </li> <li> <code>heatmap</code>             \u2013              <p>Create a heatmap from a numpy array and two lists of labels.</p> </li> <li> <code>parse_delta_performance_for_one_model</code>             \u2013              <p>Compute the delta performance of one model.</p> </li> <li> <code>plot_delta_performance</code>             \u2013              <p>Plot the delta performance of each model on each dataset.</p> </li> <li> <code>plot_delta_performance_for_one_model</code>             \u2013              <p>Plot the delta performance of one model.</p> </li> <li> <code>plot_performance_heatmap</code>             \u2013              <p>Plot the performance of each model on each dataset.</p> </li> </ul> Source code in <code>src/stimulus/analysis/analysis_default.py</code> <pre><code>def __init__(self, metrics: list, experiment: object, batch_size: int) -&gt; None:\n    \"\"\"Initialize the AnalysisRobustness class.\"\"\"\n    super().__init__()\n    self.metrics = metrics\n    self.experiment = experiment\n    self.batch_size = batch_size\n</code></pre>"},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default.AnalysisRobustness.annotate_heatmap","title":"annotate_heatmap  <code>staticmethod</code>","text":"<pre><code>annotate_heatmap(\n    im: Any,\n    data: ndarray | None = None,\n    valfmt: Union[str, StrMethodFormatter] = \"{x:.2f}\",\n    textcolors: tuple[str, str] = (\"black\", \"white\"),\n    threshold: float | None = None,\n    **textkw: Any\n) -&gt; list[Any]\n</code></pre> <p>A function to annotate a heatmap.</p>"},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default.AnalysisRobustness.annotate_heatmap--parameters","title":"Parameters","text":"<p>im     The AxesImage to be labeled. data     Data used to annotate.  If None, the image's data is used.  Optional. valfmt     The format of the annotations inside the heatmap.  This should either     use the string format method, e.g. \"$ {x:.2f}\", or be a     <code>matplotlib.ticker.Formatter</code>.  Optional. textcolors     A pair of colors.  The first is used for values below a threshold,     the second for those above.  Optional. threshold     Value in data units according to which the colors from textcolors are     applied.  If None (the default) uses the middle of the colormap as     separation.  Optional. **kwargs     All other arguments are forwarded to each call to <code>text</code> used to create     the text labels.</p> Source code in <code>src/stimulus/analysis/analysis_default.py</code> <pre><code>@staticmethod\ndef annotate_heatmap(\n    im: Any,\n    data: np.ndarray | None = None,\n    valfmt: Union[str, StrMethodFormatter] = \"{x:.2f}\",\n    textcolors: tuple[str, str] = (\"black\", \"white\"),\n    threshold: float | None = None,\n    **textkw: Any,\n) -&gt; list[Any]:\n    \"\"\"A function to annotate a heatmap.\n\n    Parameters\n    ----------\n    im\n        The AxesImage to be labeled.\n    data\n        Data used to annotate.  If None, the image's data is used.  Optional.\n    valfmt\n        The format of the annotations inside the heatmap.  This should either\n        use the string format method, e.g. \"$ {x:.2f}\", or be a\n        `matplotlib.ticker.Formatter`.  Optional.\n    textcolors\n        A pair of colors.  The first is used for values below a threshold,\n        the second for those above.  Optional.\n    threshold\n        Value in data units according to which the colors from textcolors are\n        applied.  If None (the default) uses the middle of the colormap as\n        separation.  Optional.\n    **kwargs\n        All other arguments are forwarded to each call to `text` used to create\n        the text labels.\n    \"\"\"\n    if not isinstance(data, (list, np.ndarray)):\n        data = im.get_array()\n\n    # Normalize the threshold to the images color range.\n    threshold = im.norm(threshold) if threshold is not None else im.norm(data.max()) / 2.0\n\n    # Set default alignment to center, but allow it to be\n    # overwritten by textkw.\n    kw = {\"horizontalalignment\": \"center\", \"verticalalignment\": \"center\"}\n    kw.update(textkw)\n\n    # Get the formatter in case a string is supplied\n    if isinstance(valfmt, str):\n        valfmt = StrMethodFormatter(valfmt)\n\n    # Loop over the data and create a `Text` for each \"pixel\".\n    # Change the text's color depending on the data.\n    texts = []\n    for i in range(data.shape[0]):\n        for j in range(data.shape[1]):\n            kw.update(color=textcolors[int(im.norm(data[i, j]) &gt; threshold)])\n            text = im.axes.text(j, i, valfmt(data[i, j]), **kw)\n            texts.append(text)\n\n    return texts\n</code></pre>"},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default.AnalysisRobustness.get_average_performance_table","title":"get_average_performance_table","text":"<pre><code>get_average_performance_table(df: DataFrame) -&gt; DataFrame\n</code></pre> <p>Compute the average performance of each model on each dataset.</p> <p>Parameters:</p> <ul> <li> <code>df</code>               (<code>DataFrame</code>)           \u2013            <p>DataFrame containing the performance table.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with averaged metrics.</p> </li> </ul> Source code in <code>src/stimulus/analysis/analysis_default.py</code> <pre><code>def get_average_performance_table(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Compute the average performance of each model on each dataset.\n\n    Args:\n        df: DataFrame containing the performance table.\n\n    Returns:\n        DataFrame with averaged metrics.\n    \"\"\"\n    df = df[[*self.metrics, \"model\"]]  # Use list unpacking instead of concatenation\n    return df.groupby([\"model\"]).mean().reset_index()\n</code></pre>"},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default.AnalysisRobustness.get_grid_shape","title":"get_grid_shape  <code>staticmethod</code>","text":"<pre><code>get_grid_shape(n: int) -&gt; tuple[int, int]\n</code></pre> <p>Calculates rows and columns for a rectangle layout (flexible).</p> Source code in <code>src/stimulus/analysis/analysis_default.py</code> <pre><code>@staticmethod\ndef get_grid_shape(n: int) -&gt; tuple[int, int]:\n    \"\"\"Calculates rows and columns for a rectangle layout (flexible).\"\"\"\n    rows = int(math.ceil(math.sqrt(n)))  # Round up the square root for rows\n    cols = int(math.ceil(n / rows))  # Calculate columns based on rows\n    return rows, cols\n</code></pre>"},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default.AnalysisRobustness.get_performance_table","title":"get_performance_table","text":"<pre><code>get_performance_table(\n    names: list, model_list: dict, data_list: list\n) -&gt; DataFrame\n</code></pre> <p>Compute the performance metrics of each model on each dataset.</p> <p>Parameters:</p> <ul> <li> <code>names</code>               (<code>list</code>)           \u2013            <p>List of names that identifies each model.</p> </li> <li> <code>model_list</code>               (<code>dict</code>)           \u2013            <p>Dictionary of models in same order as data_list.</p> </li> <li> <code>data_list</code>               (<code>list</code>)           \u2013            <p>List of datasets used for training.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame containing performance metrics.</p> </li> </ul> Source code in <code>src/stimulus/analysis/analysis_default.py</code> <pre><code>def get_performance_table(self, names: list, model_list: dict, data_list: list) -&gt; pd.DataFrame:\n    \"\"\"Compute the performance metrics of each model on each dataset.\n\n    Args:\n        names: List of names that identifies each model.\n        model_list: Dictionary of models in same order as data_list.\n        data_list: List of datasets used for training.\n\n    Returns:\n        DataFrame containing performance metrics.\n    \"\"\"\n    # check same length\n    if (len(names) != len(model_list)) and (len(names) != len(data_list)):\n        raise ValueError(\"The length of the names, model_list and data_list should be the same.\")\n\n    # initialize\n    df = pd.DataFrame()\n    model_names = []\n\n    # for each model, get the performance table, and concat\n    for i, model in enumerate(model_list):\n        df = pd.concat([df, self.get_performance_table_for_one_model(names, model, data_list)])\n        model_names += [names[i]] * len(data_list)\n    df[\"model\"] = model_names\n\n    return df\n</code></pre>"},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default.AnalysisRobustness.get_performance_table_for_one_model","title":"get_performance_table_for_one_model","text":"<pre><code>get_performance_table_for_one_model(\n    names: list, model: object, data_list: list\n) -&gt; DataFrame\n</code></pre> <p>Compute the performance table of one model on each dataset.</p> Source code in <code>src/stimulus/analysis/analysis_default.py</code> <pre><code>def get_performance_table_for_one_model(self, names: list, model: object, data_list: list) -&gt; pd.DataFrame:\n    \"\"\"Compute the performance table of one model on each dataset.\"\"\"\n    df = pd.DataFrame()\n    for data_path in data_list:  # for each data, get the performance metrics, and concat\n        # initialize the dataframe keeping the original order, aka no shuffle\n        dataloader = DataLoader(\n            TorchDataset(data_path, self.experiment, split=2),\n            batch_size=self.batch_size,\n            shuffle=False,\n        )\n        metric_values = PredictWrapper(model, dataloader).compute_metrics(self.metrics)\n        df = pd.concat([df, pd.DataFrame(metric_values, index=[0])])\n    df[\"data\"] = names\n    return df\n</code></pre>"},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default.AnalysisRobustness.heatmap","title":"heatmap  <code>staticmethod</code>","text":"<pre><code>heatmap(\n    data: ndarray,\n    row_labels: list[str],\n    col_labels: list[str],\n    ax: Any | None = None,\n    cbar_kw: dict | None = None,\n    cbarlabel: str = \"\",\n    **kwargs: Any\n) -&gt; tuple[Any, Any]\n</code></pre> <p>Create a heatmap from a numpy array and two lists of labels.</p>"},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default.AnalysisRobustness.heatmap--parameters","title":"Parameters","text":"<p>data     A 2D numpy array of shape (M, N). row_labels     A list or array of length M with the labels for the rows. col_labels     A list or array of length N with the labels for the columns. ax     A <code>matplotlib.axes.Axes</code> instance to which the heatmap is plotted.  If     not provided, use current axes or create a new one.  Optional. cbar_kw     A dictionary with arguments to <code>matplotlib.Figure.colorbar</code>.  Optional. cbarlabel     The label for the colorbar.  Optional. **kwargs     All other arguments are forwarded to <code>imshow</code>.</p> Source code in <code>src/stimulus/analysis/analysis_default.py</code> <pre><code>@staticmethod\ndef heatmap(\n    data: np.ndarray,\n    row_labels: list[str],\n    col_labels: list[str],\n    ax: Any | None = None,\n    cbar_kw: dict | None = None,\n    cbarlabel: str = \"\",\n    **kwargs: Any,\n) -&gt; tuple[Any, Any]:\n    \"\"\"Create a heatmap from a numpy array and two lists of labels.\n\n    Parameters\n    ----------\n    data\n        A 2D numpy array of shape (M, N).\n    row_labels\n        A list or array of length M with the labels for the rows.\n    col_labels\n        A list or array of length N with the labels for the columns.\n    ax\n        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n        not provided, use current axes or create a new one.  Optional.\n    cbar_kw\n        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n    cbarlabel\n        The label for the colorbar.  Optional.\n    **kwargs\n        All other arguments are forwarded to `imshow`.\n    \"\"\"\n    if ax is None:\n        ax = plt.gca()\n\n    if cbar_kw is None:\n        cbar_kw = {}\n\n    # Plot the heatmap\n    im = ax.imshow(data, **kwargs)\n\n    # Create colorbar\n    if ax.figure is not None and hasattr(ax.figure, \"colorbar\"):\n        cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n        cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n    else:\n        cbar = None\n\n    # Show all ticks and label them with the respective list entries.\n    ax.set_xticks(np.arange(data.shape[1]), labels=col_labels)\n    ax.set_yticks(np.arange(data.shape[0]), labels=row_labels)\n\n    # Let the horizontal axes labeling appear on top.\n    ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\", rotation_mode=\"anchor\")\n\n    # Turn spines off and create white grid.\n    ax.spines[:].set_visible(False)\n\n    ax.set_xticks(np.arange(data.shape[1] + 1) - 0.5, minor=True)\n    ax.set_yticks(np.arange(data.shape[0] + 1) - 0.5, minor=True)\n    ax.grid(which=\"minor\", color=\"w\", linestyle=\"-\", linewidth=3)\n    ax.tick_params(which=\"minor\", bottom=False, left=False)\n\n    return im, cbar\n</code></pre>"},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default.AnalysisRobustness.parse_delta_performance_for_one_model","title":"parse_delta_performance_for_one_model","text":"<pre><code>parse_delta_performance_for_one_model(\n    metric: str, df: DataFrame, model_name: str\n) -&gt; DataFrame\n</code></pre> <p>Compute the delta performance of one model.</p> Source code in <code>src/stimulus/analysis/analysis_default.py</code> <pre><code>def parse_delta_performance_for_one_model(self, metric: str, df: pd.DataFrame, model_name: str) -&gt; pd.DataFrame:\n    \"\"\"Compute the delta performance of one model.\"\"\"\n    # filter data frame\n    df = df[[\"data\", \"model\", metric]]\n    df = df[df[\"model\"] == model_name]\n\n    # compute the delta performance between each row vs the reference\n    reference_row = df.loc[df[\"data\"] == model_name]\n    df[metric] = -df[metric].sub(reference_row[metric])\n\n    return df\n</code></pre>"},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default.AnalysisRobustness.plot_delta_performance","title":"plot_delta_performance","text":"<pre><code>plot_delta_performance(\n    metric: str,\n    df: DataFrame,\n    figsize: tuple = (10, 10),\n    output: str | None = None,\n) -&gt; None\n</code></pre> <p>Plot the delta performance of each model on each dataset.</p> Source code in <code>src/stimulus/analysis/analysis_default.py</code> <pre><code>def plot_delta_performance(\n    self,\n    metric: str,\n    df: pd.DataFrame,\n    figsize: tuple = (10, 10),\n    output: str | None = None,\n) -&gt; None:\n    \"\"\"Plot the delta performance of each model on each dataset.\"\"\"\n    # create figure\n    rows, cols = self.get_grid_shape(len(df[\"model\"].unique()))\n    fig, axs = plt.subplots(rows, cols, figsize=figsize)\n\n    # if there is only one plot plot.sublots will output &lt;class 'matplotlib.axes._axes.Axes'&gt;, while if there are more than one it will return a np.ndarray. there is the need to unify the two cases. following line does this\n    if not isinstance(axs, np.ndarray):\n        axs = np.array([axs])\n\n    # plot each model\n    for i, ax in enumerate(axs.flat):\n        if i &gt;= len(df[\"model\"].unique()):\n            ax.axis(\"off\")\n            continue\n        self.plot_delta_performance_for_one_model(ax, metric, df, df[\"model\"].unique()[i])\n\n    # set common y limits\n    ymin = min([ax.get_ylim()[0] for ax in axs.flat])\n    ymax = max([ax.get_ylim()[1] for ax in axs.flat])\n    for ax in axs.flat:\n        spacer = abs(ymin - ymax)\n        spacer = spacer * 0.01\n        ax.set_ylim(ymin - spacer, ymax + spacer)\n\n    # save plot\n    plt.tight_layout()\n    if output:\n        plt.savefig(output)\n    plt.show()\n</code></pre>"},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default.AnalysisRobustness.plot_delta_performance_for_one_model","title":"plot_delta_performance_for_one_model","text":"<pre><code>plot_delta_performance_for_one_model(\n    ax: Any, metric: str, df: DataFrame, model_name: str\n) -&gt; Any\n</code></pre> <p>Plot the delta performance of one model.</p> Source code in <code>src/stimulus/analysis/analysis_default.py</code> <pre><code>def plot_delta_performance_for_one_model(self, ax: Any, metric: str, df: pd.DataFrame, model_name: str) -&gt; Any:\n    \"\"\"Plot the delta performance of one model.\"\"\"\n    df = self.parse_delta_performance_for_one_model(metric, df, model_name)\n\n    # plot a barplot with positive negative values for each row\n    # TODO use different colors for positive and negative values\n    df = df.set_index(\"data\")\n    df.plot(kind=\"bar\", ax=ax, stacked=True)\n\n    ax.set_xlabel(\"\")\n    ax.get_legend().remove()\n    ax.set_title(model_name)\n\n    return ax\n</code></pre>"},{"location":"reference/stimulus/analysis/analysis_default/#stimulus.analysis.analysis_default.AnalysisRobustness.plot_performance_heatmap","title":"plot_performance_heatmap","text":"<pre><code>plot_performance_heatmap(\n    df: DataFrame,\n    figsize: tuple = (10, 10),\n    output: str | None = None,\n) -&gt; None\n</code></pre> <p>Plot the performance of each model on each dataset.</p> Source code in <code>src/stimulus/analysis/analysis_default.py</code> <pre><code>def plot_performance_heatmap(self, df: pd.DataFrame, figsize: tuple = (10, 10), output: str | None = None) -&gt; None:\n    \"\"\"Plot the performance of each model on each dataset.\"\"\"\n    # create figure\n    rows, cols = self.get_grid_shape(len(self.metrics))\n    fig, axs = plt.subplots(rows, cols, figsize=figsize)\n\n    # if there is only one plot plot.sublots will output a simple list, while if there are more than one it will return a list of lists. there is the need to unify the two cases. following line does this\n    if not isinstance(axs, np.ndarray):\n        axs = np.array([axs])\n\n    for i, ax in enumerate(axs.flat):\n        if i &gt;= len(self.metrics):\n            ax.axis(\"off\")\n            continue\n\n        # reshape the data frame into the matrix for one metric\n        mat = df[[\"model\", \"data\", self.metrics[i]]]\n        mat = mat.pivot(index=\"model\", columns=\"data\", values=self.metrics[i])\n\n        # plot heatmap\n        im, cbar = self.heatmap(mat, mat.index, mat.columns, ax=ax, cmap=\"YlGn\", cbarlabel=self.metrics[i])\n        self.annotate_heatmap(im, valfmt=\"{x:.2f}\")  # Don't assign to unused variable\n\n    # save plot\n    plt.tight_layout()\n    if output:\n        plt.savefig(output)\n    plt.show()\n</code></pre>"},{"location":"reference/stimulus/cli/","title":"stimulus.cli","text":""},{"location":"reference/stimulus/cli/#stimulus.cli","title":"cli","text":"<p>Command line interface package for the stimulus library.</p> <p>Modules:</p> <ul> <li> <code>analysis_default</code>           \u2013            <p>Analysis default module for running model analysis and performance evaluation.</p> </li> <li> <code>check_model</code>           \u2013            <p>CLI module for checking model configuration and running initial tests.</p> </li> <li> <code>predict</code>           \u2013            <p>CLI module for model prediction on datasets.</p> </li> <li> <code>shuffle_csv</code>           \u2013            <p>CLI module for shuffling CSV data files.</p> </li> <li> <code>split_csv</code>           \u2013            <p>CLI module for splitting CSV data files.</p> </li> <li> <code>split_yaml</code>           \u2013            <p>CLI module for splitting YAML configuration files.</p> </li> <li> <code>transform_csv</code>           \u2013            <p>CLI module for transforming CSV data files.</p> </li> <li> <code>tuning</code>           \u2013            <p>CLI module for tuning model hyperparameters using Ray Tune.</p> </li> </ul>"},{"location":"reference/stimulus/cli/analysis_default/","title":"stimulus.cli.analysis_default","text":""},{"location":"reference/stimulus/cli/analysis_default/#stimulus.cli.analysis_default","title":"analysis_default","text":"<p>Analysis default module for running model analysis and performance evaluation.</p> <p>Functions:</p> <ul> <li> <code>get_args</code>             \u2013              <p>Get the arguments when using from the commandline.</p> </li> <li> <code>load_model</code>             \u2013              <p>Load the model with its config and weights.</p> </li> <li> <code>main</code>             \u2013              <p>Run the main analysis pipeline.</p> </li> <li> <code>run</code>             \u2013              <p>Run the analysis script.</p> </li> <li> <code>run_analysis_performance_model</code>             \u2013              <p>Run analysis to report model robustness.</p> </li> <li> <code>run_analysis_performance_tune</code>             \u2013              <p>Run performance analysis during tuning/training.</p> </li> </ul>"},{"location":"reference/stimulus/cli/analysis_default/#stimulus.cli.analysis_default.get_args","title":"get_args","text":"<pre><code>get_args() -&gt; Namespace\n</code></pre> <p>Get the arguments when using from the commandline.</p> <p>Returns:</p> <ul> <li> <code>Namespace</code>           \u2013            <p>Parsed command line arguments.</p> </li> </ul> Source code in <code>src/stimulus/cli/analysis_default.py</code> <pre><code>def get_args() -&gt; argparse.Namespace:\n    \"\"\"Get the arguments when using from the commandline.\n\n    Returns:\n        Parsed command line arguments.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"\")\n    parser.add_argument(\"-m\", \"--model\", type=str, required=True, metavar=\"FILE\", help=\"The model .py file\")\n    parser.add_argument(\n        \"-w\",\n        \"--weight\",\n        type=str,\n        required=True,\n        nargs=\"+\",\n        metavar=\"FILE\",\n        help=\"Model weights .pt file\",\n    )\n    parser.add_argument(\n        \"-me\",\n        \"--metrics\",\n        type=str,\n        required=True,\n        nargs=\"+\",\n        metavar=\"FILE\",\n        help=\"The file path for the metrics file obtained during tuning\",\n    )\n    parser.add_argument(\n        \"-ec\",\n        \"--experiment_config\",\n        type=str,\n        required=True,\n        nargs=\"+\",\n        metavar=\"FILE\",\n        help=\"The experiment config used to modify the data.\",\n    )\n    parser.add_argument(\n        \"-mc\",\n        \"--model_config\",\n        type=str,\n        required=True,\n        nargs=\"+\",\n        metavar=\"FILE\",\n        help=\"The tune config file.\",\n    )\n    parser.add_argument(\n        \"-d\",\n        \"--data\",\n        type=str,\n        required=True,\n        nargs=\"+\",\n        metavar=\"FILE\",\n        help=\"List of data files to be used for the analysis.\",\n    )\n    parser.add_argument(\"-o\", \"--outdir\", type=str, required=True, help=\"output directory\")\n\n    return parser.parse_args()\n</code></pre>"},{"location":"reference/stimulus/cli/analysis_default/#stimulus.cli.analysis_default.load_model","title":"load_model","text":"<pre><code>load_model(\n    model_class: Any, weight_path: str, mconfig_path: str\n) -&gt; Any\n</code></pre> <p>Load the model with its config and weights.</p> <p>Parameters:</p> <ul> <li> <code>model_class</code>               (<code>Any</code>)           \u2013            <p>Model class to instantiate</p> </li> <li> <code>weight_path</code>               (<code>str</code>)           \u2013            <p>Path to model weights</p> </li> <li> <code>mconfig_path</code>               (<code>str</code>)           \u2013            <p>Path to model config</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>Loaded model instance</p> </li> </ul> Source code in <code>src/stimulus/cli/analysis_default.py</code> <pre><code>def load_model(model_class: Any, weight_path: str, mconfig_path: str) -&gt; Any:\n    \"\"\"Load the model with its config and weights.\n\n    Args:\n        model_class: Model class to instantiate\n        weight_path: Path to model weights\n        mconfig_path: Path to model config\n\n    Returns:\n        Loaded model instance\n    \"\"\"\n    with open(mconfig_path) as in_json:\n        mconfig = json.load(in_json)[\"model_params\"]\n\n    model = model_class(**mconfig)\n    return safe_load(model, weight_path, strict=True)\n</code></pre>"},{"location":"reference/stimulus/cli/analysis_default/#stimulus.cli.analysis_default.main","title":"main","text":"<pre><code>main(\n    model_path: str,\n    weight_list: list[str],\n    mconfig_list: list[str],\n    metrics_list: list[str],\n    econfig_list: list[str],\n    data_list: list[str],\n    outdir: str,\n) -&gt; None\n</code></pre> <p>Run the main analysis pipeline.</p> <p>Parameters:</p> <ul> <li> <code>model_path</code>               (<code>str</code>)           \u2013            <p>Path to model file</p> </li> <li> <code>weight_list</code>               (<code>list[str]</code>)           \u2013            <p>List of model weight paths</p> </li> <li> <code>mconfig_list</code>               (<code>list[str]</code>)           \u2013            <p>List of model config paths</p> </li> <li> <code>metrics_list</code>               (<code>list[str]</code>)           \u2013            <p>List of metric file paths</p> </li> <li> <code>econfig_list</code>               (<code>list[str]</code>)           \u2013            <p>List of experiment config paths</p> </li> <li> <code>data_list</code>               (<code>list[str]</code>)           \u2013            <p>List of data file paths</p> </li> <li> <code>outdir</code>               (<code>str</code>)           \u2013            <p>Output directory path</p> </li> </ul> Source code in <code>src/stimulus/cli/analysis_default.py</code> <pre><code>def main(\n    model_path: str,\n    weight_list: list[str],\n    mconfig_list: list[str],\n    metrics_list: list[str],\n    econfig_list: list[str],\n    data_list: list[str],\n    outdir: str,\n) -&gt; None:\n    \"\"\"Run the main analysis pipeline.\n\n    Args:\n        model_path: Path to model file\n        weight_list: List of model weight paths\n        mconfig_list: List of model config paths\n        metrics_list: List of metric file paths\n        econfig_list: List of experiment config paths\n        data_list: List of data file paths\n        outdir: Output directory path\n    \"\"\"\n    metrics = [\"rocauc\", \"prauc\", \"mcc\", \"f1score\", \"precision\", \"recall\"]\n\n    # Plot the performance during tuning/training\n    run_analysis_performance_tune(\n        metrics_list,\n        [*metrics, \"loss\"],  # Use list unpacking instead of concatenation\n        os.path.join(outdir, \"performance_tune_train\"),\n    )\n\n    # Run robustness analysis\n    run_analysis_performance_model(\n        metrics,\n        model_path,\n        weight_list,\n        mconfig_list,\n        econfig_list,\n        data_list,\n        os.path.join(outdir, \"performance_robustness\"),\n    )\n</code></pre>"},{"location":"reference/stimulus/cli/analysis_default/#stimulus.cli.analysis_default.run","title":"run","text":"<pre><code>run() -&gt; None\n</code></pre> <p>Run the analysis script.</p> Source code in <code>src/stimulus/cli/analysis_default.py</code> <pre><code>def run() -&gt; None:\n    \"\"\"Run the analysis script.\"\"\"\n    args = get_args()\n    main(args.model, args.weight, args.model_config, args.metrics, args.experiment_config, args.data, args.outdir)\n</code></pre>"},{"location":"reference/stimulus/cli/analysis_default/#stimulus.cli.analysis_default.run_analysis_performance_model","title":"run_analysis_performance_model","text":"<pre><code>run_analysis_performance_model(\n    metrics: list[str],\n    model_path: str,\n    weight_list: list[str],\n    mconfig_list: list[str],\n    econfig_list: list[str],\n    data_list: list[str],\n    outdir: str,\n) -&gt; None\n</code></pre> <p>Run analysis to report model robustness.</p> <p>This block will compute the predictions of each model for each dataset. This information will be parsed and plots will be generated to report the model robustness.</p> <p>Parameters:</p> <ul> <li> <code>metrics</code>               (<code>list[str]</code>)           \u2013            <p>List of metrics to analyze</p> </li> <li> <code>model_path</code>               (<code>str</code>)           \u2013            <p>Path to model file</p> </li> <li> <code>weight_list</code>               (<code>list[str]</code>)           \u2013            <p>List of model weight paths</p> </li> <li> <code>mconfig_list</code>               (<code>list[str]</code>)           \u2013            <p>List of model config paths</p> </li> <li> <code>econfig_list</code>               (<code>list[str]</code>)           \u2013            <p>List of experiment config paths</p> </li> <li> <code>data_list</code>               (<code>list[str]</code>)           \u2013            <p>List of data file paths</p> </li> <li> <code>outdir</code>               (<code>str</code>)           \u2013            <p>Output directory path</p> </li> </ul> Source code in <code>src/stimulus/cli/analysis_default.py</code> <pre><code>def run_analysis_performance_model(\n    metrics: list[str],\n    model_path: str,\n    weight_list: list[str],\n    mconfig_list: list[str],\n    econfig_list: list[str],\n    data_list: list[str],\n    outdir: str,\n) -&gt; None:\n    \"\"\"Run analysis to report model robustness.\n\n    This block will compute the predictions of each model for each dataset.\n    This information will be parsed and plots will be generated to report the model robustness.\n\n    Args:\n        metrics: List of metrics to analyze\n        model_path: Path to model file\n        weight_list: List of model weight paths\n        mconfig_list: List of model config paths\n        econfig_list: List of experiment config paths\n        data_list: List of data file paths\n        outdir: Output directory path\n    \"\"\"\n    if not os.path.exists(outdir):\n        os.makedirs(outdir)\n\n    # Load all the models weights into a list\n    model_names = []\n    model_list = []\n    model_class = import_class_from_file(model_path)\n    for weight_path, mconfig_path in zip(weight_list, mconfig_list):\n        model = load_model(model_class, weight_path, mconfig_path)\n        model_names.append(mconfig_path.split(\"/\")[-1].replace(\"-config.json\", \"\"))\n        model_list.append(model)\n\n    # Read experiment config and initialize experiment class\n    with open(econfig_list[0]) as in_json:\n        experiment_name = json.load(in_json)[\"experiment\"]\n    initialized_experiment_class = get_experiment(experiment_name)\n\n    # Initialize analysis\n    analysis = AnalysisRobustness(metrics, initialized_experiment_class, batch_size=256)\n\n    # Compute performance metrics\n    df = analysis.get_performance_table(model_names, model_list, data_list)\n    df.to_csv(os.path.join(outdir, \"performance_table.csv\"), index=False)\n\n    # Get average performance\n    tmp = analysis.get_average_performance_table(df)\n    tmp.to_csv(os.path.join(outdir, \"average_performance_table.csv\"), index=False)\n\n    # Plot heatmap\n    analysis.plot_performance_heatmap(df, output=os.path.join(outdir, \"performance_heatmap.png\"))\n\n    # Plot delta performance\n    outdir2 = os.path.join(outdir, \"delta_performance_vs_data\")\n    if not os.path.exists(outdir2):\n        os.makedirs(outdir2)\n    for metric in metrics:\n        analysis.plot_delta_performance(\n            metric,\n            df,\n            output=os.path.join(outdir2, f\"delta_performance_{metric}.png\"),\n        )\n</code></pre>"},{"location":"reference/stimulus/cli/analysis_default/#stimulus.cli.analysis_default.run_analysis_performance_tune","title":"run_analysis_performance_tune","text":"<pre><code>run_analysis_performance_tune(\n    metrics_list: list[str], metrics: list[str], outdir: str\n) -&gt; None\n</code></pre> <p>Run performance analysis during tuning/training.</p> <p>Each model has a metrics file obtained during tuning/training, check the performance there and plot it. This is to track the model performance per training iteration.</p> <p>Parameters:</p> <ul> <li> <code>metrics_list</code>               (<code>list[str]</code>)           \u2013            <p>List of metric file paths</p> </li> <li> <code>metrics</code>               (<code>list[str]</code>)           \u2013            <p>List of metrics to analyze</p> </li> <li> <code>outdir</code>               (<code>str</code>)           \u2013            <p>Output directory path</p> </li> </ul> Source code in <code>src/stimulus/cli/analysis_default.py</code> <pre><code>def run_analysis_performance_tune(metrics_list: list[str], metrics: list[str], outdir: str) -&gt; None:\n    \"\"\"Run performance analysis during tuning/training.\n\n    Each model has a metrics file obtained during tuning/training,\n    check the performance there and plot it.\n    This is to track the model performance per training iteration.\n\n    Args:\n        metrics_list: List of metric file paths\n        metrics: List of metrics to analyze\n        outdir: Output directory path\n    \"\"\"\n    if not os.path.exists(outdir):\n        os.makedirs(outdir)\n\n    for metrics_path in metrics_list:\n        AnalysisPerformanceTune(metrics_path).plot_metric_vs_iteration(\n            metrics=metrics,\n            output=os.path.join(outdir, metrics_path.replace(\"-metrics.csv\", \"\") + \"-metric_vs_iteration.png\"),\n        )\n</code></pre>"},{"location":"reference/stimulus/cli/check_model/","title":"stimulus.cli.check_model","text":""},{"location":"reference/stimulus/cli/check_model/#stimulus.cli.check_model","title":"check_model","text":"<p>CLI module for checking model configuration and running initial tests.</p> <p>Functions:</p> <ul> <li> <code>get_args</code>             \u2013              <p>Get the arguments when using from the commandline.</p> </li> <li> <code>main</code>             \u2013              <p>Run the main model checking pipeline.</p> </li> <li> <code>run</code>             \u2013              <p>Run the model checking script.</p> </li> </ul>"},{"location":"reference/stimulus/cli/check_model/#stimulus.cli.check_model.get_args","title":"get_args","text":"<pre><code>get_args() -&gt; Namespace\n</code></pre> <p>Get the arguments when using from the commandline.</p> <p>Returns:</p> <ul> <li> <code>Namespace</code>           \u2013            <p>Parsed command line arguments.</p> </li> </ul> Source code in <code>src/stimulus/cli/check_model.py</code> <pre><code>def get_args() -&gt; argparse.Namespace:\n    \"\"\"Get the arguments when using from the commandline.\n\n    Returns:\n        Parsed command line arguments.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Launch check_model.\")\n    parser.add_argument(\"-d\", \"--data\", type=str, required=True, metavar=\"FILE\", help=\"Path to input csv file.\")\n    parser.add_argument(\"-m\", \"--model\", type=str, required=True, metavar=\"FILE\", help=\"Path to model file.\")\n    parser.add_argument(\n        \"-e\",\n        \"--experiment\",\n        type=str,\n        required=True,\n        metavar=\"FILE\",\n        help=\"Experiment config file. From this the experiment class name is extracted.\",\n    )\n    parser.add_argument(\n        \"-c\",\n        \"--config\",\n        type=str,\n        required=True,\n        metavar=\"FILE\",\n        help=\"Path to yaml config training file.\",\n    )\n    parser.add_argument(\n        \"-w\",\n        \"--initial_weights\",\n        type=str,\n        required=False,\n        nargs=\"?\",\n        const=None,\n        default=None,\n        metavar=\"FILE\",\n        help=\"The path to the initial weights. These can be used by the model instead of the random initialization.\",\n    )\n    parser.add_argument(\n        \"--gpus\",\n        type=int,\n        required=False,\n        nargs=\"?\",\n        const=None,\n        default=None,\n        metavar=\"NUM_OF_MAX_GPU\",\n        help=\"Use to limit the number of GPUs ray can use. This might be useful on many occasions, especially in a cluster system.\",\n    )\n    parser.add_argument(\n        \"--cpus\",\n        type=int,\n        required=False,\n        nargs=\"?\",\n        const=None,\n        default=None,\n        metavar=\"NUM_OF_MAX_CPU\",\n        help=\"Use to limit the number of CPUs ray can use. This might be useful on many occasions, especially in a cluster system.\",\n    )\n    parser.add_argument(\n        \"--memory\",\n        type=str,\n        required=False,\n        nargs=\"?\",\n        const=None,\n        default=None,\n        metavar=\"MAX_MEMORY\",\n        help=\"Ray can have a limiter on the total memory it can use. This might be useful on many occasions, especially in a cluster system.\",\n    )\n    parser.add_argument(\n        \"-n\",\n        \"--num_samples\",\n        type=int,\n        required=False,\n        nargs=\"?\",\n        const=3,\n        default=3,\n        metavar=\"NUM_SAMPLES\",\n        help=\"Number of samples for tuning. Overwrites tune.tune_params.num_samples in config.\",\n    )\n    parser.add_argument(\n        \"--ray_results_dirpath\",\n        type=str,\n        required=False,\n        nargs=\"?\",\n        const=None,\n        default=None,\n        metavar=\"DIR_PATH\",\n        help=\"Location where ray_results output dir should be written. If None, uses ~/ray_results.\",\n    )\n    parser.add_argument(\n        \"--debug_mode\",\n        action=\"store_true\",\n        help=\"Activate debug mode for tuning. Default false, no debug.\",\n    )\n\n    return parser.parse_args()\n</code></pre>"},{"location":"reference/stimulus/cli/check_model/#stimulus.cli.check_model.main","title":"main","text":"<pre><code>main(\n    data_path: str,\n    model_path: str,\n    experiment_config: str,\n    config_path: str,\n    initial_weights_path: str | None = None,\n    gpus: int | None = None,\n    cpus: int | None = None,\n    memory: str | None = None,\n    num_samples: int = 3,\n    ray_results_dirpath: str | None = None,\n    *,\n    debug_mode: bool = False\n) -&gt; None\n</code></pre> <p>Run the main model checking pipeline.</p> <p>Parameters:</p> <ul> <li> <code>data_path</code>               (<code>str</code>)           \u2013            <p>Path to input data file.</p> </li> <li> <code>model_path</code>               (<code>str</code>)           \u2013            <p>Path to model file.</p> </li> <li> <code>experiment_config</code>               (<code>str</code>)           \u2013            <p>Path to experiment config.</p> </li> <li> <code>config_path</code>               (<code>str</code>)           \u2013            <p>Path to training config.</p> </li> <li> <code>initial_weights_path</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional path to initial weights.</p> </li> <li> <code>gpus</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of GPUs to use.</p> </li> <li> <code>cpus</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of CPUs to use.</p> </li> <li> <code>memory</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum memory to use.</p> </li> <li> <code>num_samples</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Number of samples for tuning.</p> </li> <li> <code>ray_results_dirpath</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory for ray results.</p> </li> <li> <code>debug_mode</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to run in debug mode.</p> </li> </ul> Source code in <code>src/stimulus/cli/check_model.py</code> <pre><code>def main(\n    data_path: str,\n    model_path: str,\n    experiment_config: str,\n    config_path: str,\n    initial_weights_path: str | None = None,\n    gpus: int | None = None,\n    cpus: int | None = None,\n    memory: str | None = None,\n    num_samples: int = 3,\n    ray_results_dirpath: str | None = None,\n    *,\n    debug_mode: bool = False,\n) -&gt; None:\n    \"\"\"Run the main model checking pipeline.\n\n    Args:\n        data_path: Path to input data file.\n        model_path: Path to model file.\n        experiment_config: Path to experiment config.\n        config_path: Path to training config.\n        initial_weights_path: Optional path to initial weights.\n        gpus: Maximum number of GPUs to use.\n        cpus: Maximum number of CPUs to use.\n        memory: Maximum memory to use.\n        num_samples: Number of samples for tuning.\n        ray_results_dirpath: Directory for ray results.\n        debug_mode: Whether to run in debug mode.\n    \"\"\"\n    # Load experiment config\n    with open(experiment_config) as in_json:\n        exp_config = json.load(in_json)\n\n    # Initialize json schema and experiment class\n    schema = JsonSchema(exp_config)\n    initialized_experiment_class = get_experiment(schema.experiment)\n    model_class = import_class_from_file(model_path)\n\n    # Update tune config\n    updated_tune_conf = \"check_model_modified_tune_config.yaml\"\n    with open(config_path) as conf_file, open(updated_tune_conf, \"w\") as new_conf:\n        user_tune_config = yaml.safe_load(conf_file)\n        user_tune_config[\"tune\"][\"tune_params\"][\"num_samples\"] = num_samples\n\n        if user_tune_config[\"tune\"][\"scheduler\"][\"name\"] == \"ASHAScheduler\":\n            user_tune_config[\"tune\"][\"scheduler\"][\"params\"][\"max_t\"] = 1\n            user_tune_config[\"tune\"][\"scheduler\"][\"params\"][\"grace_period\"] = 1\n            user_tune_config[\"tune\"][\"step_size\"] = 1\n        elif user_tune_config[\"tune\"][\"scheduler\"][\"name\"] == \"FIFOScheduler\":\n            user_tune_config[\"tune\"][\"run_params\"][\"stop\"][\"training_iteration\"] = 1\n\n        if initial_weights_path is not None:\n            user_tune_config[\"model_params\"][\"initial_weights\"] = os.path.abspath(initial_weights_path)\n\n        yaml.dump(user_tune_config, new_conf)\n\n    # Process CSV data\n    csv_obj = CsvProcessing(initialized_experiment_class, data_path)\n    downsampled_csv = \"downsampled.csv\"\n\n    if \"split\" not in csv_obj.check_and_get_categories():\n        config_default = {\"name\": \"RandomSplitter\", \"params\": {\"split\": [0.5, 0.5, 0.0]}}\n        csv_obj.add_split(config_default)\n\n    csv_obj.save(downsampled_csv)\n\n    # Initialize ray\n    object_store_mem, mem = memory_split_for_ray_init(memory)\n    ray_results_dirpath = None if ray_results_dirpath is None else os.path.abspath(ray_results_dirpath)\n\n    # Create and run learner\n    learner = StimulusTuneWrapper(\n        updated_tune_conf,\n        model_class,\n        downsampled_csv,\n        initialized_experiment_class,\n        max_gpus=gpus,\n        max_cpus=cpus,\n        max_object_store_mem=object_store_mem,\n        max_mem=mem,\n        ray_results_dir=ray_results_dirpath,\n        _debug=debug_mode,\n    )\n\n    grid_results = learner.tune()\n\n    # Check results\n    logger = logging.getLogger(__name__)\n    for i, result in enumerate(grid_results):\n        if not result.error:\n            logger.info(\"Trial %d finished successfully with metrics %s.\", i, result.metrics)\n        else:\n            raise TypeError(f\"Trial {i} failed with error {result.error}.\")\n</code></pre>"},{"location":"reference/stimulus/cli/check_model/#stimulus.cli.check_model.run","title":"run","text":"<pre><code>run() -&gt; None\n</code></pre> <p>Run the model checking script.</p> Source code in <code>src/stimulus/cli/check_model.py</code> <pre><code>def run() -&gt; None:\n    \"\"\"Run the model checking script.\"\"\"\n    args = get_args()\n    main(\n        args.data,\n        args.model,\n        args.experiment,\n        args.config,\n        args.initial_weights,\n        args.gpus,\n        args.cpus,\n        args.memory,\n        args.num_samples,\n        args.ray_results_dirpath,\n        debug_mode=args.debug_mode,\n    )\n</code></pre>"},{"location":"reference/stimulus/cli/predict/","title":"stimulus.cli.predict","text":""},{"location":"reference/stimulus/cli/predict/#stimulus.cli.predict","title":"predict","text":"<p>CLI module for model prediction on datasets.</p> <p>Functions:</p> <ul> <li> <code>add_meta_info</code>             \u2013              <p>Add metadata columns to predictions/labels dictionary.</p> </li> <li> <code>get_args</code>             \u2013              <p>Parse command line arguments.</p> </li> <li> <code>get_batch_size</code>             \u2013              <p>Get batch size from model config.</p> </li> <li> <code>get_meta_keys</code>             \u2013              <p>Extract metadata column keys.</p> </li> <li> <code>load_model</code>             \u2013              <p>Load model with hyperparameters and weights.</p> </li> <li> <code>main</code>             \u2013              <p>Run model prediction pipeline.</p> </li> <li> <code>parse_y_keys</code>             \u2013              <p>Parse dictionary keys to match input data format.</p> </li> <li> <code>run</code>             \u2013              <p>Execute model prediction pipeline.</p> </li> </ul>"},{"location":"reference/stimulus/cli/predict/#stimulus.cli.predict.add_meta_info","title":"add_meta_info","text":"<pre><code>add_meta_info(\n    data: DataFrame, y: dict[str, Any]\n) -&gt; dict[str, Any]\n</code></pre> <p>Add metadata columns to predictions/labels dictionary.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>DataFrame</code>)           \u2013            <p>Input DataFrame with metadata.</p> </li> <li> <code>y</code>               (<code>dict[str, Any]</code>)           \u2013            <p>Dictionary of predictions/labels.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Updated dictionary with metadata.</p> </li> </ul> Source code in <code>src/stimulus/cli/predict.py</code> <pre><code>def add_meta_info(data: pl.DataFrame, y: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Add metadata columns to predictions/labels dictionary.\n\n    Args:\n        data: Input DataFrame with metadata.\n        y: Dictionary of predictions/labels.\n\n    Returns:\n        Updated dictionary with metadata.\n    \"\"\"\n    keys = get_meta_keys(data.columns)\n    for key in keys:\n        y[key] = data[key].to_list()\n    return y\n</code></pre>"},{"location":"reference/stimulus/cli/predict/#stimulus.cli.predict.get_args","title":"get_args","text":"<pre><code>get_args() -&gt; Namespace\n</code></pre> <p>Parse command line arguments.</p> <p>Returns:</p> <ul> <li> <code>Namespace</code>           \u2013            <p>Parsed command line arguments.</p> </li> </ul> Source code in <code>src/stimulus/cli/predict.py</code> <pre><code>def get_args() -&gt; argparse.Namespace:\n    \"\"\"Parse command line arguments.\n\n    Returns:\n        Parsed command line arguments.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Predict model outputs on a dataset.\")\n    parser.add_argument(\"-m\", \"--model\", type=str, required=True, metavar=\"FILE\", help=\"Path to model .py file.\")\n    parser.add_argument(\"-w\", \"--weight\", type=str, required=True, metavar=\"FILE\", help=\"Path to model weights file.\")\n    parser.add_argument(\n        \"-mc\",\n        \"--model_config\",\n        type=str,\n        required=True,\n        metavar=\"FILE\",\n        help=\"Path to tune config file with model hyperparameters.\",\n    )\n    parser.add_argument(\n        \"-ec\",\n        \"--experiment_config\",\n        type=str,\n        required=True,\n        metavar=\"FILE\",\n        help=\"Path to experiment config for data modification.\",\n    )\n    parser.add_argument(\"-d\", \"--data\", type=str, required=True, metavar=\"FILE\", help=\"Path to input data.\")\n    parser.add_argument(\"-o\", \"--output\", type=str, required=True, metavar=\"FILE\", help=\"Path for output predictions.\")\n    parser.add_argument(\"--split\", type=int, help=\"Data split to use (default: None).\")\n    parser.add_argument(\"--return_labels\", action=\"store_true\", help=\"Include labels with predictions.\")\n\n    return parser.parse_args()\n</code></pre>"},{"location":"reference/stimulus/cli/predict/#stimulus.cli.predict.get_batch_size","title":"get_batch_size","text":"<pre><code>get_batch_size(mconfig: dict[str, Any]) -&gt; int\n</code></pre> <p>Get batch size from model config.</p> <p>Parameters:</p> <ul> <li> <code>mconfig</code>               (<code>dict[str, Any]</code>)           \u2013            <p>Model configuration dictionary.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>Batch size to use for predictions.</p> </li> </ul> Source code in <code>src/stimulus/cli/predict.py</code> <pre><code>def get_batch_size(mconfig: dict[str, Any]) -&gt; int:\n    \"\"\"Get batch size from model config.\n\n    Args:\n        mconfig: Model configuration dictionary.\n\n    Returns:\n        Batch size to use for predictions.\n    \"\"\"\n    default_batch_size = 256\n    if \"data_params\" in mconfig and \"batch_size\" in mconfig[\"data_params\"]:\n        return mconfig[\"data_params\"][\"batch_size\"]\n    return default_batch_size\n</code></pre>"},{"location":"reference/stimulus/cli/predict/#stimulus.cli.predict.get_meta_keys","title":"get_meta_keys","text":"<pre><code>get_meta_keys(names: Sequence[str]) -&gt; list[str]\n</code></pre> <p>Extract metadata column keys.</p> <p>Parameters:</p> <ul> <li> <code>names</code>               (<code>Sequence[str]</code>)           \u2013            <p>List of column names.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>List of metadata column keys.</p> </li> </ul> Source code in <code>src/stimulus/cli/predict.py</code> <pre><code>def get_meta_keys(names: Sequence[str]) -&gt; list[str]:\n    \"\"\"Extract metadata column keys.\n\n    Args:\n        names: List of column names.\n\n    Returns:\n        List of metadata column keys.\n    \"\"\"\n    return [name for name in names if name.split(\":\")[1] == \"meta\"]\n</code></pre>"},{"location":"reference/stimulus/cli/predict/#stimulus.cli.predict.load_model","title":"load_model","text":"<pre><code>load_model(\n    model_class: Any,\n    weight_path: str,\n    mconfig: dict[str, Any],\n) -&gt; Any\n</code></pre> <p>Load model with hyperparameters and weights.</p> <p>Parameters:</p> <ul> <li> <code>model_class</code>               (<code>Any</code>)           \u2013            <p>Model class to instantiate.</p> </li> <li> <code>weight_path</code>               (<code>str</code>)           \u2013            <p>Path to model weights.</p> </li> <li> <code>mconfig</code>               (<code>dict[str, Any]</code>)           \u2013            <p>Model configuration dictionary.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>Loaded model instance.</p> </li> </ul> Source code in <code>src/stimulus/cli/predict.py</code> <pre><code>def load_model(model_class: Any, weight_path: str, mconfig: dict[str, Any]) -&gt; Any:\n    \"\"\"Load model with hyperparameters and weights.\n\n    Args:\n        model_class: Model class to instantiate.\n        weight_path: Path to model weights.\n        mconfig: Model configuration dictionary.\n\n    Returns:\n        Loaded model instance.\n    \"\"\"\n    hyperparameters = mconfig[\"model_params\"]\n    model = model_class(**hyperparameters)\n    model.load_state_dict(torch.load(weight_path))\n    return model\n</code></pre>"},{"location":"reference/stimulus/cli/predict/#stimulus.cli.predict.main","title":"main","text":"<pre><code>main(\n    model_path: str,\n    weight_path: str,\n    mconfig_path: str,\n    econfig_path: str,\n    data_path: str,\n    output: str,\n    *,\n    return_labels: bool = False,\n    split: int | None = None\n) -&gt; None\n</code></pre> <p>Run model prediction pipeline.</p> <p>Parameters:</p> <ul> <li> <code>model_path</code>               (<code>str</code>)           \u2013            <p>Path to model file.</p> </li> <li> <code>weight_path</code>               (<code>str</code>)           \u2013            <p>Path to model weights.</p> </li> <li> <code>mconfig_path</code>               (<code>str</code>)           \u2013            <p>Path to model config.</p> </li> <li> <code>econfig_path</code>               (<code>str</code>)           \u2013            <p>Path to experiment config.</p> </li> <li> <code>data_path</code>               (<code>str</code>)           \u2013            <p>Path to input data.</p> </li> <li> <code>output</code>               (<code>str</code>)           \u2013            <p>Path for output predictions.</p> </li> <li> <code>return_labels</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to include labels.</p> </li> <li> <code>split</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Data split to use.</p> </li> </ul> Source code in <code>src/stimulus/cli/predict.py</code> <pre><code>def main(\n    model_path: str,\n    weight_path: str,\n    mconfig_path: str,\n    econfig_path: str,\n    data_path: str,\n    output: str,\n    *,\n    return_labels: bool = False,\n    split: int | None = None,\n) -&gt; None:\n    \"\"\"Run model prediction pipeline.\n\n    Args:\n        model_path: Path to model file.\n        weight_path: Path to model weights.\n        mconfig_path: Path to model config.\n        econfig_path: Path to experiment config.\n        data_path: Path to input data.\n        output: Path for output predictions.\n        return_labels: Whether to include labels.\n        split: Data split to use.\n    \"\"\"\n    with open(mconfig_path) as in_json:\n        mconfig = json.load(in_json)\n\n    model_class = import_class_from_file(model_path)\n    model = load_model(model_class, weight_path, mconfig)\n\n    with open(econfig_path) as in_json:\n        experiment_name = json.load(in_json)[\"experiment\"]\n    initialized_experiment_class = get_experiment(experiment_name)\n\n    dataloader = DataLoader(\n        TorchDataset(data_path, initialized_experiment_class, split=split),\n        batch_size=get_batch_size(mconfig),\n        shuffle=False,\n    )\n\n    predictor = PredictWrapper(model, dataloader)\n    out = predictor.predict(return_labels=return_labels)\n    y_pred, y_true = out if return_labels else (out, {})\n\n    y_pred = {k: v.tolist() for k, v in y_pred.items()}\n    y_true = {k: v.tolist() for k, v in y_true.items()}\n\n    data = pl.read_csv(data_path)\n    y_pred = parse_y_keys(y_pred, data, y_type=\"pred\")\n    y_true = parse_y_keys(y_true, data, y_type=\"label\")\n\n    y = {**y_pred, **y_true}\n    y = add_meta_info(data, y)\n    df = pl.from_dict(y)\n    df.write_csv(output)\n</code></pre>"},{"location":"reference/stimulus/cli/predict/#stimulus.cli.predict.parse_y_keys","title":"parse_y_keys","text":"<pre><code>parse_y_keys(\n    y: dict[str, Any], data: DataFrame, y_type: str = \"pred\"\n) -&gt; dict[str, Any]\n</code></pre> <p>Parse dictionary keys to match input data format.</p> <p>Parameters:</p> <ul> <li> <code>y</code>               (<code>dict[str, Any]</code>)           \u2013            <p>Dictionary of predictions or labels.</p> </li> <li> <code>data</code>               (<code>DataFrame</code>)           \u2013            <p>Input DataFrame.</p> </li> <li> <code>y_type</code>               (<code>str</code>, default:                   <code>'pred'</code> )           \u2013            <p>Type of values ('pred' or 'label').</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Dictionary with updated keys.</p> </li> </ul> Source code in <code>src/stimulus/cli/predict.py</code> <pre><code>def parse_y_keys(y: dict[str, Any], data: pl.DataFrame, y_type: str = \"pred\") -&gt; dict[str, Any]:\n    \"\"\"Parse dictionary keys to match input data format.\n\n    Args:\n        y: Dictionary of predictions or labels.\n        data: Input DataFrame.\n        y_type: Type of values ('pred' or 'label').\n\n    Returns:\n        Dictionary with updated keys.\n    \"\"\"\n    if not y:\n        return y\n\n    parsed_y = {}\n    for k1, v1 in y.items():\n        for k2 in data.columns:\n            if k1 == k2.split(\":\")[0]:\n                new_key = f\"{k1}:{y_type}:{k2.split(':')[2]}\"\n                parsed_y[new_key] = v1\n\n    return parsed_y\n</code></pre>"},{"location":"reference/stimulus/cli/predict/#stimulus.cli.predict.run","title":"run","text":"<pre><code>run() -&gt; None\n</code></pre> <p>Execute model prediction pipeline.</p> Source code in <code>src/stimulus/cli/predict.py</code> <pre><code>def run() -&gt; None:\n    \"\"\"Execute model prediction pipeline.\"\"\"\n    args = get_args()\n    main(\n        args.model,\n        args.weight,\n        args.model_config,\n        args.experiment_config,\n        args.data,\n        args.output,\n        return_labels=args.return_labels,\n        split=args.split,\n    )\n</code></pre>"},{"location":"reference/stimulus/cli/shuffle_csv/","title":"stimulus.cli.shuffle_csv","text":""},{"location":"reference/stimulus/cli/shuffle_csv/#stimulus.cli.shuffle_csv","title":"shuffle_csv","text":"<p>CLI module for shuffling CSV data files.</p> <p>Functions:</p> <ul> <li> <code>get_args</code>             \u2013              <p>Get the arguments when using from the commandline.</p> </li> <li> <code>main</code>             \u2013              <p>Shuffle the data and split it according to the default split method.</p> </li> <li> <code>run</code>             \u2013              <p>Run the CSV shuffling script.</p> </li> </ul>"},{"location":"reference/stimulus/cli/shuffle_csv/#stimulus.cli.shuffle_csv.get_args","title":"get_args","text":"<pre><code>get_args() -&gt; Namespace\n</code></pre> <p>Get the arguments when using from the commandline.</p> <p>Returns:</p> <ul> <li> <code>Namespace</code>           \u2013            <p>Parsed command line arguments.</p> </li> </ul> Source code in <code>src/stimulus/cli/shuffle_csv.py</code> <pre><code>def get_args() -&gt; argparse.Namespace:\n    \"\"\"Get the arguments when using from the commandline.\n\n    Returns:\n        Parsed command line arguments.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Shuffle rows in a CSV data file.\")\n    parser.add_argument(\n        \"-c\",\n        \"--csv\",\n        type=str,\n        required=True,\n        metavar=\"FILE\",\n        help=\"The file path for the csv containing all data\",\n    )\n    parser.add_argument(\n        \"-y\",\n        \"--yaml\",\n        type=str,\n        required=True,\n        metavar=\"FILE\",\n        help=\"The YAML config file that hold all parameter info\",\n    )\n    parser.add_argument(\n        \"-o\",\n        \"--output\",\n        type=str,\n        required=True,\n        metavar=\"FILE\",\n        help=\"The output file path to write the noised csv\",\n    )\n\n    return parser.parse_args()\n</code></pre>"},{"location":"reference/stimulus/cli/shuffle_csv/#stimulus.cli.shuffle_csv.main","title":"main","text":"<pre><code>main(\n    data_csv: str, config_yaml: str, out_path: str\n) -&gt; None\n</code></pre> <p>Shuffle the data and split it according to the default split method.</p> <p>Parameters:</p> <ul> <li> <code>data_csv</code>               (<code>str</code>)           \u2013            <p>Path to input CSV file.</p> </li> <li> <code>config_yaml</code>               (<code>str</code>)           \u2013            <p>Path to config YAML file.</p> </li> <li> <code>out_path</code>               (<code>str</code>)           \u2013            <p>Path to output shuffled CSV.</p> </li> </ul> <p>TODO major changes when this is going to select a given shuffle method and integration with split.</p> Source code in <code>src/stimulus/cli/shuffle_csv.py</code> <pre><code>def main(data_csv: str, config_yaml: str, out_path: str) -&gt; None:\n    \"\"\"Shuffle the data and split it according to the default split method.\n\n    Args:\n        data_csv: Path to input CSV file.\n        config_yaml: Path to config YAML file.\n        out_path: Path to output shuffled CSV.\n\n    TODO major changes when this is going to select a given shuffle method and integration with split.\n    \"\"\"\n    # create a DatasetProcessor object from the config and the csv\n    processor = DatasetProcessor(config_path=config_yaml, csv_path=data_csv)\n\n    # shuffle the data with a default seed. TODO get the seed for the config if and when that is going to be set there.\n    processor.shuffle_labels(seed=42)\n\n    # save the modified csv\n    processor.save(out_path)\n</code></pre>"},{"location":"reference/stimulus/cli/shuffle_csv/#stimulus.cli.shuffle_csv.run","title":"run","text":"<pre><code>run() -&gt; None\n</code></pre> <p>Run the CSV shuffling script.</p> Source code in <code>src/stimulus/cli/shuffle_csv.py</code> <pre><code>def run() -&gt; None:\n    \"\"\"Run the CSV shuffling script.\"\"\"\n    args = get_args()\n    main(args.csv, args.yaml, args.output)\n</code></pre>"},{"location":"reference/stimulus/cli/split_csv/","title":"stimulus.cli.split_csv","text":""},{"location":"reference/stimulus/cli/split_csv/#stimulus.cli.split_csv","title":"split_csv","text":"<p>CLI module for splitting CSV data files.</p> <p>Functions:</p> <ul> <li> <code>get_args</code>             \u2013              <p>Get the arguments when using from the commandline.</p> </li> <li> <code>main</code>             \u2013              <p>Connect CSV and YAML configuration and handle sanity checks.</p> </li> <li> <code>run</code>             \u2013              <p>Run the CSV splitting script.</p> </li> </ul>"},{"location":"reference/stimulus/cli/split_csv/#stimulus.cli.split_csv.get_args","title":"get_args","text":"<pre><code>get_args() -&gt; Namespace\n</code></pre> <p>Get the arguments when using from the commandline.</p> Source code in <code>src/stimulus/cli/split_csv.py</code> <pre><code>def get_args() -&gt; argparse.Namespace:\n    \"\"\"Get the arguments when using from the commandline.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Split a CSV data file.\")\n    parser.add_argument(\n        \"-c\",\n        \"--csv\",\n        type=str,\n        required=True,\n        metavar=\"FILE\",\n        help=\"The file path for the csv containing all data\",\n    )\n    parser.add_argument(\n        \"-y\",\n        \"--yaml\",\n        type=str,\n        required=True,\n        metavar=\"FILE\",\n        help=\"The YAML config file that hold all parameter info\",\n    )\n    parser.add_argument(\n        \"-o\",\n        \"--output\",\n        type=str,\n        required=True,\n        metavar=\"FILE\",\n        help=\"The output file path to write the noised csv\",\n    )\n    parser.add_argument(\n        \"-f\",\n        \"--force\",\n        type=bool,\n        required=False,\n        default=False,\n        help=\"Overwrite the split column if it already exists in the csv\",\n    )\n\n    return parser.parse_args()\n</code></pre>"},{"location":"reference/stimulus/cli/split_csv/#stimulus.cli.split_csv.main","title":"main","text":"<pre><code>main(\n    data_csv: str,\n    config_yaml: str,\n    out_path: str,\n    *,\n    force: bool = False\n) -&gt; None\n</code></pre> <p>Connect CSV and YAML configuration and handle sanity checks.</p> <p>Parameters:</p> <ul> <li> <code>data_csv</code>               (<code>str</code>)           \u2013            <p>Path to input CSV file.</p> </li> <li> <code>config_yaml</code>               (<code>str</code>)           \u2013            <p>Path to config YAML file.</p> </li> <li> <code>out_path</code>               (<code>str</code>)           \u2013            <p>Path to output split CSV.</p> </li> <li> <code>force</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Overwrite the split column if it already exists in the CSV.</p> </li> </ul> Source code in <code>src/stimulus/cli/split_csv.py</code> <pre><code>def main(data_csv: str, config_yaml: str, out_path: str, *, force: bool = False) -&gt; None:\n    \"\"\"Connect CSV and YAML configuration and handle sanity checks.\n\n    Args:\n        data_csv: Path to input CSV file.\n        config_yaml: Path to config YAML file.\n        out_path: Path to output split CSV.\n        force: Overwrite the split column if it already exists in the CSV.\n    \"\"\"\n    # create a DatasetProcessor object from the config and the csv\n    processor = DatasetProcessor(config_path=config_yaml, csv_path=data_csv)\n\n    # create a split manager from the config\n    split_config = processor.dataset_manager.config.split\n    with open(config_yaml) as f:\n        yaml_config = YamlSubConfigDict(**yaml.safe_load(f))\n    split_loader = SplitLoader(seed=yaml_config.global_params.seed)\n    split_loader.initialize_splitter_from_config(split_config)\n    split_manager = SplitManager(split_loader)\n\n    # apply the split method to the data\n    processor.add_split(split_manager=split_manager, force=force)\n\n    # save the modified csv\n    processor.save(out_path)\n</code></pre>"},{"location":"reference/stimulus/cli/split_csv/#stimulus.cli.split_csv.run","title":"run","text":"<pre><code>run() -&gt; None\n</code></pre> <p>Run the CSV splitting script.</p> Source code in <code>src/stimulus/cli/split_csv.py</code> <pre><code>def run() -&gt; None:\n    \"\"\"Run the CSV splitting script.\"\"\"\n    args = get_args()\n    main(args.csv, args.json, args.output, force=args.force)\n</code></pre>"},{"location":"reference/stimulus/cli/split_yaml/","title":"stimulus.cli.split_yaml","text":""},{"location":"reference/stimulus/cli/split_yaml/#stimulus.cli.split_yaml","title":"split_yaml","text":"<p>CLI module for splitting YAML configuration files.</p> <p>This module provides functionality to split a single YAML configuration file into multiple YAML files, each containing a specific combination of data transformations and splits. The resulting YAML files can be used as input configurations for the stimulus package.</p> <p>Functions:</p> <ul> <li> <code>get_args</code>             \u2013              <p>Get the arguments when using from the command line.</p> </li> <li> <code>main</code>             \u2013              <p>Reads a YAML config file and generates all possible data configurations.</p> </li> </ul>"},{"location":"reference/stimulus/cli/split_yaml/#stimulus.cli.split_yaml.get_args","title":"get_args","text":"<pre><code>get_args() -&gt; Namespace\n</code></pre> <p>Get the arguments when using from the command line.</p> Source code in <code>src/stimulus/cli/split_yaml.py</code> <pre><code>def get_args() -&gt; argparse.Namespace:\n    \"\"\"Get the arguments when using from the command line.\"\"\"\n    parser = argparse.ArgumentParser(description=\"\")\n    parser.add_argument(\n        \"-j\",\n        \"--yaml\",\n        type=str,\n        required=True,\n        metavar=\"FILE\",\n        help=\"The YAML config file that hold all transform - split - parameter info\",\n    )\n    parser.add_argument(\n        \"-d\",\n        \"--out_dir\",\n        type=str,\n        required=False,\n        nargs=\"?\",\n        const=\"./\",\n        default=\"./\",\n        metavar=\"DIR\",\n        help=\"The output dir where all the YAMLs are written to. Output YAML will be called split-#[number].yaml transform-#[number].yaml. Default -&gt; ./\",\n    )\n\n    return parser.parse_args()\n</code></pre>"},{"location":"reference/stimulus/cli/split_yaml/#stimulus.cli.split_yaml.main","title":"main","text":"<pre><code>main(config_yaml: str, out_dir_path: str) -&gt; None\n</code></pre> <p>Reads a YAML config file and generates all possible data configurations.</p> <p>This script reads a YAML with a defined structure and creates all the YAML files ready to be passed to the stimulus package.</p> <p>The structure of the YAML is described here -&gt; TODO paste here link to documentation. This YAML and it's structure summarize how to generate all the transform - split and respective parameter combinations. Each resulting YAML will hold only one combination of the above three things.</p> <p>This script will always generate at least one YAML file that represent the combination that does not touch the data (no transform) and uses the default split behavior.</p> Source code in <code>src/stimulus/cli/split_yaml.py</code> <pre><code>def main(config_yaml: str, out_dir_path: str) -&gt; None:\n    \"\"\"Reads a YAML config file and generates all possible data configurations.\n\n    This script reads a YAML with a defined structure and creates all the YAML files ready to be passed to\n    the stimulus package.\n\n    The structure of the YAML is described here -&gt; TODO paste here link to documentation.\n    This YAML and it's structure summarize how to generate all the transform - split and respective parameter combinations.\n    Each resulting YAML will hold only one combination of the above three things.\n\n    This script will always generate at least one YAML file that represent the combination that does not touch the data (no transform)\n    and uses the default split behavior.\n    \"\"\"\n    # read the yaml experiment config and load it to dictionary\n    yaml_config: dict[str, Any] = {}\n    with open(config_yaml) as conf_file:\n        yaml_config = yaml.safe_load(conf_file)\n\n    yaml_config_dict: YamlConfigDict = YamlConfigDict(**yaml_config)\n    # check if the yaml schema is correct\n    check_yaml_schema(yaml_config_dict)\n\n    # generate all the YAML configs\n    data_configs = generate_data_configs(yaml_config_dict)\n\n    # dump all the YAML configs into files\n    dump_yaml_list_into_files(data_configs, out_dir_path, \"test\")\n</code></pre>"},{"location":"reference/stimulus/cli/transform_csv/","title":"stimulus.cli.transform_csv","text":""},{"location":"reference/stimulus/cli/transform_csv/#stimulus.cli.transform_csv","title":"transform_csv","text":"<p>CLI module for transforming CSV data files.</p> <p>Functions:</p> <ul> <li> <code>get_args</code>             \u2013              <p>Get the arguments when using from the commandline.</p> </li> <li> <code>main</code>             \u2013              <p>Connect CSV and YAML configuration and handle sanity checks.</p> </li> <li> <code>run</code>             \u2013              <p>Run the CSV transformation script.</p> </li> </ul>"},{"location":"reference/stimulus/cli/transform_csv/#stimulus.cli.transform_csv.get_args","title":"get_args","text":"<pre><code>get_args() -&gt; Namespace\n</code></pre> <p>Get the arguments when using from the commandline.</p> Source code in <code>src/stimulus/cli/transform_csv.py</code> <pre><code>def get_args() -&gt; argparse.Namespace:\n    \"\"\"Get the arguments when using from the commandline.\"\"\"\n    parser = argparse.ArgumentParser(description=\"CLI for transforming CSV data files using YAML configuration.\")\n    parser.add_argument(\n        \"-c\",\n        \"--csv\",\n        type=str,\n        required=True,\n        metavar=\"FILE\",\n        help=\"The file path for the csv containing all data\",\n    )\n    parser.add_argument(\n        \"-y\",\n        \"--yaml\",\n        type=str,\n        required=True,\n        metavar=\"FILE\",\n        help=\"The YAML config file that holds all parameter info\",\n    )\n    parser.add_argument(\n        \"-o\",\n        \"--output\",\n        type=str,\n        required=True,\n        metavar=\"FILE\",\n        help=\"The output file path to write the noised csv\",\n    )\n\n    return parser.parse_args()\n</code></pre>"},{"location":"reference/stimulus/cli/transform_csv/#stimulus.cli.transform_csv.main","title":"main","text":"<pre><code>main(\n    data_csv: str, config_yaml: str, out_path: str\n) -&gt; None\n</code></pre> <p>Connect CSV and YAML configuration and handle sanity checks.</p> <p>This launcher will be the connection between the csv and one YAML configuration. It should also handle some sanity checks.</p> Source code in <code>src/stimulus/cli/transform_csv.py</code> <pre><code>def main(data_csv: str, config_yaml: str, out_path: str) -&gt; None:\n    \"\"\"Connect CSV and YAML configuration and handle sanity checks.\n\n    This launcher will be the connection between the csv and one YAML configuration.\n    It should also handle some sanity checks.\n    \"\"\"\n    # initialize the csv processing class, it open and reads the csv in automatic\n    processor = DatasetProcessor(config_path=config_yaml, csv_path=data_csv)\n\n    # initialize the transform manager\n    transform_config = processor.dataset_manager.config.transforms\n    with open(config_yaml) as f:\n        yaml_config = YamlSubConfigDict(**yaml.safe_load(f))\n    transform_loader = TransformLoader(seed=yaml_config.global_params.seed)\n    transform_loader.initialize_column_data_transformers_from_config(transform_config)\n    transform_manager = TransformManager(transform_loader)\n\n    # apply the transformations to the data\n    processor.apply_transformation_group(transform_manager)\n\n    # write the transformed data to a new csv\n    processor.save(out_path)\n</code></pre>"},{"location":"reference/stimulus/cli/transform_csv/#stimulus.cli.transform_csv.run","title":"run","text":"<pre><code>run() -&gt; None\n</code></pre> <p>Run the CSV transformation script.</p> Source code in <code>src/stimulus/cli/transform_csv.py</code> <pre><code>def run() -&gt; None:\n    \"\"\"Run the CSV transformation script.\"\"\"\n    args = get_args()\n    main(args.csv, args.yaml, args.output)\n</code></pre>"},{"location":"reference/stimulus/cli/tuning/","title":"stimulus.cli.tuning","text":""},{"location":"reference/stimulus/cli/tuning/#stimulus.cli.tuning","title":"tuning","text":"<p>CLI module for tuning model hyperparameters using Ray Tune.</p> <p>This module provides functionality to tune hyperparameters of machine learning models using Ray Tune. It supports configuring resources like GPUs/CPUs, saving best models and metrics, and debugging capabilities.</p> <p>Functions:</p> <ul> <li> <code>get_args</code>             \u2013              <p>Get the arguments when using from the commandline.</p> </li> <li> <code>main</code>             \u2013              <p>This launcher use ray tune to find the best hyperparameters for a given model.</p> </li> <li> <code>run</code>             \u2013              <p>Run the model tuning script.</p> </li> </ul>"},{"location":"reference/stimulus/cli/tuning/#stimulus.cli.tuning.get_args","title":"get_args","text":"<pre><code>get_args() -&gt; Namespace\n</code></pre> <p>Get the arguments when using from the commandline.</p> Source code in <code>src/stimulus/cli/tuning.py</code> <pre><code>def get_args() -&gt; argparse.Namespace:\n    \"\"\"Get the arguments when using from the commandline.\"\"\"\n    parser = argparse.ArgumentParser(description=\"\")\n    parser.add_argument(\n        \"-c\",\n        \"--config\",\n        type=str,\n        required=True,\n        metavar=\"FILE\",\n        help=\"The file path for the config file\",\n    )\n    parser.add_argument(\"-m\", \"--model\", type=str, required=True, metavar=\"FILE\", help=\"The model file\")\n    parser.add_argument(\"-d\", \"--data\", type=str, required=True, metavar=\"FILE\", help=\"The data file\")\n    parser.add_argument(\n        \"-e\",\n        \"--experiment_config\",\n        type=str,\n        required=True,\n        metavar=\"FILE\",\n        help=\"The json used to modify the data. Inside it has the experiment name as specified in the experimets.py, this will then be dinamically imported during training. It is necessary to recover how the user specified the encoding of the data. Data is encoded on the fly.\",\n    )\n    parser.add_argument(\n        \"-o\",\n        \"--output\",\n        type=str,\n        required=False,\n        nargs=\"?\",\n        const=\"best_model.pt\",\n        default=\"best_model.pt\",\n        metavar=\"FILE\",\n        help=\"The output file path to write the trained model to\",\n    )\n    parser.add_argument(\n        \"-bc\",\n        \"--best_config\",\n        type=str,\n        required=False,\n        nargs=\"?\",\n        const=\"best_config.json\",\n        default=\"best_config.json\",\n        metavar=\"FILE\",\n        help=\"The path to write the best config to\",\n    )\n    parser.add_argument(\n        \"-bm\",\n        \"--best_metrics\",\n        type=str,\n        required=False,\n        nargs=\"?\",\n        const=\"best_metrics.csv\",\n        default=\"best_metrics.csv\",\n        metavar=\"FILE\",\n        help=\"The path to write the best metrics to\",\n    )\n    parser.add_argument(\n        \"-bo\",\n        \"--best_optimizer\",\n        type=str,\n        required=False,\n        nargs=\"?\",\n        const=\"best_optimizer.pt\",\n        default=\"best_optimizer.pt\",\n        metavar=\"FILE\",\n        help=\"The path to write the best optimizer to\",\n    )\n    parser.add_argument(\n        \"-w\",\n        \"--initial_weights\",\n        type=str,\n        required=False,\n        nargs=\"?\",\n        const=None,\n        default=None,\n        metavar=\"FILE\",\n        help=\"The path to the initial weights. These can be used by the model instead of the random initialization\",\n    )\n    parser.add_argument(\n        \"--gpus\",\n        type=int,\n        required=False,\n        nargs=\"?\",\n        const=None,\n        default=None,\n        metavar=\"NUM_OF_MAX_GPU\",\n        help=\"Use to limit the number of GPUs ray can use. This might be useful on many occasions, especially in a cluster system. The default value is None meaning ray will use all GPUs available. It can be set to 0 to use only CPUs.\",\n    )\n    parser.add_argument(\n        \"--cpus\",\n        type=int,\n        required=False,\n        nargs=\"?\",\n        const=None,\n        default=None,\n        metavar=\"NUM_OF_MAX_CPU\",\n        help=\"Use to limit the number of CPUs ray can use. This might be useful on many occasions, especially in a cluster system. The default value is None meaning ray will use all CPUs available. It can be set to 0 to use only GPUs.\",\n    )\n    parser.add_argument(\n        \"--memory\",\n        type=str,\n        required=False,\n        nargs=\"?\",\n        const=None,\n        default=None,\n        metavar=\"MAX_MEMORY\",\n        help=\"ray can have a limiter on the total memory it can use. This might be useful on many occasions, especially in a cluster system. The default value is None meaning ray will use all memory available.\",\n    )\n    parser.add_argument(\n        \"--ray_results_dirpath\",\n        type=str,\n        required=False,\n        nargs=\"?\",\n        const=None,\n        default=None,\n        metavar=\"DIR_PATH\",\n        help=\"the location where ray_results output dir should be written. if set to None (default) ray will be place it in ~/ray_results \",\n    )\n    parser.add_argument(\n        \"--tune_run_name\",\n        type=str,\n        required=False,\n        nargs=\"?\",\n        const=None,\n        default=None,\n        metavar=\"CUSTOM_RUN_NAME\",\n        help=\"tells ray tune what that the 'experiment_name' aka the given tune_run name should be. This is controlled be the variable name in the RunConfig class of tune. This has two behaviuors: 1 if set the subdir of ray_results is going to be named with this value, 2 the subdir of the above mentioned will also have this value as prefix for the single train dir name. Default None, meaning ray will generate such a name on its own.\",\n    )\n    parser.add_argument(\n        \"--debug_mode\",\n        type=str,\n        required=False,\n        nargs=\"?\",\n        const=False,\n        default=False,\n        metavar=\"DEV\",\n        help=\"activate debug mode for tuning. default false, no debug.\",\n    )\n\n    return parser.parse_args()\n</code></pre>"},{"location":"reference/stimulus/cli/tuning/#stimulus.cli.tuning.main","title":"main","text":"<pre><code>main(\n    config_path: str,\n    model_path: str,\n    data_path: str,\n    experiment_config: str,\n    output: str,\n    best_config_path: str,\n    best_metrics_path: str,\n    best_optimizer_path: str,\n    initial_weights_path: Optional[str] = None,\n    gpus: Optional[int] = None,\n    cpus: Optional[int] = None,\n    memory: Optional[str] = None,\n    ray_results_dirpath: Optional[str] = None,\n    tune_run_name: Optional[str] = None,\n    *,\n    debug_mode: bool = False\n) -&gt; None\n</code></pre> <p>This launcher use ray tune to find the best hyperparameters for a given model.</p> Source code in <code>src/stimulus/cli/tuning.py</code> <pre><code>def main(\n    config_path: str,\n    model_path: str,\n    data_path: str,\n    experiment_config: str,\n    output: str,\n    best_config_path: str,\n    best_metrics_path: str,\n    best_optimizer_path: str,\n    initial_weights_path: Optional[str] = None,\n    gpus: Optional[int] = None,\n    cpus: Optional[int] = None,\n    memory: Optional[str] = None,\n    ray_results_dirpath: Optional[str] = None,\n    tune_run_name: Optional[str] = None,\n    *,\n    debug_mode: bool = False,\n) -&gt; None:\n    \"\"\"This launcher use ray tune to find the best hyperparameters for a given model.\"\"\"\n    # TODO update to yaml the experiment config\n    # load json into dictionary\n    exp_config = {}\n    with open(experiment_config) as in_json:\n        exp_config = json.load(in_json)\n\n    # initialize the experiment class\n    initialized_experiment_class = get_experiment(exp_config[\"experiment\"])\n\n    # import the model correctly but do not initialize it yet, ray_tune does that itself\n    model_class = import_class_from_file(model_path)\n\n    # Update the tune config file. Because if resources are specified for cpu and gpu they are overwritten with what nextflow has otherwise this field is created\n    updated_tune_conf = \"check_model_modified_tune_config.yaml\"\n    with open(config_path) as conf_file, open(updated_tune_conf, \"w\") as new_conf:\n        user_tune_config = yaml.safe_load(conf_file)\n\n        # add initial weights to the config, when provided\n        if initial_weights_path is not None:\n            user_tune_config[\"model_params\"][\"initial_weights\"] = os.path.abspath(initial_weights_path)\n\n        # save to file the new dictionary because StimulusTuneWrapper only takes paths\n        yaml.dump(user_tune_config, new_conf)\n\n    # compute the memory requirements for ray init. Usefull in case ray detects them wrongly. Memory is split in two for ray: for store_object memory and the other actual memory for tuning. The following function takes the total possible usable/allocated memory as a string parameter and return in bytes the values for store_memory (30% as default in ray) and memory (70%).\n    object_store_mem, mem = memory_split_for_ray_init(memory)\n\n    # set ray_result dir ubication. TODO this version of pytorch does not support relative paths, in future maybe good to remove abspath.\n    ray_results_dirpath = None if ray_results_dirpath is None else os.path.abspath(ray_results_dirpath)\n\n    # Create the learner\n    learner = StimulusTuneWrapper(\n        updated_tune_conf,\n        model_class,\n        data_path,\n        initialized_experiment_class,\n        max_gpus=gpus,\n        max_cpus=cpus,\n        max_object_store_mem=object_store_mem,\n        max_mem=mem,\n        ray_results_dir=ray_results_dirpath,\n        tune_run_name=tune_run_name,\n        _debug=debug_mode,\n    )\n\n    # Tune the model and get the tuning results\n    grid_results = learner.tune()\n\n    # parse raytune results\n    results = StimulusTuneParser(grid_results)\n    results.save_best_model(output)\n    results.save_best_config(best_config_path)\n    results.save_best_metrics_dataframe(best_metrics_path)\n    results.save_best_optimizer(best_optimizer_path)\n\n    # debug section. predict the validation data using the best model.\n    if debug_mode:\n        # imitialize the model class with the respective tune parameters from the associated config\n        best_tune_config = results.get_best_config()\n        best_model = model_class(**best_tune_config[\"model_params\"])\n        # get the weights associated to the best model and load them onto the model class\n        best_model.load_state_dict(results.get_best_model())\n        # load the data in a dataloader and then predict them in an ordered manner, aka no shuffle.\n        validation_set = DataLoader(\n            TorchDataset(data_path, initialized_experiment_class, split=1),\n            batch_size=learner.config[\"data_params\"][\"batch_size\"].sample(),\n            shuffle=False,\n        )\n        predictions = PredictWrapper(best_model, validation_set).predict()\n        # write to file the predictions, in the ray result tune specific folder.\n        pred_filename = os.path.join(learner.config[\"tune_run_path\"], \"debug\", \"best_model_val_pred.txt\")\n        # save which was the best model found, the easiest is to get its seed\n        best_model_seed = os.path.join(learner.config[\"tune_run_path\"], \"debug\", \"best_model_seed.txt\")\n        with open(pred_filename, \"w\") as pred_f, open(best_model_seed, \"w\") as seed_f:\n            pred_f.write(str(predictions))\n            seed_f.write(str(best_tune_config[\"ray_worker_seed\"]))\n</code></pre>"},{"location":"reference/stimulus/cli/tuning/#stimulus.cli.tuning.run","title":"run","text":"<pre><code>run() -&gt; None\n</code></pre> <p>Run the model tuning script.</p> Source code in <code>src/stimulus/cli/tuning.py</code> <pre><code>def run() -&gt; None:\n    \"\"\"Run the model tuning script.\"\"\"\n    args = get_args()\n    main(\n        args.config,\n        args.model,\n        args.data,\n        args.experiment_config,\n        args.output,\n        args.best_config,\n        args.best_metrics,\n        args.best_optimizer,\n        args.initial_weights,\n        args.gpus,\n        args.cpus,\n        args.memory,\n        args.ray_results_dirpath,\n        args.tune_run_name,\n        debug_mode=args.debug_mode,\n    )\n</code></pre>"},{"location":"reference/stimulus/data/","title":"stimulus.data","text":""},{"location":"reference/stimulus/data/#stimulus.data","title":"data","text":"<p>Data handling and processing module.</p> <p>This module provides functionality for loading, transforming, and managing data in various formats like CSV. It includes classes and utilities for:</p> <ul> <li>Loading and processing CSV data files</li> <li>Applying data transformations and augmentations</li> <li>Splitting data into train/validation/test sets</li> <li>Converting data into PyTorch datasets</li> </ul> <p>Modules:</p> <ul> <li> <code>data_handlers</code>           \u2013            <p>This module provides classes for handling CSV data files in the STIMULUS format.</p> </li> <li> <code>encoding</code>           \u2013            <p>Encoding package for data transformation.</p> </li> <li> <code>handlertorch</code>           \u2013            <p>This file provides the class API for handling the data in pytorch using the Dataset and Dataloader classes.</p> </li> <li> <code>loaders</code>           \u2013            <p>Loaders serve as interfaces between the CSV master class and custom methods.</p> </li> <li> <code>splitters</code>           \u2013            <p>This package provides splitter classes for splitting data into train, validation, and test sets.</p> </li> <li> <code>transform</code>           \u2013            <p>Transform package for data manipulation.</p> </li> </ul>"},{"location":"reference/stimulus/data/data_handlers/","title":"stimulus.data.data_handlers","text":""},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers","title":"data_handlers","text":"<p>This module provides classes for handling CSV data files in the STIMULUS format.</p> <p>The module contains three main classes: - DatasetHandler: Base class for loading and managing CSV data - DatasetProcessor: Class for preprocessing data with transformations and splits - DatasetLoader: Class for loading processed data for model training</p> <p>The data format consists of: 1. A CSV file containing the raw data 2. A YAML configuration file that defines:    - Column names and their roles (input/label/meta)    - Data types and encoders for each column    - Transformations to apply (noise, augmentation, etc.)    - Split configuration for train/val/test sets</p> <p>The data handling pipeline consists of: 1. Loading raw CSV data according to the YAML config 2. Applying configured transformations 3. Splitting into train/val/test sets based on config 4. Encoding data for model training using specified encoders</p> <p>See titanic.yaml in tests/test_data/titanic/ for an example configuration file format.</p> <p>Classes:</p> <ul> <li> <code>DatasetHandler</code>           \u2013            <p>Main class for handling dataset loading, encoding, transformation and splitting.</p> </li> <li> <code>DatasetLoader</code>           \u2013            <p>Class for loading dataset and passing it to the deep learning model.</p> </li> <li> <code>DatasetManager</code>           \u2013            <p>Class for managing the dataset.</p> </li> <li> <code>DatasetProcessor</code>           \u2013            <p>Class for loading dataset, applying transformations and splitting.</p> </li> <li> <code>EncodeManager</code>           \u2013            <p>Manages the encoding of data columns using configured encoders.</p> </li> <li> <code>SplitManager</code>           \u2013            <p>Class for managing the splitting.</p> </li> <li> <code>TransformManager</code>           \u2013            <p>Class for managing the transformations.</p> </li> </ul>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetHandler","title":"DatasetHandler","text":"<pre><code>DatasetHandler(config_path: str, csv_path: str)\n</code></pre> <p>Main class for handling dataset loading, encoding, transformation and splitting.</p> <p>This class coordinates the interaction between different managers to process CSV datasets according to the provided configuration.</p> <p>Attributes:</p> <ul> <li> <code>encoder_manager</code>               (<code>EncodeManager</code>)           \u2013            <p>Manager for handling data encoding operations.</p> </li> <li> <code>transform_manager</code>               (<code>TransformManager</code>)           \u2013            <p>Manager for handling data transformations.</p> </li> <li> <code>split_manager</code>               (<code>SplitManager</code>)           \u2013            <p>Manager for handling dataset splitting.</p> </li> <li> <code>dataset_manager</code>               (<code>DatasetManager</code>)           \u2013            <p>Manager for organizing dataset columns and config.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>config_path</code>               (<code>str</code>)           \u2013            <p>Path to the dataset configuration file.</p> </li> <li> <code>csv_path</code>               (<code>str</code>)           \u2013            <p>Path to the CSV data file.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>load_csv</code>             \u2013              <p>Load the CSV file into a polars DataFrame.</p> </li> <li> <code>read_csv_header</code>             \u2013              <p>Get the column names from the header of the CSV file.</p> </li> <li> <code>save</code>             \u2013              <p>Saves the data to a csv file.</p> </li> <li> <code>select_columns</code>             \u2013              <p>Select specific columns from the DataFrame and return as a dictionary.</p> </li> </ul> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    csv_path: str,\n) -&gt; None:\n    \"\"\"Initialize the DatasetHandler with required config.\n\n    Args:\n        config_path (str): Path to the dataset configuration file.\n        csv_path (str): Path to the CSV data file.\n    \"\"\"\n    self.dataset_manager = DatasetManager(config_path)\n    self.columns = self.read_csv_header(csv_path)\n    self.data = self.load_csv(csv_path)\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetHandler.load_csv","title":"load_csv","text":"<pre><code>load_csv(csv_path: str) -&gt; DataFrame\n</code></pre> <p>Load the CSV file into a polars DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>csv_path</code>               (<code>str</code>)           \u2013            <p>Path to the CSV file to load.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pl.DataFrame: Polars DataFrame containing the loaded CSV data.</p> </li> </ul> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def load_csv(self, csv_path: str) -&gt; pl.DataFrame:\n    \"\"\"Load the CSV file into a polars DataFrame.\n\n    Args:\n        csv_path (str): Path to the CSV file to load.\n\n    Returns:\n        pl.DataFrame: Polars DataFrame containing the loaded CSV data.\n    \"\"\"\n    return pl.read_csv(csv_path)\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetHandler.read_csv_header","title":"read_csv_header","text":"<pre><code>read_csv_header(csv_path: str) -&gt; list\n</code></pre> <p>Get the column names from the header of the CSV file.</p> <p>Parameters:</p> <ul> <li> <code>csv_path</code>               (<code>str</code>)           \u2013            <p>Path to the CSV file to read headers from.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (              <code>list</code> )          \u2013            <p>List of column names from the CSV header.</p> </li> </ul> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def read_csv_header(self, csv_path: str) -&gt; list:\n    \"\"\"Get the column names from the header of the CSV file.\n\n    Args:\n        csv_path (str): Path to the CSV file to read headers from.\n\n    Returns:\n        list: List of column names from the CSV header.\n    \"\"\"\n    with open(csv_path) as f:\n        return f.readline().strip().split(\",\")\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetHandler.save","title":"save","text":"<pre><code>save(path: str) -&gt; None\n</code></pre> <p>Saves the data to a csv file.</p> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def save(self, path: str) -&gt; None:\n    \"\"\"Saves the data to a csv file.\"\"\"\n    self.data.write_csv(path)\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetHandler.select_columns","title":"select_columns","text":"<pre><code>select_columns(columns: list) -&gt; dict\n</code></pre> <p>Select specific columns from the DataFrame and return as a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>columns</code>               (<code>list</code>)           \u2013            <p>List of column names to select.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict</code> )          \u2013            <p>A dictionary where keys are column names and values are lists containing the column data.</p> </li> </ul> Example <p>handler = DatasetHandler(...) data_dict = handler.select_columns([\"col1\", \"col2\"])</p> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def select_columns(self, columns: list) -&gt; dict:\n    \"\"\"Select specific columns from the DataFrame and return as a dictionary.\n\n    Args:\n        columns (list): List of column names to select.\n\n    Returns:\n        dict: A dictionary where keys are column names and values are lists containing the column data.\n\n    Example:\n        &gt;&gt;&gt; handler = DatasetHandler(...)\n        &gt;&gt;&gt; data_dict = handler.select_columns([\"col1\", \"col2\"])\n        &gt;&gt;&gt; # Returns {'col1': [1, 2, 3], 'col2': [4, 5, 6]}\n    \"\"\"\n    df = self.data.select(columns)\n    return {col: df[col].to_list() for col in columns}\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetHandler.select_columns--returns","title":"Returns","text":""},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetLoader","title":"DatasetLoader","text":"<pre><code>DatasetLoader(\n    config_path: str,\n    csv_path: str,\n    encoder_loader: EncoderLoader,\n    split: Union[int, None] = None,\n)\n</code></pre> <p>               Bases: <code>DatasetHandler</code></p> <p>Class for loading dataset and passing it to the deep learning model.</p> <p>Methods:</p> <ul> <li> <code>get_all_items</code>             \u2013              <p>Get the full dataset as three separate dictionaries for inputs, labels and metadata.</p> </li> <li> <code>get_all_items_and_length</code>             \u2013              <p>Get the full dataset as three separate dictionaries for inputs, labels and metadata, and the length of the data.</p> </li> <li> <code>load_csv</code>             \u2013              <p>Load the CSV file into a polars DataFrame.</p> </li> <li> <code>load_csv_per_split</code>             \u2013              <p>Load the part of csv file that has the specified split value.</p> </li> <li> <code>read_csv_header</code>             \u2013              <p>Get the column names from the header of the CSV file.</p> </li> <li> <code>save</code>             \u2013              <p>Saves the data to a csv file.</p> </li> <li> <code>select_columns</code>             \u2013              <p>Select specific columns from the DataFrame and return as a dictionary.</p> </li> </ul> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    csv_path: str,\n    encoder_loader: loaders.EncoderLoader,\n    split: Union[int, None] = None,\n) -&gt; None:\n    \"\"\"Initialize the DatasetLoader.\"\"\"\n    super().__init__(config_path, csv_path)\n    self.encoder_manager = EncodeManager(encoder_loader)\n    self.data = self.load_csv_per_split(csv_path, split) if split is not None else self.load_csv(csv_path)\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetLoader.get_all_items","title":"get_all_items","text":"<pre><code>get_all_items() -&gt; tuple[dict, dict, dict]\n</code></pre> <p>Get the full dataset as three separate dictionaries for inputs, labels and metadata.</p> <p>Returns:</p> <ul> <li> <code>tuple[dict, dict, dict]</code>           \u2013            <p>tuple[dict, dict, dict]: Three dictionaries containing: - Input dictionary mapping input column names to encoded input data - Label dictionary mapping label column names to encoded label data - Meta dictionary mapping meta column names to meta data</p> </li> </ul> Example <p>handler = DatasetHandler(...) input_dict, label_dict, meta_dict = handler.get_dataset() print(input_dict.keys()) dict_keys(['age', 'fare']) print(label_dict.keys()) dict_keys(['survived']) print(meta_dict.keys()) dict_keys(['passenger_id'])</p> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def get_all_items(self) -&gt; tuple[dict, dict, dict]:\n    \"\"\"Get the full dataset as three separate dictionaries for inputs, labels and metadata.\n\n    Returns:\n        tuple[dict, dict, dict]: Three dictionaries containing:\n            - Input dictionary mapping input column names to encoded input data\n            - Label dictionary mapping label column names to encoded label data\n            - Meta dictionary mapping meta column names to meta data\n\n    Example:\n        &gt;&gt;&gt; handler = DatasetHandler(...)\n        &gt;&gt;&gt; input_dict, label_dict, meta_dict = handler.get_dataset()\n        &gt;&gt;&gt; print(input_dict.keys())\n        dict_keys(['age', 'fare'])\n        &gt;&gt;&gt; print(label_dict.keys())\n        dict_keys(['survived'])\n        &gt;&gt;&gt; print(meta_dict.keys())\n        dict_keys(['passenger_id'])\n    \"\"\"\n    input_columns, label_columns, meta_columns = (\n        self.dataset_manager.column_categories[\"input\"],\n        self.dataset_manager.column_categories[\"label\"],\n        self.dataset_manager.column_categories[\"meta\"],\n    )\n    input_data = self.encoder_manager.encode_dataframe(self.data[input_columns])\n    label_data = self.encoder_manager.encode_dataframe(self.data[label_columns])\n    meta_data = {key: self.data[key].to_list() for key in meta_columns}\n    return input_data, label_data, meta_data\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetLoader.get_all_items_and_length","title":"get_all_items_and_length","text":"<pre><code>get_all_items_and_length() -&gt; (\n    tuple[tuple[dict, dict, dict], int]\n)\n</code></pre> <p>Get the full dataset as three separate dictionaries for inputs, labels and metadata, and the length of the data.</p> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def get_all_items_and_length(self) -&gt; tuple[tuple[dict, dict, dict], int]:\n    \"\"\"Get the full dataset as three separate dictionaries for inputs, labels and metadata, and the length of the data.\"\"\"\n    return self.get_all_items(), len(self.data)\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetLoader.load_csv","title":"load_csv","text":"<pre><code>load_csv(csv_path: str) -&gt; DataFrame\n</code></pre> <p>Load the CSV file into a polars DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>csv_path</code>               (<code>str</code>)           \u2013            <p>Path to the CSV file to load.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pl.DataFrame: Polars DataFrame containing the loaded CSV data.</p> </li> </ul> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def load_csv(self, csv_path: str) -&gt; pl.DataFrame:\n    \"\"\"Load the CSV file into a polars DataFrame.\n\n    Args:\n        csv_path (str): Path to the CSV file to load.\n\n    Returns:\n        pl.DataFrame: Polars DataFrame containing the loaded CSV data.\n    \"\"\"\n    return pl.read_csv(csv_path)\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetLoader.load_csv_per_split","title":"load_csv_per_split","text":"<pre><code>load_csv_per_split(csv_path: str, split: int) -&gt; DataFrame\n</code></pre> <p>Load the part of csv file that has the specified split value.</p> <p>Split is a number that for 0 is train, 1 is validation, 2 is test. This is accessed through the column with category <code>split</code>. Example column name could be <code>split:split:int</code>.</p> <p>NOTE that the aim of having this function is that depending on the training, validation and test scenarios, we are gonna load only the relevant data for it.</p> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def load_csv_per_split(self, csv_path: str, split: int) -&gt; pl.DataFrame:\n    \"\"\"Load the part of csv file that has the specified split value.\n\n    Split is a number that for 0 is train, 1 is validation, 2 is test.\n    This is accessed through the column with category `split`. Example column name could be `split:split:int`.\n\n    NOTE that the aim of having this function is that depending on the training, validation and test scenarios,\n    we are gonna load only the relevant data for it.\n    \"\"\"\n    if \"split\" not in self.columns:\n        raise ValueError(\"The category split is not present in the csv file\")\n    if split not in [0, 1, 2]:\n        raise ValueError(f\"The split value should be 0, 1 or 2. The specified split value is {split}\")\n    return pl.scan_csv(csv_path).filter(pl.col(\"split\") == split).collect()\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetLoader.read_csv_header","title":"read_csv_header","text":"<pre><code>read_csv_header(csv_path: str) -&gt; list\n</code></pre> <p>Get the column names from the header of the CSV file.</p> <p>Parameters:</p> <ul> <li> <code>csv_path</code>               (<code>str</code>)           \u2013            <p>Path to the CSV file to read headers from.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (              <code>list</code> )          \u2013            <p>List of column names from the CSV header.</p> </li> </ul> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def read_csv_header(self, csv_path: str) -&gt; list:\n    \"\"\"Get the column names from the header of the CSV file.\n\n    Args:\n        csv_path (str): Path to the CSV file to read headers from.\n\n    Returns:\n        list: List of column names from the CSV header.\n    \"\"\"\n    with open(csv_path) as f:\n        return f.readline().strip().split(\",\")\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetLoader.save","title":"save","text":"<pre><code>save(path: str) -&gt; None\n</code></pre> <p>Saves the data to a csv file.</p> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def save(self, path: str) -&gt; None:\n    \"\"\"Saves the data to a csv file.\"\"\"\n    self.data.write_csv(path)\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetLoader.select_columns","title":"select_columns","text":"<pre><code>select_columns(columns: list) -&gt; dict\n</code></pre> <p>Select specific columns from the DataFrame and return as a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>columns</code>               (<code>list</code>)           \u2013            <p>List of column names to select.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict</code> )          \u2013            <p>A dictionary where keys are column names and values are lists containing the column data.</p> </li> </ul> Example <p>handler = DatasetHandler(...) data_dict = handler.select_columns([\"col1\", \"col2\"])</p> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def select_columns(self, columns: list) -&gt; dict:\n    \"\"\"Select specific columns from the DataFrame and return as a dictionary.\n\n    Args:\n        columns (list): List of column names to select.\n\n    Returns:\n        dict: A dictionary where keys are column names and values are lists containing the column data.\n\n    Example:\n        &gt;&gt;&gt; handler = DatasetHandler(...)\n        &gt;&gt;&gt; data_dict = handler.select_columns([\"col1\", \"col2\"])\n        &gt;&gt;&gt; # Returns {'col1': [1, 2, 3], 'col2': [4, 5, 6]}\n    \"\"\"\n    df = self.data.select(columns)\n    return {col: df[col].to_list() for col in columns}\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetLoader.select_columns--returns","title":"Returns","text":""},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetManager","title":"DatasetManager","text":"<pre><code>DatasetManager(config_path: str)\n</code></pre> <p>Class for managing the dataset.</p> <p>This class handles loading and organizing dataset configuration from YAML files. It manages column categorization into input, label and meta types based on the config.</p> <p>Attributes:</p> <ul> <li> <code>config</code>               (<code>dict</code>)           \u2013            <p>The loaded configuration dictionary from YAML</p> </li> <li> <code>column_categories</code>               (<code>dict</code>)           \u2013            <p>Dictionary mapping column types to lists of column names</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>_load_config</code>             \u2013              <p>str) -&gt; dict: Loads the config from a YAML file.</p> </li> <li> <code>categorize_columns_by_type</code>             \u2013              <p>Organizes the columns into input, label, meta based on the config.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>categorize_columns_by_type</code>             \u2013              <p>Organizes columns from config into input, label, and meta categories.</p> </li> <li> <code>get_split_columns</code>             \u2013              <p>Get the columns that are used for splitting.</p> </li> <li> <code>get_transform_logic</code>             \u2013              <p>Get the transformation logic.</p> </li> </ul> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n) -&gt; None:\n    \"\"\"Initialize the DatasetManager.\"\"\"\n    self.config = self._load_config(config_path)\n    self.column_categories = self.categorize_columns_by_type()\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetManager.categorize_columns_by_type","title":"categorize_columns_by_type","text":"<pre><code>categorize_columns_by_type() -&gt; dict\n</code></pre> <p>Organizes columns from config into input, label, and meta categories.</p> <p>Reads the column definitions from the config and sorts them into categories based on their column_type field.</p> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing lists of column names for each category: {     \"input\": [\"col1\", \"col2\"],  # Input columns     \"label\": [\"target\"],        # Label/output columns     \"meta\": [\"id\"]     # Metadata columns }</p> </li> </ul> Example <p>manager = DatasetManager(\"config.yaml\") categories = manager.categorize_columns_by_type() print(categories) {     'input': ['hello', 'bonjour'],     'label': ['ciao'],     'meta': [\"id\"] }</p> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def categorize_columns_by_type(self) -&gt; dict:\n    \"\"\"Organizes columns from config into input, label, and meta categories.\n\n    Reads the column definitions from the config and sorts them into categories\n    based on their column_type field.\n\n    Returns:\n        dict: Dictionary containing lists of column names for each category:\n            {\n                \"input\": [\"col1\", \"col2\"],  # Input columns\n                \"label\": [\"target\"],        # Label/output columns\n                \"meta\": [\"id\"]     # Metadata columns\n            }\n\n    Example:\n        &gt;&gt;&gt; manager = DatasetManager(\"config.yaml\")\n        &gt;&gt;&gt; categories = manager.categorize_columns_by_type()\n        &gt;&gt;&gt; print(categories)\n        {\n            'input': ['hello', 'bonjour'],\n            'label': ['ciao'],\n            'meta': [\"id\"]\n        }\n    \"\"\"\n    input_columns = []\n    label_columns = []\n    meta_columns = []\n    for column in self.config.columns:\n        if column.column_type == \"input\":\n            input_columns.append(column.column_name)\n        elif column.column_type == \"label\":\n            label_columns.append(column.column_name)\n        elif column.column_type == \"meta\":\n            meta_columns.append(column.column_name)\n\n    return {\"input\": input_columns, \"label\": label_columns, \"meta\": meta_columns}\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetManager.get_split_columns","title":"get_split_columns","text":"<pre><code>get_split_columns() -&gt; list[str]\n</code></pre> <p>Get the columns that are used for splitting.</p> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def get_split_columns(self) -&gt; list[str]:\n    \"\"\"Get the columns that are used for splitting.\"\"\"\n    return self.config.split.split_input_columns\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetManager.get_transform_logic","title":"get_transform_logic","text":"<pre><code>get_transform_logic() -&gt; dict\n</code></pre> <p>Get the transformation logic.</p> <p>Returns a dictionary in the following structure : {     \"transformation_name\": str,     \"transformations\": list[tuple[str, str, dict]] }</p> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def get_transform_logic(self) -&gt; dict:\n    \"\"\"Get the transformation logic.\n\n    Returns a dictionary in the following structure :\n    {\n        \"transformation_name\": str,\n        \"transformations\": list[tuple[str, str, dict]]\n    }\n    \"\"\"\n    transformation_logic = {\n        \"transformation_name\": self.config.transforms.transformation_name,\n        \"transformations\": [],\n    }\n    for column in self.config.transforms.columns:\n        for transformation in column.transformations:\n            transformation_logic[\"transformations\"].append(\n                (column.column_name, transformation.name, transformation.params),\n            )\n    return transformation_logic\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetProcessor","title":"DatasetProcessor","text":"<pre><code>DatasetProcessor(config_path: str, csv_path: str)\n</code></pre> <p>               Bases: <code>DatasetHandler</code></p> <p>Class for loading dataset, applying transformations and splitting.</p> <p>Methods:</p> <ul> <li> <code>add_split</code>             \u2013              <p>Add a column specifying the train, validation, test splits of the data.</p> </li> <li> <code>apply_transformation_group</code>             \u2013              <p>Apply the transformation group to the data.</p> </li> <li> <code>load_csv</code>             \u2013              <p>Load the CSV file into a polars DataFrame.</p> </li> <li> <code>read_csv_header</code>             \u2013              <p>Get the column names from the header of the CSV file.</p> </li> <li> <code>save</code>             \u2013              <p>Saves the data to a csv file.</p> </li> <li> <code>select_columns</code>             \u2013              <p>Select specific columns from the DataFrame and return as a dictionary.</p> </li> <li> <code>shuffle_labels</code>             \u2013              <p>Shuffles the labels in the data.</p> </li> </ul> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def __init__(self, config_path: str, csv_path: str) -&gt; None:\n    \"\"\"Initialize the DatasetProcessor.\"\"\"\n    super().__init__(config_path, csv_path)\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetProcessor.add_split","title":"add_split","text":"<pre><code>add_split(\n    split_manager: SplitManager, *, force: bool = False\n) -&gt; None\n</code></pre> <p>Add a column specifying the train, validation, test splits of the data.</p> <p>An error exception is raised if the split column is already present in the csv file. This behaviour can be overriden by setting force=True.</p> <p>Parameters:</p> <ul> <li> <code>split_manager</code>               (<code>SplitManager</code>)           \u2013            <p>Manager for handling dataset splitting</p> </li> <li> <code>force</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the split column present in the csv file will be overwritten.</p> </li> </ul> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def add_split(self, split_manager: SplitManager, *, force: bool = False) -&gt; None:\n    \"\"\"Add a column specifying the train, validation, test splits of the data.\n\n    An error exception is raised if the split column is already present in the csv file. This behaviour can be overriden by setting force=True.\n\n    Args:\n        split_manager (SplitManager): Manager for handling dataset splitting\n        force (bool): If True, the split column present in the csv file will be overwritten.\n    \"\"\"\n    if (\"split\" in self.columns) and (not force):\n        raise ValueError(\n            \"The category split is already present in the csv file. If you want to still use this function, set force=True\",\n        )\n    # get relevant split columns from the dataset_manager\n    split_columns = self.dataset_manager.get_split_columns()\n    split_input_data = self.select_columns(split_columns)\n\n    # get the split indices\n    train, validation, test = split_manager.get_split_indices(split_input_data)\n\n    # add the split column to the data\n    split_column = np.full(len(self.data), -1).astype(int)\n    split_column[train] = 0\n    split_column[validation] = 1\n    split_column[test] = 2\n    self.data = self.data.with_columns(pl.Series(\"split\", split_column))\n\n    if \"split\" not in self.columns:\n        self.columns.append(\"split\")\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetProcessor.apply_transformation_group","title":"apply_transformation_group","text":"<pre><code>apply_transformation_group(\n    transform_manager: TransformManager,\n) -&gt; None\n</code></pre> <p>Apply the transformation group to the data.</p> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def apply_transformation_group(self, transform_manager: TransformManager) -&gt; None:\n    \"\"\"Apply the transformation group to the data.\"\"\"\n    for column_name, transform_name, _params in self.dataset_manager.get_transform_logic()[\"transformations\"]:\n        transformed_data, add_row = transform_manager.transform_column(\n            column_name,\n            transform_name,\n            self.data[column_name],\n        )\n        if add_row:\n            new_rows = self.data.with_columns(pl.Series(column_name, transformed_data))\n            self.data = pl.vstack(self.data, new_rows)\n        else:\n            self.data = self.data.with_columns(pl.Series(column_name, transformed_data))\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetProcessor.load_csv","title":"load_csv","text":"<pre><code>load_csv(csv_path: str) -&gt; DataFrame\n</code></pre> <p>Load the CSV file into a polars DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>csv_path</code>               (<code>str</code>)           \u2013            <p>Path to the CSV file to load.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pl.DataFrame: Polars DataFrame containing the loaded CSV data.</p> </li> </ul> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def load_csv(self, csv_path: str) -&gt; pl.DataFrame:\n    \"\"\"Load the CSV file into a polars DataFrame.\n\n    Args:\n        csv_path (str): Path to the CSV file to load.\n\n    Returns:\n        pl.DataFrame: Polars DataFrame containing the loaded CSV data.\n    \"\"\"\n    return pl.read_csv(csv_path)\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetProcessor.read_csv_header","title":"read_csv_header","text":"<pre><code>read_csv_header(csv_path: str) -&gt; list\n</code></pre> <p>Get the column names from the header of the CSV file.</p> <p>Parameters:</p> <ul> <li> <code>csv_path</code>               (<code>str</code>)           \u2013            <p>Path to the CSV file to read headers from.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (              <code>list</code> )          \u2013            <p>List of column names from the CSV header.</p> </li> </ul> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def read_csv_header(self, csv_path: str) -&gt; list:\n    \"\"\"Get the column names from the header of the CSV file.\n\n    Args:\n        csv_path (str): Path to the CSV file to read headers from.\n\n    Returns:\n        list: List of column names from the CSV header.\n    \"\"\"\n    with open(csv_path) as f:\n        return f.readline().strip().split(\",\")\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetProcessor.save","title":"save","text":"<pre><code>save(path: str) -&gt; None\n</code></pre> <p>Saves the data to a csv file.</p> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def save(self, path: str) -&gt; None:\n    \"\"\"Saves the data to a csv file.\"\"\"\n    self.data.write_csv(path)\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetProcessor.select_columns","title":"select_columns","text":"<pre><code>select_columns(columns: list) -&gt; dict\n</code></pre> <p>Select specific columns from the DataFrame and return as a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>columns</code>               (<code>list</code>)           \u2013            <p>List of column names to select.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict</code> )          \u2013            <p>A dictionary where keys are column names and values are lists containing the column data.</p> </li> </ul> Example <p>handler = DatasetHandler(...) data_dict = handler.select_columns([\"col1\", \"col2\"])</p> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def select_columns(self, columns: list) -&gt; dict:\n    \"\"\"Select specific columns from the DataFrame and return as a dictionary.\n\n    Args:\n        columns (list): List of column names to select.\n\n    Returns:\n        dict: A dictionary where keys are column names and values are lists containing the column data.\n\n    Example:\n        &gt;&gt;&gt; handler = DatasetHandler(...)\n        &gt;&gt;&gt; data_dict = handler.select_columns([\"col1\", \"col2\"])\n        &gt;&gt;&gt; # Returns {'col1': [1, 2, 3], 'col2': [4, 5, 6]}\n    \"\"\"\n    df = self.data.select(columns)\n    return {col: df[col].to_list() for col in columns}\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetProcessor.select_columns--returns","title":"Returns","text":""},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetProcessor.shuffle_labels","title":"shuffle_labels","text":"<pre><code>shuffle_labels(seed: Optional[float] = None) -&gt; None\n</code></pre> <p>Shuffles the labels in the data.</p> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def shuffle_labels(self, seed: Optional[float] = None) -&gt; None:\n    \"\"\"Shuffles the labels in the data.\"\"\"\n    # set the np seed\n    np.random.seed(seed)\n\n    label_keys = self.dataset_manager.column_categories[\"label\"]\n    for key in label_keys:\n        self.data = self.data.with_columns(pl.Series(key, np.random.permutation(list(self.data[key]))))\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.EncodeManager","title":"EncodeManager","text":"<pre><code>EncodeManager(encoder_loader: EncoderLoader)\n</code></pre> <p>Manages the encoding of data columns using configured encoders.</p> <p>This class handles encoding of data columns based on the encoders specified in the configuration. It uses an EncoderLoader to get the appropriate encoder for each column and applies the encoding.</p> <p>Attributes:</p> <ul> <li> <code>encoder_loader</code>               (<code>EncoderLoader</code>)           \u2013            <p>Loader that provides encoders based on config.</p> </li> </ul> Example <p>encoder_loader = EncoderLoader(config) encode_manager = EncodeManager(encoder_loader) data = [\"ACGT\", \"TGCA\", \"GCTA\"] encoded = encode_manager.encode_column(\"dna_seq\", data) print(encoded.shape) torch.Size([3, 4, 4])  # 3 sequences, length 4, one-hot encoded</p> <p>Parameters:</p> <ul> <li> <code>encoder_loader</code>               (<code>EncoderLoader</code>)           \u2013            <p>Loader that provides encoders based on configuration.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>encode_column</code>             \u2013              <p>Encodes a column of data using the configured encoder.</p> </li> <li> <code>encode_columns</code>             \u2013              <p>Encodes multiple columns of data using the configured encoders.</p> </li> <li> <code>encode_dataframe</code>             \u2013              <p>Encode the dataframe using the encoders.</p> </li> </ul> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def __init__(\n    self,\n    encoder_loader: loaders.EncoderLoader,\n) -&gt; None:\n    \"\"\"Initialize the EncodeManager.\n\n    Args:\n        encoder_loader: Loader that provides encoders based on configuration.\n    \"\"\"\n    self.encoder_loader = encoder_loader\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.EncodeManager.encode_column","title":"encode_column","text":"<pre><code>encode_column(\n    column_name: str, column_data: list\n) -&gt; Tensor\n</code></pre> <p>Encodes a column of data using the configured encoder.</p> <p>Gets the appropriate encoder for the column from the encoder_loader and uses it to encode all the data in the column.</p> <p>Parameters:</p> <ul> <li> <code>column_name</code>               (<code>str</code>)           \u2013            <p>Name of the column to encode.</p> </li> <li> <code>column_data</code>               (<code>list</code>)           \u2013            <p>List of data values from the column to encode.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Encoded data as a torch.Tensor. The exact shape depends on the encoder used.</p> </li> </ul> Example <p>data = [\"ACGT\", \"TGCA\"] encoded = encode_manager.encode_column(\"dna_seq\", data) print(encoded.shape) torch.Size([2, 4, 4])  # 2 sequences, length 4, one-hot encoded</p> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def encode_column(self, column_name: str, column_data: list) -&gt; torch.Tensor:\n    \"\"\"Encodes a column of data using the configured encoder.\n\n    Gets the appropriate encoder for the column from the encoder_loader and uses it\n    to encode all the data in the column.\n\n    Args:\n        column_name: Name of the column to encode.\n        column_data: List of data values from the column to encode.\n\n    Returns:\n        Encoded data as a torch.Tensor. The exact shape depends on the encoder used.\n\n    Example:\n        &gt;&gt;&gt; data = [\"ACGT\", \"TGCA\"]\n        &gt;&gt;&gt; encoded = encode_manager.encode_column(\"dna_seq\", data)\n        &gt;&gt;&gt; print(encoded.shape)\n        torch.Size([2, 4, 4])  # 2 sequences, length 4, one-hot encoded\n    \"\"\"\n    encode_all_function = self.encoder_loader.get_function_encode_all(column_name)\n    return encode_all_function(column_data)\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.EncodeManager.encode_columns","title":"encode_columns","text":"<pre><code>encode_columns(column_data: dict) -&gt; dict\n</code></pre> <p>Encodes multiple columns of data using the configured encoders.</p> <p>Gets the appropriate encoder for each column from the encoder_loader and encodes all data values in those columns.</p> <p>Parameters:</p> <ul> <li> <code>column_data</code>               (<code>dict</code>)           \u2013            <p>Dict mapping column names to lists of data values to encode.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code>           \u2013            <p>Dict mapping column names to their encoded tensors. The exact shape of each</p> </li> <li> <code>dict</code>           \u2013            <p>tensor depends on the encoder used for that column.</p> </li> </ul> Example <p>data = {\"dna_seq\": [\"ACGT\", \"TGCA\"], \"labels\": [\"1\", \"2\"]} encoded = encode_manager.encode_columns(data) print(encoded[\"dna_seq\"].shape) torch.Size([2, 4, 4])  # 2 sequences, length 4, one-hot encoded</p> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def encode_columns(self, column_data: dict) -&gt; dict:\n    \"\"\"Encodes multiple columns of data using the configured encoders.\n\n    Gets the appropriate encoder for each column from the encoder_loader and encodes\n    all data values in those columns.\n\n    Args:\n        column_data: Dict mapping column names to lists of data values to encode.\n\n    Returns:\n        Dict mapping column names to their encoded tensors. The exact shape of each\n        tensor depends on the encoder used for that column.\n\n    Example:\n        &gt;&gt;&gt; data = {\"dna_seq\": [\"ACGT\", \"TGCA\"], \"labels\": [\"1\", \"2\"]}\n        &gt;&gt;&gt; encoded = encode_manager.encode_columns(data)\n        &gt;&gt;&gt; print(encoded[\"dna_seq\"].shape)\n        torch.Size([2, 4, 4])  # 2 sequences, length 4, one-hot encoded\n    \"\"\"\n    return {col: self.encode_column(col, values) for col, values in column_data.items()}\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.EncodeManager.encode_dataframe","title":"encode_dataframe","text":"<pre><code>encode_dataframe(dataframe: DataFrame) -&gt; dict[str, Tensor]\n</code></pre> <p>Encode the dataframe using the encoders.</p> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def encode_dataframe(self, dataframe: pl.DataFrame) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Encode the dataframe using the encoders.\"\"\"\n    return {col: self.encode_column(col, dataframe[col].to_list()) for col in dataframe.columns}\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.SplitManager","title":"SplitManager","text":"<pre><code>SplitManager(split_loader: SplitLoader)\n</code></pre> <p>Class for managing the splitting.</p> <p>Methods:</p> <ul> <li> <code>get_split_indices</code>             \u2013              <p>Get the indices for train, validation, and test splits.</p> </li> </ul> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def __init__(\n    self,\n    split_loader: loaders.SplitLoader,\n) -&gt; None:\n    \"\"\"Initialize the SplitManager.\"\"\"\n    self.split_loader = split_loader\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.SplitManager.get_split_indices","title":"get_split_indices","text":"<pre><code>get_split_indices(\n    data: dict,\n) -&gt; tuple[ndarray, ndarray, ndarray]\n</code></pre> <p>Get the indices for train, validation, and test splits.</p> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def get_split_indices(self, data: dict) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Get the indices for train, validation, and test splits.\"\"\"\n    return self.split_loader.get_function_split()(data)\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.TransformManager","title":"TransformManager","text":"<pre><code>TransformManager(transform_loader: TransformLoader)\n</code></pre> <p>Class for managing the transformations.</p> <p>Methods:</p> <ul> <li> <code>transform_column</code>             \u2013              <p>Transform a column of data using the specified transformation.</p> </li> </ul> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def __init__(\n    self,\n    transform_loader: loaders.TransformLoader,\n) -&gt; None:\n    \"\"\"Initialize the TransformManager.\"\"\"\n    self.transform_loader = transform_loader\n</code></pre>"},{"location":"reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.TransformManager.transform_column","title":"transform_column","text":"<pre><code>transform_column(\n    column_name: str, transform_name: str, column_data: list\n) -&gt; tuple[list, bool]\n</code></pre> <p>Transform a column of data using the specified transformation.</p> <p>Parameters:</p> <ul> <li> <code>column_name</code>               (<code>str</code>)           \u2013            <p>The name of the column to transform.</p> </li> <li> <code>transform_name</code>               (<code>str</code>)           \u2013            <p>The name of the transformation to use.</p> </li> <li> <code>column_data</code>               (<code>list</code>)           \u2013            <p>The data to transform.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (              <code>list</code> )          \u2013            <p>The transformed data.</p> </li> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>Whether the transformation added new rows to the data.</p> </li> </ul> Source code in <code>src/stimulus/data/data_handlers.py</code> <pre><code>def transform_column(self, column_name: str, transform_name: str, column_data: list) -&gt; tuple[list, bool]:\n    \"\"\"Transform a column of data using the specified transformation.\n\n    Args:\n        column_name (str): The name of the column to transform.\n        transform_name (str): The name of the transformation to use.\n        column_data (list): The data to transform.\n\n    Returns:\n        list: The transformed data.\n        bool: Whether the transformation added new rows to the data.\n    \"\"\"\n    transformer = self.transform_loader.__getattribute__(column_name)[transform_name]\n    return transformer.transform_all(column_data), transformer.add_row\n</code></pre>"},{"location":"reference/stimulus/data/handlertorch/","title":"stimulus.data.handlertorch","text":""},{"location":"reference/stimulus/data/handlertorch/#stimulus.data.handlertorch","title":"handlertorch","text":"<p>This file provides the class API for handling the data in pytorch using the Dataset and Dataloader classes.</p> <p>Classes:</p> <ul> <li> <code>TorchDataset</code>           \u2013            <p>Class for creating a torch dataset.</p> </li> </ul>"},{"location":"reference/stimulus/data/handlertorch/#stimulus.data.handlertorch.TorchDataset","title":"TorchDataset","text":"<pre><code>TorchDataset(\n    config_path: str,\n    csv_path: str,\n    encoder_loader: EncoderLoader,\n    split: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>Class for creating a torch dataset.</p> <p>Parameters:</p> <ul> <li> <code>config_path</code>               (<code>str</code>)           \u2013            <p>Path to the configuration file</p> </li> <li> <code>csv_path</code>               (<code>str</code>)           \u2013            <p>Path to the CSV data file</p> </li> <li> <code>encoder_loader</code>               (<code>EncoderLoader</code>)           \u2013            <p>Encoder loader instance</p> </li> <li> <code>split</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Optional tuple containing split information</p> </li> </ul> Source code in <code>src/stimulus/data/handlertorch.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    csv_path: str,\n    encoder_loader: loaders.EncoderLoader,\n    split: Optional[int] = None,\n) -&gt; None:\n    \"\"\"Initialize the TorchDataset.\n\n    Args:\n        config_path: Path to the configuration file\n        csv_path: Path to the CSV data file\n        encoder_loader: Encoder loader instance\n        split: Optional tuple containing split information\n    \"\"\"\n    self.loader = data_handlers.DatasetLoader(\n        config_path=config_path,\n        csv_path=csv_path,\n        encoder_loader=encoder_loader,\n        split=split,\n    )\n</code></pre>"},{"location":"reference/stimulus/data/loaders/","title":"stimulus.data.loaders","text":""},{"location":"reference/stimulus/data/loaders/#stimulus.data.loaders","title":"loaders","text":"<p>Loaders serve as interfaces between the CSV master class and custom methods.</p> <p>Mainly, three types of custom methods are supported: - Encoders: methods for encoding data before it is fed into the model - Data transformers: methods for transforming data (i.e. augmenting, noising...) - Splitters: methods for splitting data into train, validation and test sets</p> <p>Loaders are built from an input config YAML file which format is described in the documentation, you can find an example here: tests/test_data/dna_experiment/dna_experiment_config_template.yaml</p> <p>Classes:</p> <ul> <li> <code>EncoderLoader</code>           \u2013            <p>Class for loading encoders from a config file.</p> </li> <li> <code>SplitLoader</code>           \u2013            <p>Class for loading splitters from a config file.</p> </li> <li> <code>TransformLoader</code>           \u2013            <p>Class for loading transformations from a config file.</p> </li> </ul>"},{"location":"reference/stimulus/data/loaders/#stimulus.data.loaders.EncoderLoader","title":"EncoderLoader","text":"<pre><code>EncoderLoader(seed: Optional[float] = None)\n</code></pre> <p>Class for loading encoders from a config file.</p> <p>Parameters:</p> <ul> <li> <code>seed</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Random seed for reproducibility</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>get_encoder</code>             \u2013              <p>Gets an encoder object from the encoders module and initializes it with the given parameters.</p> </li> <li> <code>get_function_encode_all</code>             \u2013              <p>Gets the encoding function for a specific field.</p> </li> <li> <code>initialize_column_encoders_from_config</code>             \u2013              <p>Build the loader from a config dictionary.</p> </li> <li> <code>set_encoder_as_attribute</code>             \u2013              <p>Sets the encoder as an attribute of the loader.</p> </li> </ul> Source code in <code>src/stimulus/data/loaders.py</code> <pre><code>def __init__(self, seed: Optional[float] = None) -&gt; None:\n    \"\"\"Initialize the encoder loader.\n\n    Args:\n        seed: Random seed for reproducibility\n    \"\"\"\n    self.seed = seed\n</code></pre>"},{"location":"reference/stimulus/data/loaders/#stimulus.data.loaders.EncoderLoader.get_encoder","title":"get_encoder","text":"<pre><code>get_encoder(\n    encoder_name: str, encoder_params: Optional[dict] = None\n) -&gt; Any\n</code></pre> <p>Gets an encoder object from the encoders module and initializes it with the given parameters.</p> <p>Parameters:</p> <ul> <li> <code>encoder_name</code>               (<code>str</code>)           \u2013            <p>The name of the encoder to get</p> </li> <li> <code>encoder_params</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>The parameters for the encoder</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code> (              <code>Any</code> )          \u2013            <p>The encoder function for the specified field and parameters</p> </li> </ul> Source code in <code>src/stimulus/data/loaders.py</code> <pre><code>def get_encoder(self, encoder_name: str, encoder_params: Optional[dict] = None) -&gt; Any:\n    \"\"\"Gets an encoder object from the encoders module and initializes it with the given parameters.\n\n    Args:\n        encoder_name (str): The name of the encoder to get\n        encoder_params (dict): The parameters for the encoder\n\n    Returns:\n        Any: The encoder function for the specified field and parameters\n    \"\"\"\n    try:\n        return getattr(encoders, encoder_name)(**encoder_params)\n    except AttributeError:\n        logging.exception(f\"Encoder '{encoder_name}' not found in the encoders module.\")\n        logging.exception(\n            f\"Available encoders: {[name for name, obj in encoders.__dict__.items() if isinstance(obj, type) and name not in ('ABC', 'Any')]}\",\n        )\n        raise\n\n    except TypeError:\n        if encoder_params is None:\n            return getattr(encoders, encoder_name)()\n        logging.exception(f\"Encoder '{encoder_name}' has incorrect parameters: {encoder_params}\")\n        logging.exception(\n            f\"Expected parameters for '{encoder_name}': {inspect.signature(getattr(encoders, encoder_name))}\",\n        )\n        raise\n</code></pre>"},{"location":"reference/stimulus/data/loaders/#stimulus.data.loaders.EncoderLoader.get_function_encode_all","title":"get_function_encode_all","text":"<pre><code>get_function_encode_all(field_name: str) -&gt; Any\n</code></pre> <p>Gets the encoding function for a specific field.</p> <p>Parameters:</p> <ul> <li> <code>field_name</code>               (<code>str</code>)           \u2013            <p>The field name to get the encoder for</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code> (              <code>Any</code> )          \u2013            <p>The encode_all function for the specified field</p> </li> </ul> Source code in <code>src/stimulus/data/loaders.py</code> <pre><code>def get_function_encode_all(self, field_name: str) -&gt; Any:\n    \"\"\"Gets the encoding function for a specific field.\n\n    Args:\n        field_name (str): The field name to get the encoder for\n\n    Returns:\n        Any: The encode_all function for the specified field\n    \"\"\"\n    return getattr(self, field_name).encode_all\n</code></pre>"},{"location":"reference/stimulus/data/loaders/#stimulus.data.loaders.EncoderLoader.initialize_column_encoders_from_config","title":"initialize_column_encoders_from_config","text":"<pre><code>initialize_column_encoders_from_config(\n    column_config: YamlColumns,\n) -&gt; None\n</code></pre> <p>Build the loader from a config dictionary.</p> <p>Parameters:</p> <ul> <li> <code>column_config</code>               (<code>YamlColumns</code>)           \u2013            <p>Configuration dictionary containing field names (column_name) and their encoder specifications.</p> </li> </ul> Source code in <code>src/stimulus/data/loaders.py</code> <pre><code>def initialize_column_encoders_from_config(self, column_config: yaml_data.YamlColumns) -&gt; None:\n    \"\"\"Build the loader from a config dictionary.\n\n    Args:\n        column_config (yaml_data.YamlColumns): Configuration dictionary containing field names (column_name) and their encoder specifications.\n    \"\"\"\n    for field in column_config:\n        encoder = self.get_encoder(field.encoder[0].name, field.encoder[0].params)\n        self.set_encoder_as_attribute(field.column_name, encoder)\n</code></pre>"},{"location":"reference/stimulus/data/loaders/#stimulus.data.loaders.EncoderLoader.set_encoder_as_attribute","title":"set_encoder_as_attribute","text":"<pre><code>set_encoder_as_attribute(\n    field_name: str, encoder: AbstractEncoder\n) -&gt; None\n</code></pre> <p>Sets the encoder as an attribute of the loader.</p> <p>Parameters:</p> <ul> <li> <code>field_name</code>               (<code>str</code>)           \u2013            <p>The name of the field to set the encoder for</p> </li> <li> <code>encoder</code>               (<code>AbstractEncoder</code>)           \u2013            <p>The encoder to set</p> </li> </ul> Source code in <code>src/stimulus/data/loaders.py</code> <pre><code>def set_encoder_as_attribute(self, field_name: str, encoder: encoders.AbstractEncoder) -&gt; None:\n    \"\"\"Sets the encoder as an attribute of the loader.\n\n    Args:\n        field_name (str): The name of the field to set the encoder for\n        encoder (encoders.AbstractEncoder): The encoder to set\n    \"\"\"\n    setattr(self, field_name, encoder)\n</code></pre>"},{"location":"reference/stimulus/data/loaders/#stimulus.data.loaders.SplitLoader","title":"SplitLoader","text":"<pre><code>SplitLoader(seed: Optional[float] = None)\n</code></pre> <p>Class for loading splitters from a config file.</p> <p>Parameters:</p> <ul> <li> <code>seed</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Random seed for reproducibility</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>get_function_split</code>             \u2013              <p>Gets the function for splitting the data.</p> </li> <li> <code>get_splitter</code>             \u2013              <p>Gets a splitter object from the splitters module.</p> </li> <li> <code>initialize_splitter_from_config</code>             \u2013              <p>Build the loader from a config dictionary.</p> </li> <li> <code>set_splitter_as_attribute</code>             \u2013              <p>Sets the splitter as an attribute of the loader.</p> </li> </ul> Source code in <code>src/stimulus/data/loaders.py</code> <pre><code>def __init__(self, seed: Optional[float] = None) -&gt; None:\n    \"\"\"Initialize the split loader.\n\n    Args:\n        seed: Random seed for reproducibility\n    \"\"\"\n    self.seed = seed\n</code></pre>"},{"location":"reference/stimulus/data/loaders/#stimulus.data.loaders.SplitLoader.get_function_split","title":"get_function_split","text":"<pre><code>get_function_split() -&gt; Any\n</code></pre> <p>Gets the function for splitting the data.</p> <p>Returns:</p> <ul> <li> <code>Any</code> (              <code>Any</code> )          \u2013            <p>The split function for the specified method</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>AttributeError</code>             \u2013            <p>If splitter hasn't been initialized using initialize_splitter_from_config()</p> </li> </ul> Source code in <code>src/stimulus/data/loaders.py</code> <pre><code>def get_function_split(self) -&gt; Any:\n    \"\"\"Gets the function for splitting the data.\n\n    Returns:\n        Any: The split function for the specified method\n\n    Raises:\n        AttributeError: If splitter hasn't been initialized using initialize_splitter_from_config()\n    \"\"\"\n    if not hasattr(self, \"split\"):\n        # Raise a more specific error and chain it to the original AttributeError\n        raise AttributeError(\n            \"Splitter not initialized. Please call initialize_splitter_from_config() or set_splitter_as_attribute() \"\n            \"before attempting to get split function.\",\n        )\n    return self.split.get_split_indexes\n</code></pre>"},{"location":"reference/stimulus/data/loaders/#stimulus.data.loaders.SplitLoader.get_splitter","title":"get_splitter","text":"<pre><code>get_splitter(\n    splitter_name: str,\n    splitter_params: Optional[dict] = None,\n) -&gt; Any\n</code></pre> <p>Gets a splitter object from the splitters module.</p> <p>Parameters:</p> <ul> <li> <code>splitter_name</code>               (<code>str</code>)           \u2013            <p>The name of the splitter to get</p> </li> <li> <code>splitter_params</code>               (<code>Optional[dict]</code>, default:                   <code>None</code> )           \u2013            <p>Parameters for the splitter</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code> (              <code>Any</code> )          \u2013            <p>The splitter function for the specified splitter</p> </li> </ul> Source code in <code>src/stimulus/data/loaders.py</code> <pre><code>def get_splitter(self, splitter_name: str, splitter_params: Optional[dict] = None) -&gt; Any:\n    \"\"\"Gets a splitter object from the splitters module.\n\n    Args:\n        splitter_name (str): The name of the splitter to get\n        splitter_params (Optional[dict]): Parameters for the splitter\n\n    Returns:\n        Any: The splitter function for the specified splitter\n    \"\"\"\n    try:\n        return getattr(splitters, splitter_name)(**splitter_params)\n    except TypeError:\n        if splitter_params is None:\n            return getattr(splitters, splitter_name)()\n        logging.exception(f\"Splitter '{splitter_name}' has incorrect parameters: {splitter_params}\")\n        logging.exception(\n            f\"Expected parameters for '{splitter_name}': {inspect.signature(getattr(splitters, splitter_name))}\",\n        )\n        raise\n</code></pre>"},{"location":"reference/stimulus/data/loaders/#stimulus.data.loaders.SplitLoader.initialize_splitter_from_config","title":"initialize_splitter_from_config","text":"<pre><code>initialize_splitter_from_config(\n    split_config: YamlSplit,\n) -&gt; None\n</code></pre> <p>Build the loader from a config dictionary.</p> <p>Parameters:</p> <ul> <li> <code>split_config</code>               (<code>YamlSplit</code>)           \u2013            <p>Configuration dictionary containing split configurations.</p> </li> </ul> Source code in <code>src/stimulus/data/loaders.py</code> <pre><code>def initialize_splitter_from_config(self, split_config: yaml_data.YamlSplit) -&gt; None:\n    \"\"\"Build the loader from a config dictionary.\n\n    Args:\n        split_config (yaml_data.YamlSplit): Configuration dictionary containing split configurations.\n    \"\"\"\n    splitter = self.get_splitter(split_config.split_method, split_config.params)\n    self.set_splitter_as_attribute(splitter)\n</code></pre>"},{"location":"reference/stimulus/data/loaders/#stimulus.data.loaders.SplitLoader.set_splitter_as_attribute","title":"set_splitter_as_attribute","text":"<pre><code>set_splitter_as_attribute(splitter: Any) -&gt; None\n</code></pre> <p>Sets the splitter as an attribute of the loader.</p> <p>Parameters:</p> <ul> <li> <code>splitter</code>               (<code>Any</code>)           \u2013            <p>The splitter to set</p> </li> </ul> Source code in <code>src/stimulus/data/loaders.py</code> <pre><code>def set_splitter_as_attribute(self, splitter: Any) -&gt; None:\n    \"\"\"Sets the splitter as an attribute of the loader.\n\n    Args:\n        splitter (Any): The splitter to set\n    \"\"\"\n    self.split = splitter\n</code></pre>"},{"location":"reference/stimulus/data/loaders/#stimulus.data.loaders.TransformLoader","title":"TransformLoader","text":"<pre><code>TransformLoader(seed: Optional[float] = None)\n</code></pre> <p>Class for loading transformations from a config file.</p> <p>Parameters:</p> <ul> <li> <code>seed</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Random seed for reproducibility</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>get_data_transformer</code>             \u2013              <p>Gets a transformer object from the transformers module.</p> </li> <li> <code>initialize_column_data_transformers_from_config</code>             \u2013              <p>Build the loader from a config dictionary.</p> </li> <li> <code>set_data_transformer_as_attribute</code>             \u2013              <p>Sets the data transformer as an attribute of the loader.</p> </li> </ul> Source code in <code>src/stimulus/data/loaders.py</code> <pre><code>def __init__(self, seed: Optional[float] = None) -&gt; None:\n    \"\"\"Initialize the transform loader.\n\n    Args:\n        seed: Random seed for reproducibility\n    \"\"\"\n    self.seed = seed\n</code></pre>"},{"location":"reference/stimulus/data/loaders/#stimulus.data.loaders.TransformLoader.get_data_transformer","title":"get_data_transformer","text":"<pre><code>get_data_transformer(\n    transformation_name: str,\n    transformation_params: Optional[dict] = None,\n) -&gt; Any\n</code></pre> <p>Gets a transformer object from the transformers module.</p> <p>Parameters:</p> <ul> <li> <code>transformation_name</code>               (<code>str</code>)           \u2013            <p>The name of the transformer to get</p> </li> <li> <code>transformation_params</code>               (<code>Optional[dict]</code>, default:                   <code>None</code> )           \u2013            <p>Parameters for the transformer</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code> (              <code>Any</code> )          \u2013            <p>The transformer function for the specified transformation</p> </li> </ul> Source code in <code>src/stimulus/data/loaders.py</code> <pre><code>def get_data_transformer(self, transformation_name: str, transformation_params: Optional[dict] = None) -&gt; Any:\n    \"\"\"Gets a transformer object from the transformers module.\n\n    Args:\n        transformation_name (str): The name of the transformer to get\n        transformation_params (Optional[dict]): Parameters for the transformer\n\n    Returns:\n        Any: The transformer function for the specified transformation\n    \"\"\"\n    try:\n        return getattr(data_transformation_generators, transformation_name)(**transformation_params)\n    except AttributeError:\n        logging.exception(f\"Transformer '{transformation_name}' not found in the transformers module.\")\n        logging.exception(\n            f\"Available transformers: {[name for name, obj in data_transformation_generators.__dict__.items() if isinstance(obj, type) and name not in ('ABC', 'Any')]}\",\n        )\n        raise\n\n    except TypeError:\n        if transformation_params is None:\n            return getattr(data_transformation_generators, transformation_name)()\n        logging.exception(f\"Transformer '{transformation_name}' has incorrect parameters: {transformation_params}\")\n        logging.exception(\n            f\"Expected parameters for '{transformation_name}': {inspect.signature(getattr(data_transformation_generators, transformation_name))}\",\n        )\n        raise\n</code></pre>"},{"location":"reference/stimulus/data/loaders/#stimulus.data.loaders.TransformLoader.initialize_column_data_transformers_from_config","title":"initialize_column_data_transformers_from_config","text":"<pre><code>initialize_column_data_transformers_from_config(\n    transform_config: YamlTransform,\n) -&gt; None\n</code></pre> <p>Build the loader from a config dictionary.</p> <p>Parameters:</p> <ul> <li> <code>transform_config</code>               (<code>YamlTransform</code>)           \u2013            <p>Configuration dictionary containing transforms configurations.</p> </li> </ul> Example <p>Given a YAML config like: <pre><code>transforms:\n  transformation_name: noise\n  columns:\n    - column_name: age\n      transformations:\n        - name: GaussianNoise\n          params:\n            std: 0.1\n    - column_name: fare\n      transformations:\n        - name: GaussianNoise\n          params:\n            std: 0.1\n</code></pre></p> <p>The loader will: 1. Iterate through each column (age, fare) 2. For each transformation in the column:    - Get the transformer (GaussianNoise) with its params (std=0.1)    - Set it as an attribute on the loader using the column name as key</p> Source code in <code>src/stimulus/data/loaders.py</code> <pre><code>def initialize_column_data_transformers_from_config(self, transform_config: yaml_data.YamlTransform) -&gt; None:\n    \"\"\"Build the loader from a config dictionary.\n\n    Args:\n        transform_config (yaml_data.YamlTransform): Configuration dictionary containing transforms configurations.\n\n    Example:\n        Given a YAML config like:\n        ```yaml\n        transforms:\n          transformation_name: noise\n          columns:\n            - column_name: age\n              transformations:\n                - name: GaussianNoise\n                  params:\n                    std: 0.1\n            - column_name: fare\n              transformations:\n                - name: GaussianNoise\n                  params:\n                    std: 0.1\n        ```\n\n        The loader will:\n        1. Iterate through each column (age, fare)\n        2. For each transformation in the column:\n           - Get the transformer (GaussianNoise) with its params (std=0.1)\n           - Set it as an attribute on the loader using the column name as key\n    \"\"\"\n    for column in transform_config.columns:\n        col_name = column.column_name\n        for transform_spec in column.transformations:\n            transformer = self.get_data_transformer(transform_spec.name, transform_spec.params)\n            self.set_data_transformer_as_attribute(col_name, transformer)\n</code></pre>"},{"location":"reference/stimulus/data/loaders/#stimulus.data.loaders.TransformLoader.set_data_transformer_as_attribute","title":"set_data_transformer_as_attribute","text":"<pre><code>set_data_transformer_as_attribute(\n    field_name: str, data_transformer: Any\n) -&gt; None\n</code></pre> <p>Sets the data transformer as an attribute of the loader.</p> <p>Parameters:</p> <ul> <li> <code>field_name</code>               (<code>str</code>)           \u2013            <p>The name of the field to set the data transformer for</p> </li> <li> <code>data_transformer</code>               (<code>Any</code>)           \u2013            <p>The data transformer to set</p> </li> </ul> Source code in <code>src/stimulus/data/loaders.py</code> <pre><code>def set_data_transformer_as_attribute(self, field_name: str, data_transformer: Any) -&gt; None:\n    \"\"\"Sets the data transformer as an attribute of the loader.\n\n    Args:\n        field_name (str): The name of the field to set the data transformer for\n        data_transformer (Any): The data transformer to set\n    \"\"\"\n    # check if the field already exists, if it does not, initialize it to an empty dict\n    if not hasattr(self, field_name):\n        setattr(self, field_name, {data_transformer.__class__.__name__: data_transformer})\n    else:\n        field_value = getattr(self, field_name)\n        field_value[data_transformer.__class__.__name__] = data_transformer\n</code></pre>"},{"location":"reference/stimulus/data/encoding/","title":"stimulus.data.encoding","text":""},{"location":"reference/stimulus/data/encoding/#stimulus.data.encoding","title":"encoding","text":"<p>Encoding package for data transformation.</p> <p>Modules:</p> <ul> <li> <code>encoders</code>           \u2013            <p>This file contains encoders classes for encoding various types of data.</p> </li> </ul>"},{"location":"reference/stimulus/data/encoding/encoders/","title":"stimulus.data.encoding.encoders","text":""},{"location":"reference/stimulus/data/encoding/encoders/#stimulus.data.encoding.encoders","title":"encoders","text":"<p>This file contains encoders classes for encoding various types of data.</p> <p>Classes:</p> <ul> <li> <code>AbstractEncoder</code>           \u2013            <p>Abstract class for encoders.</p> </li> <li> <code>NumericEncoder</code>           \u2013            <p>Encoder for float/int data.</p> </li> <li> <code>NumericRankEncoder</code>           \u2013            <p>Encoder for float/int data that encodes the data based on their rank.</p> </li> <li> <code>StrClassificationEncoder</code>           \u2013            <p>A string classification encoder that converts lists of strings into numeric labels using scikit-learn.</p> </li> <li> <code>TextOneHotEncoder</code>           \u2013            <p>One hot encoder for text data.</p> </li> </ul>"},{"location":"reference/stimulus/data/encoding/encoders/#stimulus.data.encoding.encoders.AbstractEncoder","title":"AbstractEncoder","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class for encoders.</p> <p>Encoders are classes that encode the raw data into torch.tensors. Different encoders provide different encoding methods. Different encoders may take different types of data as input.</p> <p>Methods:</p> <ul> <li> <code>encode</code>             \u2013              <p>encodes a single data point</p> </li> <li> <code>encode_all</code>             \u2013              <p>encodes a list of data points into a torch.tensor</p> </li> <li> <code>encode_multiprocess</code>             \u2013              <p>encodes a list of data points using multiprocessing</p> </li> <li> <code>decode</code>             \u2013              <p>decodes a single data point</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>decode</code>             \u2013              <p>Decode a single data point.</p> </li> <li> <code>encode</code>             \u2013              <p>Encode a single data point.</p> </li> <li> <code>encode_all</code>             \u2013              <p>Encode a list of data points.</p> </li> </ul>"},{"location":"reference/stimulus/data/encoding/encoders/#stimulus.data.encoding.encoders.AbstractEncoder.decode","title":"decode  <code>abstractmethod</code>","text":"<pre><code>decode(data: Any) -&gt; Any\n</code></pre> <p>Decode a single data point.</p> <p>This is an abstract method, child classes should overwrite it.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Any</code>)           \u2013            <p>a single encoded data point</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>decoded_data_point</code> (              <code>Any</code> )          \u2013            <p>the decoded data point</p> </li> </ul> Source code in <code>src/stimulus/data/encoding/encoders.py</code> <pre><code>@abstractmethod\ndef decode(self, data: Any) -&gt; Any:\n    \"\"\"Decode a single data point.\n\n    This is an abstract method, child classes should overwrite it.\n\n    Args:\n        data (Any): a single encoded data point\n\n    Returns:\n        decoded_data_point (Any): the decoded data point\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/stimulus/data/encoding/encoders/#stimulus.data.encoding.encoders.AbstractEncoder.encode","title":"encode  <code>abstractmethod</code>","text":"<pre><code>encode(data: Any) -&gt; Any\n</code></pre> <p>Encode a single data point.</p> <p>This is an abstract method, child classes should overwrite it.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Any</code>)           \u2013            <p>a single data point</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>encoded_data_point</code> (              <code>Any</code> )          \u2013            <p>the encoded data point</p> </li> </ul> Source code in <code>src/stimulus/data/encoding/encoders.py</code> <pre><code>@abstractmethod\ndef encode(self, data: Any) -&gt; Any:\n    \"\"\"Encode a single data point.\n\n    This is an abstract method, child classes should overwrite it.\n\n    Args:\n        data (Any): a single data point\n\n    Returns:\n        encoded_data_point (Any): the encoded data point\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/stimulus/data/encoding/encoders/#stimulus.data.encoding.encoders.AbstractEncoder.encode_all","title":"encode_all  <code>abstractmethod</code>","text":"<pre><code>encode_all(data: list[Any]) -&gt; Tensor\n</code></pre> <p>Encode a list of data points.</p> <p>This is an abstract method, child classes should overwrite it.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>list[Any]</code>)           \u2013            <p>a list of data points</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>encoded_data</code> (              <code>Tensor</code> )          \u2013            <p>encoded data points</p> </li> </ul> Source code in <code>src/stimulus/data/encoding/encoders.py</code> <pre><code>@abstractmethod\ndef encode_all(self, data: list[Any]) -&gt; torch.Tensor:\n    \"\"\"Encode a list of data points.\n\n    This is an abstract method, child classes should overwrite it.\n\n    Args:\n        data (list[Any]): a list of data points\n\n    Returns:\n        encoded_data (torch.Tensor): encoded data points\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/stimulus/data/encoding/encoders/#stimulus.data.encoding.encoders.NumericEncoder","title":"NumericEncoder","text":"<pre><code>NumericEncoder(dtype: dtype = float32)\n</code></pre> <p>               Bases: <code>AbstractEncoder</code></p> <p>Encoder for float/int data.</p> <p>Attributes:</p> <ul> <li> <code>dtype</code>               (<code>dtype</code>)           \u2013            <p>The data type of the encoded data. Default = torch.float32 (32-bit floating point)</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>dtype</code>               (<code>dtype</code>, default:                   <code>float32</code> )           \u2013            <p>the data type of the encoded data. Default = torch.float (32-bit floating point)</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>decode</code>             \u2013              <p>Decodes the data.</p> </li> <li> <code>encode</code>             \u2013              <p>Encodes the data.</p> </li> <li> <code>encode_all</code>             \u2013              <p>Encodes the data.</p> </li> </ul> Source code in <code>src/stimulus/data/encoding/encoders.py</code> <pre><code>def __init__(self, dtype: torch.dtype = torch.float32) -&gt; None:\n    \"\"\"Initialize the NumericEncoder class.\n\n    Args:\n        dtype (torch.dtype): the data type of the encoded data. Default = torch.float (32-bit floating point)\n    \"\"\"\n    self.dtype = dtype\n</code></pre>"},{"location":"reference/stimulus/data/encoding/encoders/#stimulus.data.encoding.encoders.NumericEncoder.decode","title":"decode","text":"<pre><code>decode(data: Tensor) -&gt; list[float]\n</code></pre> <p>Decodes the data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Tensor</code>)           \u2013            <p>the encoded data</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>decoded_data</code> (              <code>list[float]</code> )          \u2013            <p>the decoded data</p> </li> </ul> Source code in <code>src/stimulus/data/encoding/encoders.py</code> <pre><code>def decode(self, data: torch.Tensor) -&gt; list[float]:\n    \"\"\"Decodes the data.\n\n    Args:\n        data (torch.Tensor): the encoded data\n\n    Returns:\n        decoded_data (list[float]): the decoded data\n    \"\"\"\n    return data.cpu().numpy().tolist()\n</code></pre>"},{"location":"reference/stimulus/data/encoding/encoders/#stimulus.data.encoding.encoders.NumericEncoder.encode","title":"encode","text":"<pre><code>encode(data: float) -&gt; Tensor\n</code></pre> <p>Encodes the data.</p> <p>This method takes as input a single data point, should be mappable to a single output.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>float</code>)           \u2013            <p>a single data point</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>encoded_data_point</code> (              <code>Tensor</code> )          \u2013            <p>the encoded data point</p> </li> </ul> Source code in <code>src/stimulus/data/encoding/encoders.py</code> <pre><code>def encode(self, data: float) -&gt; torch.Tensor:\n    \"\"\"Encodes the data.\n\n    This method takes as input a single data point, should be mappable to a single output.\n\n    Args:\n        data (float): a single data point\n\n    Returns:\n        encoded_data_point (torch.Tensor): the encoded data point\n    \"\"\"\n    return self.encode_all([data])\n</code></pre>"},{"location":"reference/stimulus/data/encoding/encoders/#stimulus.data.encoding.encoders.NumericEncoder.encode_all","title":"encode_all","text":"<pre><code>encode_all(data: list[float]) -&gt; Tensor\n</code></pre> <p>Encodes the data.</p> <p>This method takes as input a list of data points, or a single float, and returns a torch.tensor.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>list[float]</code>)           \u2013            <p>a list of data points or a single data point</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>encoded_data</code> (              <code>Tensor</code> )          \u2013            <p>the encoded data</p> </li> </ul> Source code in <code>src/stimulus/data/encoding/encoders.py</code> <pre><code>def encode_all(self, data: list[float]) -&gt; torch.Tensor:\n    \"\"\"Encodes the data.\n\n    This method takes as input a list of data points, or a single float, and returns a torch.tensor.\n\n    Args:\n        data (list[float]): a list of data points or a single data point\n\n    Returns:\n        encoded_data (torch.Tensor): the encoded data\n    \"\"\"\n    if not isinstance(data, list):\n        data = [data]\n\n    self._check_input_dtype(data)\n    self._warn_float_is_converted_to_int(data)\n\n    return torch.tensor(data, dtype=self.dtype)\n</code></pre>"},{"location":"reference/stimulus/data/encoding/encoders/#stimulus.data.encoding.encoders.NumericRankEncoder","title":"NumericRankEncoder","text":"<pre><code>NumericRankEncoder(*, scale: bool = False)\n</code></pre> <p>               Bases: <code>AbstractEncoder</code></p> <p>Encoder for float/int data that encodes the data based on their rank.</p> <p>Attributes:</p> <ul> <li> <code>scale</code>               (<code>bool</code>)           \u2013            <p>whether to scale the ranks to be between 0 and 1. Default = False</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>encode</code>             \u2013              <p>encodes a single data point</p> </li> <li> <code>encode_all</code>             \u2013              <p>encodes a list of data points into a torch.tensor</p> </li> <li> <code>decode</code>             \u2013              <p>decodes a single data point</p> </li> <li> <code>_check_input_dtype</code>             \u2013              <p>checks if the input data is int or float data</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>scale</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to scale the ranks to be between 0 and 1. Default = False</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>decode</code>             \u2013              <p>Returns an error since decoding does not make sense without encoder information, which is not yet supported.</p> </li> <li> <code>encode</code>             \u2013              <p>Returns an error since encoding a single float does not make sense.</p> </li> <li> <code>encode_all</code>             \u2013              <p>Encodes the data.</p> </li> </ul> Source code in <code>src/stimulus/data/encoding/encoders.py</code> <pre><code>def __init__(self, *, scale: bool = False) -&gt; None:\n    \"\"\"Initialize the NumericRankEncoder class.\n\n    Args:\n        scale (bool): whether to scale the ranks to be between 0 and 1. Default = False\n    \"\"\"\n    self.scale = scale\n</code></pre>"},{"location":"reference/stimulus/data/encoding/encoders/#stimulus.data.encoding.encoders.NumericRankEncoder.decode","title":"decode","text":"<pre><code>decode(data: Any) -&gt; Any\n</code></pre> <p>Returns an error since decoding does not make sense without encoder information, which is not yet supported.</p> Source code in <code>src/stimulus/data/encoding/encoders.py</code> <pre><code>def decode(self, data: Any) -&gt; Any:\n    \"\"\"Returns an error since decoding does not make sense without encoder information, which is not yet supported.\"\"\"\n    raise NotImplementedError(\"Decoding is not yet supported for NumericRank.\")\n</code></pre>"},{"location":"reference/stimulus/data/encoding/encoders/#stimulus.data.encoding.encoders.NumericRankEncoder.encode","title":"encode","text":"<pre><code>encode(data: Any) -&gt; Tensor\n</code></pre> <p>Returns an error since encoding a single float does not make sense.</p> Source code in <code>src/stimulus/data/encoding/encoders.py</code> <pre><code>def encode(self, data: Any) -&gt; torch.Tensor:\n    \"\"\"Returns an error since encoding a single float does not make sense.\"\"\"\n    raise NotImplementedError(\"Encoding a single float does not make sense. Use encode_all instead.\")\n</code></pre>"},{"location":"reference/stimulus/data/encoding/encoders/#stimulus.data.encoding.encoders.NumericRankEncoder.encode_all","title":"encode_all","text":"<pre><code>encode_all(data: list[Union[int, float]]) -&gt; Tensor\n</code></pre> <p>Encodes the data.</p> <p>This method takes as input a list of data points, and returns the ranks of the data points. The ranks are normalized to be between 0 and 1, when scale is set to True.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>list[Union[int, float]]</code>)           \u2013            <p>a list of numeric values</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>encoded_data</code> (              <code>Tensor</code> )          \u2013            <p>the encoded data</p> </li> </ul> Source code in <code>src/stimulus/data/encoding/encoders.py</code> <pre><code>def encode_all(self, data: list[Union[int, float]]) -&gt; torch.Tensor:\n    \"\"\"Encodes the data.\n\n    This method takes as input a list of data points, and returns the ranks of the data points.\n    The ranks are normalized to be between 0 and 1, when scale is set to True.\n\n    Args:\n        data (list[Union[int, float]]): a list of numeric values\n\n    Returns:\n        encoded_data (torch.Tensor): the encoded data\n    \"\"\"\n    if not isinstance(data, list):\n        data = [data]\n    self._check_input_dtype(data)\n\n    # Get ranks (0 is lowest, n-1 is highest)\n    # and normalize to be between 0 and 1\n    array_data: np.ndarray = np.array(data)\n    ranks: np.ndarray = np.argsort(np.argsort(array_data))\n    if self.scale:\n        ranks = ranks / max(len(ranks) - 1, 1)\n    return torch.tensor(ranks)\n</code></pre>"},{"location":"reference/stimulus/data/encoding/encoders/#stimulus.data.encoding.encoders.StrClassificationEncoder","title":"StrClassificationEncoder","text":"<pre><code>StrClassificationEncoder(*, scale: bool = False)\n</code></pre> <p>               Bases: <code>AbstractEncoder</code></p> <p>A string classification encoder that converts lists of strings into numeric labels using scikit-learn.</p> <p>When scale is set to True, the labels are scaled to be between 0 and 1.</p> <p>Attributes:</p> <ul> <li> <code>scale</code>               (<code>bool</code>)           \u2013            <p>Whether to scale the labels to be between 0 and 1. Default = False</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>encode</code>             \u2013              <p>str) -&gt; int: Raises a NotImplementedError, as encoding a single string is not meaningful in this context.</p> </li> <li> <code>encode_all</code>             \u2013              <p>list[str]) -&gt; torch.tensor: Encodes an entire list of string data into a numeric representation using LabelEncoder and returns a torch tensor. Ensures that the provided data items are valid strings prior to encoding.</p> </li> <li> <code>decode</code>             \u2013              <p>Any) -&gt; Any: Raises a NotImplementedError, as decoding is not supported with the current design.</p> </li> <li> <code>_check_dtype</code>             \u2013              <p>list[str]) -&gt; None: Validates that all items in the data list are strings, raising a ValueError otherwise.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>scale</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to scale the labels to be between 0 and 1. Default = False</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>decode</code>             \u2013              <p>Returns an error since decoding does not make sense without encoder information, which is not yet supported.</p> </li> <li> <code>encode</code>             \u2013              <p>Returns an error since encoding a single string does not make sense.</p> </li> <li> <code>encode_all</code>             \u2013              <p>Encodes the data.</p> </li> </ul> Source code in <code>src/stimulus/data/encoding/encoders.py</code> <pre><code>def __init__(self, *, scale: bool = False) -&gt; None:\n    \"\"\"Initialize the StrClassificationEncoder class.\n\n    Args:\n        scale (bool): whether to scale the labels to be between 0 and 1. Default = False\n    \"\"\"\n    self.scale = scale\n</code></pre>"},{"location":"reference/stimulus/data/encoding/encoders/#stimulus.data.encoding.encoders.StrClassificationEncoder.decode","title":"decode","text":"<pre><code>decode(data: Any) -&gt; Any\n</code></pre> <p>Returns an error since decoding does not make sense without encoder information, which is not yet supported.</p> Source code in <code>src/stimulus/data/encoding/encoders.py</code> <pre><code>def decode(self, data: Any) -&gt; Any:\n    \"\"\"Returns an error since decoding does not make sense without encoder information, which is not yet supported.\"\"\"\n    raise NotImplementedError(\"Decoding is not yet supported for StrClassification.\")\n</code></pre>"},{"location":"reference/stimulus/data/encoding/encoders/#stimulus.data.encoding.encoders.StrClassificationEncoder.encode","title":"encode","text":"<pre><code>encode(data: str) -&gt; int\n</code></pre> <p>Returns an error since encoding a single string does not make sense.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>str</code>)           \u2013            <p>a single string</p> </li> </ul> Source code in <code>src/stimulus/data/encoding/encoders.py</code> <pre><code>def encode(self, data: str) -&gt; int:\n    \"\"\"Returns an error since encoding a single string does not make sense.\n\n    Args:\n        data (str): a single string\n    \"\"\"\n    raise NotImplementedError(\"Encoding a single string does not make sense. Use encode_all instead.\")\n</code></pre>"},{"location":"reference/stimulus/data/encoding/encoders/#stimulus.data.encoding.encoders.StrClassificationEncoder.encode_all","title":"encode_all","text":"<pre><code>encode_all(data: Union[str, list[str]]) -&gt; Tensor\n</code></pre> <p>Encodes the data.</p> <p>This method takes as input a list of data points, should be mappable to a single output, using LabelEncoder from scikit learn and returning a numpy array. For more info visit : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Union[str, list[str]]</code>)           \u2013            <p>a list of strings or single string</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>encoded_data</code> (              <code>tensor</code> )          \u2013            <p>the encoded data</p> </li> </ul> Source code in <code>src/stimulus/data/encoding/encoders.py</code> <pre><code>def encode_all(self, data: Union[str, list[str]]) -&gt; torch.Tensor:\n    \"\"\"Encodes the data.\n\n    This method takes as input a list of data points, should be mappable to a single output, using LabelEncoder from scikit learn and returning a numpy array.\n    For more info visit : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n\n    Args:\n        data (Union[str, list[str]]): a list of strings or single string\n\n    Returns:\n        encoded_data (torch.tensor): the encoded data\n    \"\"\"\n    if not isinstance(data, list):\n        data = [data]\n\n    self._check_dtype(data)\n\n    encoder = preprocessing.LabelEncoder()\n    encoded_data = torch.tensor(encoder.fit_transform(data))\n    if self.scale:\n        encoded_data = encoded_data / max(len(encoded_data) - 1, 1)\n\n    return encoded_data\n</code></pre>"},{"location":"reference/stimulus/data/encoding/encoders/#stimulus.data.encoding.encoders.TextOneHotEncoder","title":"TextOneHotEncoder","text":"<pre><code>TextOneHotEncoder(\n    alphabet: str = \"acgt\",\n    *,\n    convert_lowercase: bool = False,\n    padding: bool = False\n)\n</code></pre> <p>               Bases: <code>AbstractEncoder</code></p> <p>One hot encoder for text data.</p> <p>NOTE encodes based on the given alphabet If a character c is not in the alphabet, c will be represented by a vector of zeros.</p> <p>Attributes:</p> <ul> <li> <code>alphabet</code>               (<code>str</code>)           \u2013            <p>the alphabet to one hot encode the data with.</p> </li> <li> <code>convert_lowercase</code>               (<code>bool</code>)           \u2013            <p>whether to convert the sequence and alphabet to lowercase. Default is False.</p> </li> <li> <code>padding</code>               (<code>bool</code>)           \u2013            <p>whether to pad the sequences with zeros. Default is False.</p> </li> <li> <code>encoder</code>               (<code>OneHotEncoder</code>)           \u2013            <p>preprocessing.OneHotEncoder object initialized with self.alphabet</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>encode</code>             \u2013              <p>encodes a single data point</p> </li> <li> <code>encode_all</code>             \u2013              <p>encodes a list of data points into a numpy array</p> </li> <li> <code>encode_multiprocess</code>             \u2013              <p>encodes a list of data points using multiprocessing</p> </li> <li> <code>decode</code>             \u2013              <p>decodes a single data point</p> </li> <li> <code>_sequence_to_array</code>             \u2013              <p>transforms a sequence into a numpy array</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>alphabet</code>               (<code>str</code>, default:                   <code>'acgt'</code> )           \u2013            <p>the alphabet to one hot encode the data with.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the input alphabet is not a string.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>decode</code>             \u2013              <p>Decodes one-hot encoded tensor back to sequences.</p> </li> <li> <code>encode</code>             \u2013              <p>One hot encodes a single sequence.</p> </li> <li> <code>encode_all</code>             \u2013              <p>Encodes a list of sequences.</p> </li> <li> <code>encode_multiprocess</code>             \u2013              <p>Encodes a list of sequences using multiprocessing.</p> </li> </ul> Source code in <code>src/stimulus/data/encoding/encoders.py</code> <pre><code>def __init__(self, alphabet: str = \"acgt\", *, convert_lowercase: bool = False, padding: bool = False) -&gt; None:\n    \"\"\"Initialize the TextOneHotEncoder class.\n\n    Args:\n        alphabet (str): the alphabet to one hot encode the data with.\n\n    Raises:\n        TypeError: If the input alphabet is not a string.\n    \"\"\"\n    if not isinstance(alphabet, str):\n        error_msg = f\"Expected a string input for alphabet, got {type(alphabet).__name__}\"\n        logger.error(error_msg)\n        raise TypeError(error_msg)\n\n    if convert_lowercase:\n        alphabet = alphabet.lower()\n\n    self.alphabet = alphabet\n    self.convert_lowercase = convert_lowercase\n    self.padding = padding\n\n    self.encoder = preprocessing.OneHotEncoder(\n        categories=[list(alphabet)],\n        handle_unknown=\"ignore\",\n    )  # handle_unknown='ignore' unsures that a vector of zeros is returned for unknown characters, such as 'Ns' in DNA sequences\n    self.encoder.fit(np.array(list(alphabet)).reshape(-1, 1))\n</code></pre>"},{"location":"reference/stimulus/data/encoding/encoders/#stimulus.data.encoding.encoders.TextOneHotEncoder.decode","title":"decode","text":"<pre><code>decode(data: Tensor) -&gt; Union[str, list[str]]\n</code></pre> <p>Decodes one-hot encoded tensor back to sequences.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Tensor</code>)           \u2013            <p>2D or 3D tensor of one-hot encoded sequences - 2D shape: (sequence_length, alphabet_size) - 3D shape: (batch_size, sequence_length, alphabet_size)</p> </li> </ul> <p>NOTE that when decoding 3D shape tensor, it assumes all sequences have the same length.</p> <p>Returns:</p> <ul> <li> <code>Union[str, list[str]]</code>           \u2013            <p>Union[str, list[str]]: Single sequence string or list of sequence strings</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the input data is not a 2D or 3D tensor</p> </li> </ul> Source code in <code>src/stimulus/data/encoding/encoders.py</code> <pre><code>def decode(self, data: torch.Tensor) -&gt; Union[str, list[str]]:\n    \"\"\"Decodes one-hot encoded tensor back to sequences.\n\n    Args:\n        data (torch.Tensor): 2D or 3D tensor of one-hot encoded sequences\n            - 2D shape: (sequence_length, alphabet_size)\n            - 3D shape: (batch_size, sequence_length, alphabet_size)\n\n    NOTE that when decoding 3D shape tensor, it assumes all sequences have the same length.\n\n    Returns:\n        Union[str, list[str]]: Single sequence string or list of sequence strings\n\n    Raises:\n        TypeError: If the input data is not a 2D or 3D tensor\n    \"\"\"\n    expected_2d_tensor = 2\n    expected_3d_tensor = 3\n\n    if data.dim() == expected_2d_tensor:\n        # Single sequence\n        data_np = data.numpy().reshape(-1, len(self.alphabet))\n        decoded = self.encoder.inverse_transform(data_np).flatten()\n        return \"\".join([i for i in decoded if i is not None])\n\n    if data.dim() == expected_3d_tensor:\n        # Multiple sequences\n        batch_size, seq_len, _ = data.shape\n        data_np = data.reshape(-1, len(self.alphabet)).numpy()\n        decoded = self.encoder.inverse_transform(data_np)\n        sequences = decoded.reshape(batch_size, seq_len)\n        # Convert to masked array where None values are masked\n        masked_sequences = np.ma.masked_equal(sequences, None)\n        # Fill masked values with \"-\"\n        filled_sequences = masked_sequences.filled(\"-\")\n        return [\"\".join(seq) for seq in filled_sequences]\n\n    raise ValueError(f\"Expected 2D or 3D tensor, got {data.dim()}D\")\n</code></pre>"},{"location":"reference/stimulus/data/encoding/encoders/#stimulus.data.encoding.encoders.TextOneHotEncoder.encode","title":"encode","text":"<pre><code>encode(data: str) -&gt; Tensor\n</code></pre> <p>One hot encodes a single sequence.</p> <p>Takes a single string sequence and returns a torch tensor of shape (sequence_length, alphabet_length). The returned tensor corresponds to the one hot encoding of the sequence. Unknown characters are represented by a vector of zeros.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>str</code>)           \u2013            <p>single sequence</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>encoded_data_point</code> (              <code>Tensor</code> )          \u2013            <p>one hot encoded sequence</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the input data is not a string.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; encoder = TextOneHotEncoder(alphabet=\"acgt\")\n&gt;&gt;&gt; encoder.encode(\"acgt\")\ntensor([[1, 0, 0, 0],\n        [0, 1, 0, 0],\n        [0, 0, 1, 0],\n        [0, 0, 0, 1]])\n&gt;&gt;&gt; encoder.encode(\"acgtn\")\ntensor([[1, 0, 0, 0],\n        [0, 1, 0, 0],\n        [0, 0, 1, 0],\n        [0, 0, 0, 1],\n        [0, 0, 0, 0]])\n</code></pre> <pre><code>&gt;&gt;&gt; encoder = TextOneHotEncoder(alphabet=\"ACgt\")\n&gt;&gt;&gt; encoder.encode(\"acgt\")\ntensor([[0, 0, 0, 0],\n        [0, 0, 0, 0],\n        [0, 0, 1, 0],\n        [0, 0, 0, 1]])\n&gt;&gt;&gt; encoder.encode(\"ACgt\")\ntensor([[1, 0, 0, 0],\n        [0, 1, 0, 0],\n        [0, 0, 1, 0],\n        [0, 0, 0, 1]])\n</code></pre> Source code in <code>src/stimulus/data/encoding/encoders.py</code> <pre><code>def encode(self, data: str) -&gt; torch.Tensor:\n    \"\"\"One hot encodes a single sequence.\n\n    Takes a single string sequence and returns a torch tensor of shape (sequence_length, alphabet_length).\n    The returned tensor corresponds to the one hot encoding of the sequence.\n    Unknown characters are represented by a vector of zeros.\n\n    Args:\n        data (str): single sequence\n\n    Returns:\n        encoded_data_point (torch.Tensor): one hot encoded sequence\n\n    Raises:\n        TypeError: If the input data is not a string.\n\n    Examples:\n        &gt;&gt;&gt; encoder = TextOneHotEncoder(alphabet=\"acgt\")\n        &gt;&gt;&gt; encoder.encode(\"acgt\")\n        tensor([[1, 0, 0, 0],\n                [0, 1, 0, 0],\n                [0, 0, 1, 0],\n                [0, 0, 0, 1]])\n        &gt;&gt;&gt; encoder.encode(\"acgtn\")\n        tensor([[1, 0, 0, 0],\n                [0, 1, 0, 0],\n                [0, 0, 1, 0],\n                [0, 0, 0, 1],\n                [0, 0, 0, 0]])\n\n        &gt;&gt;&gt; encoder = TextOneHotEncoder(alphabet=\"ACgt\")\n        &gt;&gt;&gt; encoder.encode(\"acgt\")\n        tensor([[0, 0, 0, 0],\n                [0, 0, 0, 0],\n                [0, 0, 1, 0],\n                [0, 0, 0, 1]])\n        &gt;&gt;&gt; encoder.encode(\"ACgt\")\n        tensor([[1, 0, 0, 0],\n                [0, 1, 0, 0],\n                [0, 0, 1, 0],\n                [0, 0, 0, 1]])\n    \"\"\"\n    sequence_array = self._sequence_to_array(data)\n    transformed = self.encoder.transform(sequence_array)\n    numpy_array = np.squeeze(np.stack(transformed.toarray()))\n    return torch.from_numpy(numpy_array)\n</code></pre>"},{"location":"reference/stimulus/data/encoding/encoders/#stimulus.data.encoding.encoders.TextOneHotEncoder.encode_all","title":"encode_all","text":"<pre><code>encode_all(data: Union[str, list[str]]) -&gt; Tensor\n</code></pre> <p>Encodes a list of sequences.</p> <p>Takes a list of string sequences and returns a torch tensor of shape (number_of_sequences, sequence_length, alphabet_length). The returned tensor corresponds to the one hot encoding of the sequences. Unknown characters are represented by a vector of zeros.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Union[str, list[str]]</code>)           \u2013            <p>list of sequences or a single sequence</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>encoded_data</code> (              <code>Tensor</code> )          \u2013            <p>one hot encoded sequences</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the input data is not a list or a string.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If all sequences do not have the same length when padding is False.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; encoder = TextOneHotEncoder(alphabet=\"acgt\")\n&gt;&gt;&gt; encoder.encode_all([\"acgt\", \"acgtn\"])\ntensor([[[1, 0, 0, 0],\n         [0, 1, 0, 0],\n         [0, 0, 1, 0],\n         [0, 0, 0, 1],\n         [0, 0, 0, 0]], // this is padded with zeros\n</code></pre> <pre><code>    [[1, 0, 0, 0],\n     [0, 1, 0, 0],\n     [0, 0, 1, 0],\n     [0, 0, 0, 1],\n     [0, 0, 0, 0]]])\n</code></pre> Source code in <code>src/stimulus/data/encoding/encoders.py</code> <pre><code>def encode_all(self, data: Union[str, list[str]]) -&gt; torch.Tensor:\n    \"\"\"Encodes a list of sequences.\n\n    Takes a list of string sequences and returns a torch tensor of shape (number_of_sequences, sequence_length, alphabet_length).\n    The returned tensor corresponds to the one hot encoding of the sequences.\n    Unknown characters are represented by a vector of zeros.\n\n    Args:\n        data (Union[str, list[str]]): list of sequences or a single sequence\n\n    Returns:\n        encoded_data (torch.Tensor): one hot encoded sequences\n\n    Raises:\n        TypeError: If the input data is not a list or a string.\n        ValueError: If all sequences do not have the same length when padding is False.\n\n    Examples:\n        &gt;&gt;&gt; encoder = TextOneHotEncoder(alphabet=\"acgt\")\n        &gt;&gt;&gt; encoder.encode_all([\"acgt\", \"acgtn\"])\n        tensor([[[1, 0, 0, 0],\n                 [0, 1, 0, 0],\n                 [0, 0, 1, 0],\n                 [0, 0, 0, 1],\n                 [0, 0, 0, 0]], // this is padded with zeros\n\n                [[1, 0, 0, 0],\n                 [0, 1, 0, 0],\n                 [0, 0, 1, 0],\n                 [0, 0, 0, 1],\n                 [0, 0, 0, 0]]])\n    \"\"\"\n    encoded_data = None  # to prevent UnboundLocalError\n    # encode data\n    if isinstance(data, str):\n        encoded_data = self.encode(data)\n        return torch.stack([encoded_data])\n    if isinstance(data, list):\n        # TODO instead maybe we can run encode_multiprocess when data size is larger than a certain threshold.\n        encoded_list = self.encode_multiprocess(data)\n    else:\n        error_msg = f\"Expected list or string input for data, got {type(data).__name__}\"\n        logger.error(error_msg)\n        raise TypeError(error_msg)\n\n    # handle padding\n    if self.padding:\n        max_length = max([len(d) for d in encoded_list])\n        encoded_data = torch.stack([F.pad(d, (0, 0, 0, max_length - len(d))) for d in encoded_list])\n    else:\n        lengths = {len(d) for d in encoded_list}\n        if len(lengths) &gt; 1:\n            error_msg = \"All sequences must have the same length when padding is False.\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        encoded_data = torch.stack(encoded_list)\n\n    if encoded_data is None:\n        raise ValueError(\"Encoded data is None. This should not happen.\")\n\n    return encoded_data\n</code></pre>"},{"location":"reference/stimulus/data/encoding/encoders/#stimulus.data.encoding.encoders.TextOneHotEncoder.encode_multiprocess","title":"encode_multiprocess","text":"<pre><code>encode_multiprocess(data: list[str]) -&gt; list[Tensor]\n</code></pre> <p>Encodes a list of sequences using multiprocessing.</p> Source code in <code>src/stimulus/data/encoding/encoders.py</code> <pre><code>def encode_multiprocess(self, data: list[str]) -&gt; list[torch.Tensor]:\n    \"\"\"Encodes a list of sequences using multiprocessing.\"\"\"\n    with mp.Pool() as pool:\n        return pool.map(self.encode, data)\n</code></pre>"},{"location":"reference/stimulus/data/splitters/","title":"stimulus.data.splitters","text":""},{"location":"reference/stimulus/data/splitters/#stimulus.data.splitters","title":"splitters","text":"<p>This package provides splitter classes for splitting data into train, validation, and test sets.</p> <p>Modules:</p> <ul> <li> <code>splitters</code>           \u2013            <p>This file contains the splitter classes for splitting data accordingly.</p> </li> </ul>"},{"location":"reference/stimulus/data/splitters/splitters/","title":"stimulus.data.splitters.splitters","text":""},{"location":"reference/stimulus/data/splitters/splitters/#stimulus.data.splitters.splitters","title":"splitters","text":"<p>This file contains the splitter classes for splitting data accordingly.</p> <p>Classes:</p> <ul> <li> <code>AbstractSplitter</code>           \u2013            <p>Abstract class for splitters.</p> </li> <li> <code>RandomSplit</code>           \u2013            <p>This splitter randomly splits the data.</p> </li> </ul>"},{"location":"reference/stimulus/data/splitters/splitters/#stimulus.data.splitters.splitters.AbstractSplitter","title":"AbstractSplitter","text":"<pre><code>AbstractSplitter(seed: float = 42)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract class for splitters.</p> <p>A splitter splits the data into train, validation, and test sets.</p> <p>Methods:</p> <ul> <li> <code>get_split_indexes</code>             \u2013              <p>calculates split indices for the data</p> </li> <li> <code>distance</code>             \u2013              <p>calculates the distance between two elements of the data</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>seed</code>               (<code>float</code>, default:                   <code>42</code> )           \u2013            <p>Random seed for reproducibility</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>distance</code>             \u2013              <p>Calculates the distance between two elements of the data.</p> </li> <li> <code>get_split_indexes</code>             \u2013              <p>Splits the data. Always return indices mapping to the original list.</p> </li> </ul> Source code in <code>src/stimulus/data/splitters/splitters.py</code> <pre><code>def __init__(self, seed: float = 42) -&gt; None:\n    \"\"\"Initialize the splitter.\n\n    Args:\n        seed: Random seed for reproducibility\n    \"\"\"\n    self.seed = seed\n</code></pre>"},{"location":"reference/stimulus/data/splitters/splitters/#stimulus.data.splitters.splitters.AbstractSplitter.distance","title":"distance  <code>abstractmethod</code>","text":"<pre><code>distance(data_one: Any, data_two: Any) -&gt; float\n</code></pre> <p>Calculates the distance between two elements of the data.</p> <p>This is an abstract method that should be implemented by the child class.</p> <p>Parameters:</p> <ul> <li> <code>data_one</code>               (<code>Any</code>)           \u2013            <p>the first data point</p> </li> <li> <code>data_two</code>               (<code>Any</code>)           \u2013            <p>the second data point</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>distance</code> (              <code>float</code> )          \u2013            <p>the distance between the two data points</p> </li> </ul> Source code in <code>src/stimulus/data/splitters/splitters.py</code> <pre><code>@abstractmethod\ndef distance(self, data_one: Any, data_two: Any) -&gt; float:\n    \"\"\"Calculates the distance between two elements of the data.\n\n    This is an abstract method that should be implemented by the child class.\n\n    Args:\n        data_one (Any): the first data point\n        data_two (Any): the second data point\n\n    Returns:\n        distance (float): the distance between the two data points\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/stimulus/data/splitters/splitters/#stimulus.data.splitters.splitters.AbstractSplitter.get_split_indexes","title":"get_split_indexes  <code>abstractmethod</code>","text":"<pre><code>get_split_indexes(data: dict) -&gt; tuple[list, list, list]\n</code></pre> <p>Splits the data. Always return indices mapping to the original list.</p> <p>This is an abstract method that should be implemented by the child class.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>DataFrame</code>)           \u2013            <p>the data to be split</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>split_indices</code> (              <code>list</code> )          \u2013            <p>the indices for train, validation, and test sets</p> </li> </ul> Source code in <code>src/stimulus/data/splitters/splitters.py</code> <pre><code>@abstractmethod\ndef get_split_indexes(self, data: dict) -&gt; tuple[list, list, list]:\n    \"\"\"Splits the data. Always return indices mapping to the original list.\n\n    This is an abstract method that should be implemented by the child class.\n\n    Args:\n        data (pl.DataFrame): the data to be split\n\n    Returns:\n        split_indices (list): the indices for train, validation, and test sets\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/stimulus/data/splitters/splitters/#stimulus.data.splitters.splitters.RandomSplit","title":"RandomSplit","text":"<pre><code>RandomSplit(split: Optional[list] = None, seed: int = 42)\n</code></pre> <p>               Bases: <code>AbstractSplitter</code></p> <p>This splitter randomly splits the data.</p> <p>Parameters:</p> <ul> <li> <code>split</code>               (<code>Optional[list]</code>, default:                   <code>None</code> )           \u2013            <p>List of proportions for train/val/test splits</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>42</code> )           \u2013            <p>Random seed for reproducibility</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>distance</code>             \u2013              <p>Calculate distance between two data points.</p> </li> <li> <code>get_split_indexes</code>             \u2013              <p>Splits the data indices into train, validation, and test sets.</p> </li> </ul> Source code in <code>src/stimulus/data/splitters/splitters.py</code> <pre><code>def __init__(self, split: Optional[list] = None, seed: int = 42) -&gt; None:\n    \"\"\"Initialize the random splitter.\n\n    Args:\n        split: List of proportions for train/val/test splits\n        seed: Random seed for reproducibility\n    \"\"\"\n    super().__init__()\n    self.split = [0.7, 0.2, 0.1] if split is None else split\n    self.seed = seed\n    if len(self.split) != SPLIT_SIZE:\n        raise ValueError(\n            \"The split argument should be a list with length 3 that contains the proportions for [train, validation, test] splits.\",\n        )\n</code></pre>"},{"location":"reference/stimulus/data/splitters/splitters/#stimulus.data.splitters.splitters.RandomSplit.distance","title":"distance","text":"<pre><code>distance(data_one: Any, data_two: Any) -&gt; float\n</code></pre> <p>Calculate distance between two data points.</p> <p>Parameters:</p> <ul> <li> <code>data_one</code>               (<code>Any</code>)           \u2013            <p>First data point</p> </li> <li> <code>data_two</code>               (<code>Any</code>)           \u2013            <p>Second data point</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>Distance between the points</p> </li> </ul> Source code in <code>src/stimulus/data/splitters/splitters.py</code> <pre><code>def distance(self, data_one: Any, data_two: Any) -&gt; float:\n    \"\"\"Calculate distance between two data points.\n\n    Args:\n        data_one: First data point\n        data_two: Second data point\n\n    Returns:\n        Distance between the points\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/stimulus/data/splitters/splitters/#stimulus.data.splitters.splitters.RandomSplit.get_split_indexes","title":"get_split_indexes","text":"<pre><code>get_split_indexes(data: dict) -&gt; tuple[list, list, list]\n</code></pre> <p>Splits the data indices into train, validation, and test sets.</p> <p>One can use these lists of indices to parse the data afterwards.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>dict</code>)           \u2013            <p>Dictionary mapping column names to lists of data values.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>train</code> (              <code>list</code> )          \u2013            <p>The indices for the training set.</p> </li> <li> <code>validation</code> (              <code>list</code> )          \u2013            <p>The indices for the validation set.</p> </li> <li> <code>test</code> (              <code>list</code> )          \u2013            <p>The indices for the test set.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the split argument is not a list with length 3.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the sum of the split proportions is not 1.</p> </li> </ul> Source code in <code>src/stimulus/data/splitters/splitters.py</code> <pre><code>def get_split_indexes(\n    self,\n    data: dict,\n) -&gt; tuple[list, list, list]:\n    \"\"\"Splits the data indices into train, validation, and test sets.\n\n    One can use these lists of indices to parse the data afterwards.\n\n    Args:\n        data (dict): Dictionary mapping column names to lists of data values.\n\n    Returns:\n        train (list): The indices for the training set.\n        validation (list): The indices for the validation set.\n        test (list): The indices for the test set.\n\n    Raises:\n        ValueError: If the split argument is not a list with length 3.\n        ValueError: If the sum of the split proportions is not 1.\n    \"\"\"\n    # Use round to avoid errors due to floating point imprecisions\n    if round(sum(self.split), 3) &lt; 1.0:\n        raise ValueError(f\"The sum of the split proportions should be 1. Instead, it is {sum(self.split)}.\")\n\n    if not data:\n        raise ValueError(\"No data provided for splitting\")\n    # Get length from first column's data list\n    length_of_data = len(next(iter(data.values())))\n\n    # Generate a list of indices and shuffle it\n    indices = np.arange(length_of_data)\n    np.random.seed(self.seed)\n    np.random.shuffle(indices)\n\n    # Calculate the sizes of the train, validation, and test sets\n    train_size = int(self.split[0] * length_of_data)\n    validation_size = int(self.split[1] * length_of_data)\n\n    # Split the shuffled indices according to the calculated sizes\n    train = indices[:train_size].tolist()\n    validation = indices[train_size : train_size + validation_size].tolist()\n    test = indices[train_size + validation_size :].tolist()\n\n    return train, validation, test\n</code></pre>"},{"location":"reference/stimulus/data/transform/","title":"stimulus.data.transform","text":""},{"location":"reference/stimulus/data/transform/#stimulus.data.transform","title":"transform","text":"<p>Transform package for data manipulation.</p> <p>Modules:</p> <ul> <li> <code>data_transformation_generators</code>           \u2013            <p>This file contains noise generators classes for generating various types of noise.</p> </li> </ul>"},{"location":"reference/stimulus/data/transform/data_transformation_generators/","title":"stimulus.data.transform.data_transformation_generators","text":""},{"location":"reference/stimulus/data/transform/data_transformation_generators/#stimulus.data.transform.data_transformation_generators","title":"data_transformation_generators","text":"<p>This file contains noise generators classes for generating various types of noise.</p> <p>Classes:</p> <ul> <li> <code>AbstractAugmentationGenerator</code>           \u2013            <p>Abstract class for augmentation generators.</p> </li> <li> <code>AbstractDataTransformer</code>           \u2013            <p>Abstract class for data transformers.</p> </li> <li> <code>AbstractNoiseGenerator</code>           \u2013            <p>Abstract class for noise generators.</p> </li> <li> <code>GaussianChunk</code>           \u2013            <p>Subset data around a random midpoint.</p> </li> <li> <code>GaussianNoise</code>           \u2013            <p>Add Gaussian noise to data.</p> </li> <li> <code>ReverseComplement</code>           \u2013            <p>Reverse complement biological sequences.</p> </li> <li> <code>UniformTextMasker</code>           \u2013            <p>Mask characters in text.</p> </li> </ul>"},{"location":"reference/stimulus/data/transform/data_transformation_generators/#stimulus.data.transform.data_transformation_generators.AbstractAugmentationGenerator","title":"AbstractAugmentationGenerator","text":"<pre><code>AbstractAugmentationGenerator()\n</code></pre> <p>               Bases: <code>AbstractDataTransformer</code></p> <p>Abstract class for augmentation generators.</p> <p>All augmentation function should have the seed in it. This is because the multiprocessing of them could unset the seed.</p> <p>Methods:</p> <ul> <li> <code>transform</code>             \u2013              <p>Transforms a single data point.</p> </li> <li> <code>transform_all</code>             \u2013              <p>Transforms a list of data points.</p> </li> </ul> Source code in <code>src/stimulus/data/transform/data_transformation_generators.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the augmentation generator.\"\"\"\n    super().__init__()\n    self.add_row = True\n</code></pre>"},{"location":"reference/stimulus/data/transform/data_transformation_generators/#stimulus.data.transform.data_transformation_generators.AbstractAugmentationGenerator.transform","title":"transform  <code>abstractmethod</code>","text":"<pre><code>transform(data: Any) -&gt; Any\n</code></pre> <p>Transforms a single data point.</p> <p>This is an abstract method that should be implemented by the child class.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Any</code>)           \u2013            <p>the data to be transformed</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>transformed_data</code> (              <code>Any</code> )          \u2013            <p>the transformed data</p> </li> </ul> Source code in <code>src/stimulus/data/transform/data_transformation_generators.py</code> <pre><code>@abstractmethod\ndef transform(self, data: Any) -&gt; Any:\n    \"\"\"Transforms a single data point.\n\n    This is an abstract method that should be implemented by the child class.\n\n    Args:\n        data (Any): the data to be transformed\n\n    Returns:\n        transformed_data (Any): the transformed data\n    \"\"\"\n    #  np.random.seed(self.seed)\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/stimulus/data/transform/data_transformation_generators/#stimulus.data.transform.data_transformation_generators.AbstractAugmentationGenerator.transform_all","title":"transform_all  <code>abstractmethod</code>","text":"<pre><code>transform_all(data: list) -&gt; list\n</code></pre> <p>Transforms a list of data points.</p> <p>This is an abstract method that should be implemented by the child class.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>list</code>)           \u2013            <p>the data to be transformed</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>transformed_data</code> (              <code>list</code> )          \u2013            <p>the transformed data</p> </li> </ul> Source code in <code>src/stimulus/data/transform/data_transformation_generators.py</code> <pre><code>@abstractmethod\ndef transform_all(self, data: list) -&gt; list:\n    \"\"\"Transforms a list of data points.\n\n    This is an abstract method that should be implemented by the child class.\n\n    Args:\n        data (list): the data to be transformed\n\n    Returns:\n        transformed_data (list): the transformed data\n    \"\"\"\n    #  np.random.seed(self.seed)\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/stimulus/data/transform/data_transformation_generators/#stimulus.data.transform.data_transformation_generators.AbstractDataTransformer","title":"AbstractDataTransformer","text":"<pre><code>AbstractDataTransformer()\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract class for data transformers.</p> <p>Data transformers implement in_place or augmentation transformations. Whether it is in_place or augmentation is specified in the \"add_row\" attribute (should be True or False and set in children classes constructor)</p> <p>Child classes should override the <code>transform</code> and <code>transform_all</code> methods.</p> <p><code>transform_all</code> should always return a list</p> <p>Both methods should take an optional <code>seed</code> argument set to <code>None</code> by default to be compliant with stimulus' core principle of reproducibility. Seed should be initialized through <code>np.random.seed(seed)</code> in the method implementation.</p> <p>Attributes:</p> <ul> <li> <code>add_row</code>               (<code>bool</code>)           \u2013            <p>whether the transformer adds rows to the data</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>transform</code>             \u2013              <p>transforms a data point</p> </li> <li> <code>transform_all</code>             \u2013              <p>transforms a list of data points</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>transform</code>             \u2013              <p>Transforms a single data point.</p> </li> <li> <code>transform_all</code>             \u2013              <p>Transforms a list of data points.</p> </li> </ul> Source code in <code>src/stimulus/data/transform/data_transformation_generators.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the data transformer.\"\"\"\n    self.add_row: bool = False\n    self.seed: int = 42\n</code></pre>"},{"location":"reference/stimulus/data/transform/data_transformation_generators/#stimulus.data.transform.data_transformation_generators.AbstractDataTransformer.transform","title":"transform  <code>abstractmethod</code>","text":"<pre><code>transform(data: Any) -&gt; Any\n</code></pre> <p>Transforms a single data point.</p> <p>This is an abstract method that should be implemented by the child class.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Any</code>)           \u2013            <p>the data to be transformed</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>transformed_data</code> (              <code>Any</code> )          \u2013            <p>the transformed data</p> </li> </ul> Source code in <code>src/stimulus/data/transform/data_transformation_generators.py</code> <pre><code>@abstractmethod\ndef transform(self, data: Any) -&gt; Any:\n    \"\"\"Transforms a single data point.\n\n    This is an abstract method that should be implemented by the child class.\n\n    Args:\n        data (Any): the data to be transformed\n\n    Returns:\n        transformed_data (Any): the transformed data\n    \"\"\"\n    #  np.random.seed(self.seed)\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/stimulus/data/transform/data_transformation_generators/#stimulus.data.transform.data_transformation_generators.AbstractDataTransformer.transform_all","title":"transform_all  <code>abstractmethod</code>","text":"<pre><code>transform_all(data: list) -&gt; list\n</code></pre> <p>Transforms a list of data points.</p> <p>This is an abstract method that should be implemented by the child class.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>list</code>)           \u2013            <p>the data to be transformed</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>transformed_data</code> (              <code>list</code> )          \u2013            <p>the transformed data</p> </li> </ul> Source code in <code>src/stimulus/data/transform/data_transformation_generators.py</code> <pre><code>@abstractmethod\ndef transform_all(self, data: list) -&gt; list:\n    \"\"\"Transforms a list of data points.\n\n    This is an abstract method that should be implemented by the child class.\n\n    Args:\n        data (list): the data to be transformed\n\n    Returns:\n        transformed_data (list): the transformed data\n    \"\"\"\n    #  np.random.seed(self.seed)\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/stimulus/data/transform/data_transformation_generators/#stimulus.data.transform.data_transformation_generators.AbstractNoiseGenerator","title":"AbstractNoiseGenerator","text":"<pre><code>AbstractNoiseGenerator()\n</code></pre> <p>               Bases: <code>AbstractDataTransformer</code></p> <p>Abstract class for noise generators.</p> <p>All noise function should have the seed in it. This is because the multiprocessing of them could unset the seed.</p> <p>Methods:</p> <ul> <li> <code>transform</code>             \u2013              <p>Transforms a single data point.</p> </li> <li> <code>transform_all</code>             \u2013              <p>Transforms a list of data points.</p> </li> </ul> Source code in <code>src/stimulus/data/transform/data_transformation_generators.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the noise generator.\"\"\"\n    super().__init__()\n    self.add_row = False\n</code></pre>"},{"location":"reference/stimulus/data/transform/data_transformation_generators/#stimulus.data.transform.data_transformation_generators.AbstractNoiseGenerator.transform","title":"transform  <code>abstractmethod</code>","text":"<pre><code>transform(data: Any) -&gt; Any\n</code></pre> <p>Transforms a single data point.</p> <p>This is an abstract method that should be implemented by the child class.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Any</code>)           \u2013            <p>the data to be transformed</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>transformed_data</code> (              <code>Any</code> )          \u2013            <p>the transformed data</p> </li> </ul> Source code in <code>src/stimulus/data/transform/data_transformation_generators.py</code> <pre><code>@abstractmethod\ndef transform(self, data: Any) -&gt; Any:\n    \"\"\"Transforms a single data point.\n\n    This is an abstract method that should be implemented by the child class.\n\n    Args:\n        data (Any): the data to be transformed\n\n    Returns:\n        transformed_data (Any): the transformed data\n    \"\"\"\n    #  np.random.seed(self.seed)\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/stimulus/data/transform/data_transformation_generators/#stimulus.data.transform.data_transformation_generators.AbstractNoiseGenerator.transform_all","title":"transform_all  <code>abstractmethod</code>","text":"<pre><code>transform_all(data: list) -&gt; list\n</code></pre> <p>Transforms a list of data points.</p> <p>This is an abstract method that should be implemented by the child class.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>list</code>)           \u2013            <p>the data to be transformed</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>transformed_data</code> (              <code>list</code> )          \u2013            <p>the transformed data</p> </li> </ul> Source code in <code>src/stimulus/data/transform/data_transformation_generators.py</code> <pre><code>@abstractmethod\ndef transform_all(self, data: list) -&gt; list:\n    \"\"\"Transforms a list of data points.\n\n    This is an abstract method that should be implemented by the child class.\n\n    Args:\n        data (list): the data to be transformed\n\n    Returns:\n        transformed_data (list): the transformed data\n    \"\"\"\n    #  np.random.seed(self.seed)\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/stimulus/data/transform/data_transformation_generators/#stimulus.data.transform.data_transformation_generators.GaussianChunk","title":"GaussianChunk","text":"<pre><code>GaussianChunk(\n    chunk_size: int, seed: int = 42, std: float = 1\n)\n</code></pre> <p>               Bases: <code>AbstractAugmentationGenerator</code></p> <p>Subset data around a random midpoint.</p> <p>This augmentation strategy chunks the input sequences, for which the middle positions are obtained through a gaussian distribution.</p> <p>In concrete, it changes the middle position (ie. peak summit) to another position. This position is chosen based on a gaussian distribution, so the region close to the middle point are more likely to be chosen than the rest. Then a chunk with size <code>chunk_size</code> around the new middle point is returned. This process will be repeated for each sequence with <code>transform_all</code>.</p> <p>Methods:</p> <ul> <li> <code>transform</code>             \u2013              <p>chunk a single list</p> </li> <li> <code>transform_all</code>             \u2013              <p>chunks multiple lists</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>chunk_size</code>               (<code>int</code>)           \u2013            <p>Size of chunks to extract</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>42</code> )           \u2013            <p>Random seed for reproducibility</p> </li> <li> <code>std</code>               (<code>float</code>, default:                   <code>1</code> )           \u2013            <p>Standard deviation for the Gaussian distribution</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>transform</code>             \u2013              <p>Chunks a sequence of size chunk_size from the middle position +/- a value obtained through a gaussian distribution.</p> </li> <li> <code>transform_all</code>             \u2013              <p>Adds chunks to multiple lists using multiprocessing.</p> </li> </ul> Source code in <code>src/stimulus/data/transform/data_transformation_generators.py</code> <pre><code>def __init__(self, chunk_size: int, seed: int = 42, std: float = 1) -&gt; None:\n    \"\"\"Initialize the Gaussian chunk generator.\n\n    Args:\n        chunk_size: Size of chunks to extract\n        seed: Random seed for reproducibility\n        std: Standard deviation for the Gaussian distribution\n    \"\"\"\n    super().__init__()\n    self.chunk_size = chunk_size\n    self.seed = seed\n    self.std = std\n</code></pre>"},{"location":"reference/stimulus/data/transform/data_transformation_generators/#stimulus.data.transform.data_transformation_generators.GaussianChunk.transform","title":"transform","text":"<pre><code>transform(data: str) -&gt; str\n</code></pre> <p>Chunks a sequence of size chunk_size from the middle position +/- a value obtained through a gaussian distribution.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>str</code>)           \u2013            <p>the sequence to be transformed</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>transformed_data</code> (              <code>str</code> )          \u2013            <p>the chunk of the sequence</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>AssertionError</code>             \u2013            <p>if the input data is shorter than the chunk size</p> </li> </ul> Source code in <code>src/stimulus/data/transform/data_transformation_generators.py</code> <pre><code>def transform(self, data: str) -&gt; str:\n    \"\"\"Chunks a sequence of size chunk_size from the middle position +/- a value obtained through a gaussian distribution.\n\n    Args:\n        data (str): the sequence to be transformed\n\n    Returns:\n        transformed_data (str): the chunk of the sequence\n\n    Raises:\n        AssertionError: if the input data is shorter than the chunk size\n    \"\"\"\n    np.random.seed(self.seed)\n\n    # make sure that the data is longer than chunk_size otherwise raise an error\n    if len(data) &lt;= self.chunk_size:\n        raise ValueError(\"The input data is shorter than the chunk size\")\n\n    # Get the middle position of the input sequence\n    middle_position = len(data) // 2\n\n    # Change the middle position by a value obtained through a gaussian distribution\n    new_middle_position = int(middle_position + np.random.normal(0, self.std))\n\n    # Get the start and end position of the chunk\n    start_position = new_middle_position - self.chunk_size // 2\n    end_position = new_middle_position + self.chunk_size // 2\n\n    # if the start position is negative, set it to 0\n    start_position = max(start_position, 0)\n\n    # Get the chunk of size chunk_size from the start position if the end position is smaller than the length of the data\n    if end_position &lt; len(data):\n        return data[start_position : start_position + self.chunk_size]\n    # Otherwise return the chunk of the sequence from the end of the sequence of size chunk_size\n    return data[-self.chunk_size :]\n</code></pre>"},{"location":"reference/stimulus/data/transform/data_transformation_generators/#stimulus.data.transform.data_transformation_generators.GaussianChunk.transform_all","title":"transform_all","text":"<pre><code>transform_all(data: list) -&gt; list\n</code></pre> <p>Adds chunks to multiple lists using multiprocessing.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>list</code>)           \u2013            <p>the sequences to be transformed</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>transformed_data</code> (              <code>list</code> )          \u2013            <p>the transformed sequences</p> </li> </ul> Source code in <code>src/stimulus/data/transform/data_transformation_generators.py</code> <pre><code>def transform_all(self, data: list) -&gt; list:\n    \"\"\"Adds chunks to multiple lists using multiprocessing.\n\n    Args:\n        data (list): the sequences to be transformed\n\n    Returns:\n        transformed_data (list): the transformed sequences\n    \"\"\"\n    with mp.Pool(mp.cpu_count()) as pool:\n        function_specific_input = list(data)\n        return pool.starmap(self.transform, function_specific_input)\n</code></pre>"},{"location":"reference/stimulus/data/transform/data_transformation_generators/#stimulus.data.transform.data_transformation_generators.GaussianNoise","title":"GaussianNoise","text":"<pre><code>GaussianNoise(\n    mean: float = 0, std: float = 1, seed: int = 42\n)\n</code></pre> <p>               Bases: <code>AbstractNoiseGenerator</code></p> <p>Add Gaussian noise to data.</p> <p>This noise generator adds Gaussian noise to float values.</p> <p>Methods:</p> <ul> <li> <code>transform</code>             \u2013              <p>adds noise to a single data point</p> </li> <li> <code>transform_all</code>             \u2013              <p>adds noise to a list of data points</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>mean</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Mean of the Gaussian noise</p> </li> <li> <code>std</code>               (<code>float</code>, default:                   <code>1</code> )           \u2013            <p>Standard deviation of the Gaussian noise</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>42</code> )           \u2013            <p>Random seed for reproducibility</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>transform</code>             \u2013              <p>Adds Gaussian noise to a single point of data.</p> </li> <li> <code>transform_all</code>             \u2013              <p>Adds Gaussian noise to a list of data points.</p> </li> </ul> Source code in <code>src/stimulus/data/transform/data_transformation_generators.py</code> <pre><code>def __init__(self, mean: float = 0, std: float = 1, seed: int = 42) -&gt; None:\n    \"\"\"Initialize the Gaussian noise generator.\n\n    Args:\n        mean: Mean of the Gaussian noise\n        std: Standard deviation of the Gaussian noise\n        seed: Random seed for reproducibility\n    \"\"\"\n    super().__init__()\n    self.mean = mean\n    self.std = std\n    self.seed = seed\n</code></pre>"},{"location":"reference/stimulus/data/transform/data_transformation_generators/#stimulus.data.transform.data_transformation_generators.GaussianNoise.transform","title":"transform","text":"<pre><code>transform(data: float) -&gt; float\n</code></pre> <p>Adds Gaussian noise to a single point of data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>float</code>)           \u2013            <p>the data to be transformed</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>transformed_data</code> (              <code>float</code> )          \u2013            <p>the transformed data point</p> </li> </ul> Source code in <code>src/stimulus/data/transform/data_transformation_generators.py</code> <pre><code>def transform(self, data: float) -&gt; float:\n    \"\"\"Adds Gaussian noise to a single point of data.\n\n    Args:\n        data (float): the data to be transformed\n\n    Returns:\n        transformed_data (float): the transformed data point\n    \"\"\"\n    np.random.seed(self.seed)\n    return data + np.random.normal(self.mean, self.std)\n</code></pre>"},{"location":"reference/stimulus/data/transform/data_transformation_generators/#stimulus.data.transform.data_transformation_generators.GaussianNoise.transform_all","title":"transform_all","text":"<pre><code>transform_all(data: list) -&gt; list\n</code></pre> <p>Adds Gaussian noise to a list of data points.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>list</code>)           \u2013            <p>the data to be transformed</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>transformed_data</code> (              <code>list</code> )          \u2013            <p>the transformed data points</p> </li> </ul> Source code in <code>src/stimulus/data/transform/data_transformation_generators.py</code> <pre><code>def transform_all(self, data: list) -&gt; list:\n    \"\"\"Adds Gaussian noise to a list of data points.\n\n    Args:\n        data (list): the data to be transformed\n\n    Returns:\n        transformed_data (list): the transformed data points\n    \"\"\"\n    np.random.seed(self.seed)\n    return list(np.array(data) + np.random.normal(self.mean, self.std, len(data)))\n</code></pre>"},{"location":"reference/stimulus/data/transform/data_transformation_generators/#stimulus.data.transform.data_transformation_generators.ReverseComplement","title":"ReverseComplement","text":"<pre><code>ReverseComplement(sequence_type: str = 'DNA')\n</code></pre> <p>               Bases: <code>AbstractAugmentationGenerator</code></p> <p>Reverse complement biological sequences.</p> <p>This augmentation strategy reverse complements the input nucleotide sequences.</p> <p>Methods:</p> <ul> <li> <code>transform</code>             \u2013              <p>reverse complements a single data point</p> </li> <li> <code>transform_all</code>             \u2013              <p>reverse complements a list of data points</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>if the type of the sequence is not DNA or RNA</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>sequence_type</code>               (<code>str</code>, default:                   <code>'DNA'</code> )           \u2013            <p>Type of sequence ('DNA' or 'RNA')</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>transform</code>             \u2013              <p>Returns the reverse complement of a list of string data using the complement_mapping.</p> </li> <li> <code>transform_all</code>             \u2013              <p>Reverse complement multiple data points using multiprocessing.</p> </li> </ul> Source code in <code>src/stimulus/data/transform/data_transformation_generators.py</code> <pre><code>def __init__(self, sequence_type: str = \"DNA\") -&gt; None:\n    \"\"\"Initialize the reverse complement generator.\n\n    Args:\n        sequence_type: Type of sequence ('DNA' or 'RNA')\n    \"\"\"\n    super().__init__()\n    if sequence_type not in (\"DNA\", \"RNA\"):\n        raise ValueError(\n            \"Currently only DNA and RNA sequences are supported. Update the class ReverseComplement to support other types.\",\n        )\n    if sequence_type == \"DNA\":\n        self.complement_mapping = str.maketrans(\"ATCG\", \"TAGC\")\n    elif sequence_type == \"RNA\":\n        self.complement_mapping = str.maketrans(\"AUCG\", \"UAGC\")\n</code></pre>"},{"location":"reference/stimulus/data/transform/data_transformation_generators/#stimulus.data.transform.data_transformation_generators.ReverseComplement.transform","title":"transform","text":"<pre><code>transform(data: str) -&gt; str\n</code></pre> <p>Returns the reverse complement of a list of string data using the complement_mapping.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>str</code>)           \u2013            <p>the sequence to be transformed</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>transformed_data</code> (              <code>str</code> )          \u2013            <p>the reverse complement of the sequence</p> </li> </ul> Source code in <code>src/stimulus/data/transform/data_transformation_generators.py</code> <pre><code>def transform(self, data: str) -&gt; str:\n    \"\"\"Returns the reverse complement of a list of string data using the complement_mapping.\n\n    Args:\n        data (str): the sequence to be transformed\n\n    Returns:\n        transformed_data (str): the reverse complement of the sequence\n    \"\"\"\n    return data.translate(self.complement_mapping)[::-1]\n</code></pre>"},{"location":"reference/stimulus/data/transform/data_transformation_generators/#stimulus.data.transform.data_transformation_generators.ReverseComplement.transform_all","title":"transform_all","text":"<pre><code>transform_all(data: list) -&gt; list\n</code></pre> <p>Reverse complement multiple data points using multiprocessing.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>list</code>)           \u2013            <p>the sequences to be transformed</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>transformed_data</code> (              <code>list</code> )          \u2013            <p>the reverse complement of the sequences</p> </li> </ul> Source code in <code>src/stimulus/data/transform/data_transformation_generators.py</code> <pre><code>def transform_all(self, data: list) -&gt; list:\n    \"\"\"Reverse complement multiple data points using multiprocessing.\n\n    Args:\n        data (list): the sequences to be transformed\n\n    Returns:\n        transformed_data (list): the reverse complement of the sequences\n    \"\"\"\n    with mp.Pool(mp.cpu_count()) as pool:\n        function_specific_input = list(data)\n        return pool.map(self.transform, function_specific_input)\n</code></pre>"},{"location":"reference/stimulus/data/transform/data_transformation_generators/#stimulus.data.transform.data_transformation_generators.UniformTextMasker","title":"UniformTextMasker","text":"<pre><code>UniformTextMasker(\n    probability: float = 0.1,\n    mask: str = \"*\",\n    seed: int = 42,\n)\n</code></pre> <p>               Bases: <code>AbstractNoiseGenerator</code></p> <p>Mask characters in text.</p> <p>This noise generators replace characters with a masking character with a given probability.</p> <p>Methods:</p> <ul> <li> <code>transform</code>             \u2013              <p>adds character masking to a single data point</p> </li> <li> <code>transform_all</code>             \u2013              <p>adds character masking to a list of data points</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>probability</code>               (<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>Probability of masking each character</p> </li> <li> <code>mask</code>               (<code>str</code>, default:                   <code>'*'</code> )           \u2013            <p>Character to use for masking</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>42</code> )           \u2013            <p>Random seed for reproducibility</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>transform</code>             \u2013              <p>Adds character masking to the data.</p> </li> <li> <code>transform_all</code>             \u2013              <p>Adds character masking to multiple data points using multiprocessing.</p> </li> </ul> Source code in <code>src/stimulus/data/transform/data_transformation_generators.py</code> <pre><code>def __init__(self, probability: float = 0.1, mask: str = \"*\", seed: int = 42) -&gt; None:\n    \"\"\"Initialize the text masker.\n\n    Args:\n        probability: Probability of masking each character\n        mask: Character to use for masking\n        seed: Random seed for reproducibility\n    \"\"\"\n    super().__init__()\n    self.probability = probability\n    self.mask = mask\n    self.seed = seed\n</code></pre>"},{"location":"reference/stimulus/data/transform/data_transformation_generators/#stimulus.data.transform.data_transformation_generators.UniformTextMasker.transform","title":"transform","text":"<pre><code>transform(data: str) -&gt; str\n</code></pre> <p>Adds character masking to the data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>str</code>)           \u2013            <p>the data to be transformed</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>transformed_data</code> (              <code>str</code> )          \u2013            <p>the transformed data point</p> </li> </ul> Source code in <code>src/stimulus/data/transform/data_transformation_generators.py</code> <pre><code>def transform(self, data: str) -&gt; str:\n    \"\"\"Adds character masking to the data.\n\n    Args:\n        data (str): the data to be transformed\n\n    Returns:\n        transformed_data (str): the transformed data point\n    \"\"\"\n    np.random.seed(self.seed)\n    return \"\".join([c if np.random.rand() &gt; self.probability else self.mask for c in data])\n</code></pre>"},{"location":"reference/stimulus/data/transform/data_transformation_generators/#stimulus.data.transform.data_transformation_generators.UniformTextMasker.transform_all","title":"transform_all","text":"<pre><code>transform_all(data: list) -&gt; list\n</code></pre> <p>Adds character masking to multiple data points using multiprocessing.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>list</code>)           \u2013            <p>the data to be transformed</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>transformed_data</code> (              <code>list</code> )          \u2013            <p>the transformed data points</p> </li> </ul> Source code in <code>src/stimulus/data/transform/data_transformation_generators.py</code> <pre><code>def transform_all(self, data: list) -&gt; list:\n    \"\"\"Adds character masking to multiple data points using multiprocessing.\n\n    Args:\n        data (list): the data to be transformed\n\n\n    Returns:\n        transformed_data (list): the transformed data points\n    \"\"\"\n    with mp.Pool(mp.cpu_count()) as pool:\n        function_specific_input = list(data)\n        return pool.starmap(self.transform, function_specific_input)\n</code></pre>"},{"location":"reference/stimulus/learner/","title":"stimulus.learner","text":""},{"location":"reference/stimulus/learner/#stimulus.learner","title":"learner","text":"<p>Learner package for model training and evaluation.</p> <p>Modules:</p> <ul> <li> <code>predict</code>           \u2013            <p>A module for making predictions with PyTorch models using DataLoaders.</p> </li> <li> <code>raytune_learner</code>           \u2013            <p>Ray Tune wrapper and trainable model classes for hyperparameter optimization.</p> </li> <li> <code>raytune_parser</code>           \u2013            <p>Ray Tune results parser for extracting and saving best model configurations and weights.</p> </li> </ul>"},{"location":"reference/stimulus/learner/predict/","title":"stimulus.learner.predict","text":""},{"location":"reference/stimulus/learner/predict/#stimulus.learner.predict","title":"predict","text":"<p>A module for making predictions with PyTorch models using DataLoaders.</p> <p>Classes:</p> <ul> <li> <code>PredictWrapper</code>           \u2013            <p>A wrapper to predict the output of a model on a datset loaded into a torch DataLoader.</p> </li> </ul>"},{"location":"reference/stimulus/learner/predict/#stimulus.learner.predict.PredictWrapper","title":"PredictWrapper","text":"<pre><code>PredictWrapper(\n    model: Module,\n    dataloader: DataLoader,\n    loss_dict: Optional[dict[str, Any]] = None,\n)\n</code></pre> <p>A wrapper to predict the output of a model on a datset loaded into a torch DataLoader.</p> <p>It also provides the functionalities to measure the performance of the model.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The PyTorch model to make predictions with</p> </li> <li> <code>dataloader</code>               (<code>DataLoader</code>)           \u2013            <p>DataLoader containing the evaluation data</p> </li> <li> <code>loss_dict</code>               (<code>Optional[dict[str, Any]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional dictionary of loss functions</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>compute_loss</code>             \u2013              <p>Compute the loss.</p> </li> <li> <code>compute_metric</code>             \u2013              <p>Wrapper to compute the performance metric.</p> </li> <li> <code>compute_metrics</code>             \u2013              <p>Wrapper to compute the performance metrics.</p> </li> <li> <code>compute_other_metric</code>             \u2013              <p>Compute the performance metric.</p> </li> <li> <code>handle_predictions</code>             \u2013              <p>Handle the model outputs from forward pass, into a dictionary of tensors, just like y.</p> </li> <li> <code>predict</code>             \u2013              <p>Get the model predictions.</p> </li> </ul> Source code in <code>src/stimulus/learner/predict.py</code> <pre><code>def __init__(self, model: nn.Module, dataloader: DataLoader, loss_dict: Optional[dict[str, Any]] = None) -&gt; None:\n    \"\"\"Initialize the PredictWrapper.\n\n    Args:\n        model: The PyTorch model to make predictions with\n        dataloader: DataLoader containing the evaluation data\n        loss_dict: Optional dictionary of loss functions\n    \"\"\"\n    self.model = model\n    self.dataloader = dataloader\n    self.loss_dict = loss_dict\n    try:\n        self.model.eval()\n    except RuntimeError as e:\n        # Using logging instead of print\n        import logging\n\n        logging.warning(\"Not able to run model.eval: %s\", str(e))\n</code></pre>"},{"location":"reference/stimulus/learner/predict/#stimulus.learner.predict.PredictWrapper.compute_loss","title":"compute_loss","text":"<pre><code>compute_loss() -&gt; float\n</code></pre> <p>Compute the loss.</p> <p>The current implmentation basically computes the loss for each batch and then averages them. TODO we could potentially summarize the los across batches in a different way. Or sometimes we may potentially even have 1+ losses.</p> Source code in <code>src/stimulus/learner/predict.py</code> <pre><code>def compute_loss(self) -&gt; float:\n    \"\"\"Compute the loss.\n\n    The current implmentation basically computes the loss for each batch and then averages them.\n    TODO we could potentially summarize the los across batches in a different way.\n    Or sometimes we may potentially even have 1+ losses.\n    \"\"\"\n    if self.loss_dict is None:\n        raise ValueError(\"Loss function is not provided.\")\n    loss = 0.0\n    with torch.no_grad():\n        for x, y, _ in self.dataloader:\n            # the loss_dict could be unpacked with ** and the function declaration handle it differently like **kwargs. to be decided, personally find this more clean and understable.\n            current_loss = self.model.batch(x=x, y=y, **self.loss_dict)[0]\n            loss += current_loss.item()\n    return loss / len(self.dataloader)\n</code></pre>"},{"location":"reference/stimulus/learner/predict/#stimulus.learner.predict.PredictWrapper.compute_metric","title":"compute_metric","text":"<pre><code>compute_metric(metric: str = 'loss') -&gt; float\n</code></pre> <p>Wrapper to compute the performance metric.</p> Source code in <code>src/stimulus/learner/predict.py</code> <pre><code>def compute_metric(self, metric: str = \"loss\") -&gt; float:\n    \"\"\"Wrapper to compute the performance metric.\"\"\"\n    if metric == \"loss\":\n        return self.compute_loss()\n    return self.compute_other_metric(metric)\n</code></pre>"},{"location":"reference/stimulus/learner/predict/#stimulus.learner.predict.PredictWrapper.compute_metrics","title":"compute_metrics","text":"<pre><code>compute_metrics(metrics: list[str]) -&gt; dict[str, float]\n</code></pre> <p>Wrapper to compute the performance metrics.</p> Source code in <code>src/stimulus/learner/predict.py</code> <pre><code>def compute_metrics(self, metrics: list[str]) -&gt; dict[str, float]:\n    \"\"\"Wrapper to compute the performance metrics.\"\"\"\n    return {m: self.compute_metric(m) for m in metrics}\n</code></pre>"},{"location":"reference/stimulus/learner/predict/#stimulus.learner.predict.PredictWrapper.compute_other_metric","title":"compute_other_metric","text":"<pre><code>compute_other_metric(metric: str) -&gt; float\n</code></pre> <p>Compute the performance metric.</p>"},{"location":"reference/stimulus/learner/predict/#stimulus.learner.predict.PredictWrapper.compute_other_metric--todo-currently-we-computes-the-average-performance-metric-across-target-y-but-maybe-in-the-future-we-want-something-different","title":"TODO currently we computes the average performance metric across target y, but maybe in the future we want something different","text":"Source code in <code>src/stimulus/learner/predict.py</code> <pre><code>def compute_other_metric(self, metric: str) -&gt; float:\n    \"\"\"Compute the performance metric.\n\n    # TODO currently we computes the average performance metric across target y, but maybe in the future we want something different\n    \"\"\"\n    if not hasattr(self, \"predictions\") or not hasattr(self, \"labels\"):\n        predictions, labels = self.predict(return_labels=True)\n        self.predictions = predictions\n        self.labels = labels\n\n    # Explicitly type the labels and predictions as dictionaries with str keys\n    labels_dict: dict[str, Tensor] = self.labels if isinstance(self.labels, dict) else {}\n    predictions_dict: dict[str, Tensor] = self.predictions if isinstance(self.predictions, dict) else {}\n\n    return sum(\n        Performance(labels=labels_dict[k], predictions=predictions_dict[k], metric=metric).val for k in labels_dict\n    ) / len(labels_dict)\n</code></pre>"},{"location":"reference/stimulus/learner/predict/#stimulus.learner.predict.PredictWrapper.handle_predictions","title":"handle_predictions","text":"<pre><code>handle_predictions(\n    predictions: Any, y: dict[str, Tensor]\n) -&gt; dict[str, Tensor]\n</code></pre> <p>Handle the model outputs from forward pass, into a dictionary of tensors, just like y.</p> Source code in <code>src/stimulus/learner/predict.py</code> <pre><code>def handle_predictions(self, predictions: Any, y: dict[str, Tensor]) -&gt; dict[str, Tensor]:\n    \"\"\"Handle the model outputs from forward pass, into a dictionary of tensors, just like y.\"\"\"\n    if len(y) == 1:\n        return {next(iter(y.keys())): predictions}\n    return dict(zip(y.keys(), predictions))\n</code></pre>"},{"location":"reference/stimulus/learner/predict/#stimulus.learner.predict.PredictWrapper.predict","title":"predict","text":"<pre><code>predict(*, return_labels: bool = False) -&gt; Union[\n    dict[str, Tensor],\n    tuple[dict[str, Tensor], dict[str, Tensor]],\n]\n</code></pre> <p>Get the model predictions.</p> <p>Basically, it runs a foward pass on the model for each batch, gets the predictions and concatenate them for all batches. Since the returned <code>current_predictions</code> are formed by tensors computed for one batch, the final <code>predictions</code> are obtained by concatenating them.</p> <p>At the end it returns <code>predictions</code> as a dictionary of tensors with the same keys as <code>y</code>.</p> <p>If return_labels if True, then the <code>labels</code> will be returned as well, also as a dictionary of tensors.</p> <p>Parameters:</p> <ul> <li> <code>return_labels</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to also return the labels</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[dict[str, Tensor], tuple[dict[str, Tensor], dict[str, Tensor]]]</code>           \u2013            <p>Dictionary of predictions, and optionally labels</p> </li> </ul> Source code in <code>src/stimulus/learner/predict.py</code> <pre><code>def predict(\n    self,\n    *,\n    return_labels: bool = False,\n) -&gt; Union[dict[str, Tensor], tuple[dict[str, Tensor], dict[str, Tensor]]]:\n    \"\"\"Get the model predictions.\n\n    Basically, it runs a foward pass on the model for each batch,\n    gets the predictions and concatenate them for all batches.\n    Since the returned `current_predictions` are formed by tensors computed for one batch,\n    the final `predictions` are obtained by concatenating them.\n\n    At the end it returns `predictions` as a dictionary of tensors with the same keys as `y`.\n\n    If return_labels if True, then the `labels` will be returned as well, also as a dictionary of tensors.\n\n    Args:\n        return_labels: Whether to also return the labels\n\n    Returns:\n        Dictionary of predictions, and optionally labels\n    \"\"\"\n    # create empty dictionaries with the column names\n    first_batch = next(iter(self.dataloader))\n    keys = first_batch[1].keys()\n    predictions: dict[str, list[Tensor]] = {k: [] for k in keys}\n    labels: dict[str, list[Tensor]] = {k: [] for k in keys}\n\n    # get the predictions (and labels) for each batch\n    with torch.no_grad():\n        for x, y, _ in self.dataloader:\n            current_predictions = self.model(**x)\n            current_predictions = self.handle_predictions(current_predictions, y)\n            for k in keys:\n                # it might happen that the batch consists of one element only so the torch.cat will fail. To prevent this the function to ensure at least one dimensionality is called.\n                predictions[k].append(ensure_at_least_1d(current_predictions[k]))\n                if return_labels:\n                    labels[k].append(ensure_at_least_1d(y[k]))\n\n    # return the predictions (and labels) as a dictionary of tensors for the entire dataset.\n    if not return_labels:\n        return {k: torch.cat(v) for k, v in predictions.items()}\n    return {k: torch.cat(v) for k, v in predictions.items()}, {k: torch.cat(v) for k, v in labels.items()}\n</code></pre>"},{"location":"reference/stimulus/learner/raytune_learner/","title":"stimulus.learner.raytune_learner","text":""},{"location":"reference/stimulus/learner/raytune_learner/#stimulus.learner.raytune_learner","title":"raytune_learner","text":"<p>Ray Tune wrapper and trainable model classes for hyperparameter optimization.</p> <p>Classes:</p> <ul> <li> <code>CheckpointDict</code>           \u2013            <p>Dictionary type for checkpoint data.</p> </li> <li> <code>TuneModel</code>           \u2013            <p>Trainable model class for Ray Tune.</p> </li> <li> <code>TuneWrapper</code>           \u2013            <p>Wrapper class for Ray Tune hyperparameter optimization.</p> </li> </ul>"},{"location":"reference/stimulus/learner/raytune_learner/#stimulus.learner.raytune_learner.CheckpointDict","title":"CheckpointDict","text":"<p>               Bases: <code>TypedDict</code></p> <p>Dictionary type for checkpoint data.</p>"},{"location":"reference/stimulus/learner/raytune_learner/#stimulus.learner.raytune_learner.TuneModel","title":"TuneModel","text":"<p>               Bases: <code>Trainable</code></p> <p>Trainable model class for Ray Tune.</p> <p>Methods:</p> <ul> <li> <code>export_model</code>             \u2013              <p>Export model to safetensors format.</p> </li> <li> <code>load_checkpoint</code>             \u2013              <p>Load model and optimizer state from checkpoint.</p> </li> <li> <code>objective</code>             \u2013              <p>Compute the objective metric(s) for the tuning process.</p> </li> <li> <code>save_checkpoint</code>             \u2013              <p>Save model and optimizer state to checkpoint.</p> </li> <li> <code>setup</code>             \u2013              <p>Get the model, loss function(s), optimizer, train and test data from the config.</p> </li> <li> <code>step</code>             \u2013              <p>For each batch in the training data, calculate the loss and update the model parameters.</p> </li> </ul>"},{"location":"reference/stimulus/learner/raytune_learner/#stimulus.learner.raytune_learner.TuneModel.export_model","title":"export_model","text":"<pre><code>export_model(export_dir: str | None = None) -&gt; None\n</code></pre> <p>Export model to safetensors format.</p> Source code in <code>src/stimulus/learner/raytune_learner.py</code> <pre><code>def export_model(self, export_dir: str | None = None) -&gt; None:  # type: ignore[override]\n    \"\"\"Export model to safetensors format.\"\"\"\n    if export_dir is None:\n        return\n    safe_save_model(self.model, os.path.join(export_dir, \"model.safetensors\"))\n</code></pre>"},{"location":"reference/stimulus/learner/raytune_learner/#stimulus.learner.raytune_learner.TuneModel.load_checkpoint","title":"load_checkpoint","text":"<pre><code>load_checkpoint(checkpoint: dict[Any, Any] | None) -&gt; None\n</code></pre> <p>Load model and optimizer state from checkpoint.</p> Source code in <code>src/stimulus/learner/raytune_learner.py</code> <pre><code>def load_checkpoint(self, checkpoint: dict[Any, Any] | None) -&gt; None:\n    \"\"\"Load model and optimizer state from checkpoint.\"\"\"\n    if checkpoint is None:\n        return\n    checkpoint_dir = checkpoint[\"checkpoint_dir\"]\n    self.model = safe_load_model(self.model, os.path.join(checkpoint_dir, \"model.safetensors\"))\n    self.optimizer.load_state_dict(torch.load(os.path.join(checkpoint_dir, \"optimizer.pt\")))\n</code></pre>"},{"location":"reference/stimulus/learner/raytune_learner/#stimulus.learner.raytune_learner.TuneModel.objective","title":"objective","text":"<pre><code>objective() -&gt; dict[str, float]\n</code></pre> <p>Compute the objective metric(s) for the tuning process.</p> Source code in <code>src/stimulus/learner/raytune_learner.py</code> <pre><code>def objective(self) -&gt; dict[str, float]:\n    \"\"\"Compute the objective metric(s) for the tuning process.\"\"\"\n    metrics = [\n        \"loss\",\n        \"rocauc\",\n        \"prauc\",\n        \"mcc\",\n        \"f1score\",\n        \"precision\",\n        \"recall\",\n        \"spearmanr\",\n    ]  # TODO maybe we report only a subset of metrics, given certain criteria (eg. if classification or regression)\n    predict_val = PredictWrapper(self.model, self.validation, loss_dict=self.loss_dict)\n    predict_train = PredictWrapper(self.model, self.training, loss_dict=self.loss_dict)\n    return {\n        **{\"val_\" + metric: value for metric, value in predict_val.compute_metrics(metrics).items()},\n        **{\"train_\" + metric: value for metric, value in predict_train.compute_metrics(metrics).items()},\n    }\n</code></pre>"},{"location":"reference/stimulus/learner/raytune_learner/#stimulus.learner.raytune_learner.TuneModel.save_checkpoint","title":"save_checkpoint","text":"<pre><code>save_checkpoint(checkpoint_dir: str) -&gt; dict[Any, Any]\n</code></pre> <p>Save model and optimizer state to checkpoint.</p> Source code in <code>src/stimulus/learner/raytune_learner.py</code> <pre><code>def save_checkpoint(self, checkpoint_dir: str) -&gt; dict[Any, Any]:\n    \"\"\"Save model and optimizer state to checkpoint.\"\"\"\n    safe_save_model(self.model, os.path.join(checkpoint_dir, \"model.safetensors\"))\n    torch.save(self.optimizer.state_dict(), os.path.join(checkpoint_dir, \"optimizer.pt\"))\n    return {\"checkpoint_dir\": checkpoint_dir}\n</code></pre>"},{"location":"reference/stimulus/learner/raytune_learner/#stimulus.learner.raytune_learner.TuneModel.setup","title":"setup","text":"<pre><code>setup(config: dict[Any, Any]) -&gt; None\n</code></pre> <p>Get the model, loss function(s), optimizer, train and test data from the config.</p> Source code in <code>src/stimulus/learner/raytune_learner.py</code> <pre><code>def setup(self, config: dict[Any, Any]) -&gt; None:\n    \"\"\"Get the model, loss function(s), optimizer, train and test data from the config.\"\"\"\n    # set the seeds the second time, first in TuneWrapper initialization. This will make all important seed worker specific.\n    set_general_seeds(self.config[\"ray_worker_seed\"])\n\n    # Initialize model with the config params\n    self.model = config[\"model\"](**config[\"model_params\"])\n\n    # Add data path\n    self.data_path = config[\"data_path\"]\n\n    # Get the loss function(s) from the config model params\n    # Note that the loss function(s) are stored in a dictionary,\n    # where the keys are the key of loss_params in the yaml config file and the values are the loss functions associated to such keys.\n    self.loss_dict = config[\"loss_params\"]\n    for key, loss_fn in self.loss_dict.items():\n        try:\n            self.loss_dict[key] = getattr(nn, loss_fn)()\n        except AttributeError as err:\n            raise ValueError(\n                f\"Invalid loss function: {loss_fn}, check PyTorch for documentation on available loss functions\",\n            ) from err\n\n    # get the optimizer parameters\n    optimizer_lr = config[\"optimizer_params\"][\"lr\"]\n\n    # get the optimizer from PyTorch\n    self.optimizer = getattr(optim, config[\"optimizer_params\"][\"method\"])(self.model.parameters(), lr=optimizer_lr)\n\n    # get step size from the config\n    self.step_size = config[\"tune\"][\"step_size\"]\n\n    # use dataloader on training/validation data\n    self.batch_size = config[\"data_params\"][\"batch_size\"]\n    training: Dataset = config[\"training\"]\n    validation: Dataset = config[\"validation\"]\n    self.training = DataLoader(\n        training,\n        batch_size=self.batch_size,\n        shuffle=True,\n    )  # TODO need to check the reproducibility of this shuffling\n    self.validation = DataLoader(validation, batch_size=self.batch_size, shuffle=True)\n\n    # debug section, first create a dedicated directory for each worker inside Ray_results/&lt;tune_model_run_specific_dir&gt; location\n    debug_dir = os.path.join(\n        config[\"tune_run_path\"],\n        \"debug\",\n        (\"worker_with_seed_\" + str(self.config[\"ray_worker_seed\"])),\n    )\n    if config[\"_debug\"]:\n        # creating a special directory for it one that is worker/trial/experiment specific\n        os.makedirs(debug_dir)\n        seed_filename = os.path.join(debug_dir, \"seeds.txt\")\n\n        # save the initialized model weights\n        self.export_model(export_dir=debug_dir)\n\n        # save the seeds\n        with open(seed_filename, \"a\") as seed_f:\n            # you can not retrieve the actual seed once it set, or the current seed neither for python, numpy nor torch. so we select five numbers randomly. If that is the first draw of numbers they are always the same.\n            python_values = random.sample(range(100), 5)\n            numpy_values = list(np.random.randint(0, 100, size=5))\n            torch_values = torch.randint(0, 100, (5,)).tolist()\n            seed_f.write(\n                f\"python drawn numbers : {python_values}\\nnumpy drawn numbers : {numpy_values}\\ntorch drawn numbers : {torch_values}\\n\",\n            )\n</code></pre>"},{"location":"reference/stimulus/learner/raytune_learner/#stimulus.learner.raytune_learner.TuneModel.step","title":"step","text":"<pre><code>step() -&gt; dict\n</code></pre> <p>For each batch in the training data, calculate the loss and update the model parameters.</p> <p>This calculation is performed based on the model's batch function. At the end, return the objective metric(s) for the tuning process.</p> Source code in <code>src/stimulus/learner/raytune_learner.py</code> <pre><code>def step(self) -&gt; dict:\n    \"\"\"For each batch in the training data, calculate the loss and update the model parameters.\n\n    This calculation is performed based on the model's batch function.\n    At the end, return the objective metric(s) for the tuning process.\n    \"\"\"\n    for _step_size in range(self.step_size):\n        for x, y, _meta in self.training:\n            # the loss dict could be unpacked with ** and the function declaration handle it differently like **kwargs. to be decided, personally find this more clean and understable.\n            self.model.batch(x=x, y=y, optimizer=self.optimizer, **self.loss_dict)\n    return self.objective()\n</code></pre>"},{"location":"reference/stimulus/learner/raytune_learner/#stimulus.learner.raytune_learner.TuneWrapper","title":"TuneWrapper","text":"<pre><code>TuneWrapper(\n    config_path: str,\n    model_class: Module,\n    data_path: str,\n    experiment_object: object,\n    max_gpus: Optional[int] = None,\n    max_cpus: Optional[int] = None,\n    max_object_store_mem: Optional[float] = None,\n    max_mem: Optional[float] = None,\n    ray_results_dir: Optional[str] = None,\n    tune_run_name: Optional[str] = None,\n    *,\n    debug: bool = False\n)\n</code></pre> <p>Wrapper class for Ray Tune hyperparameter optimization.</p> <p>Methods:</p> <ul> <li> <code>tune</code>             \u2013              <p>Run the tuning process.</p> </li> <li> <code>tuner_initialization</code>             \u2013              <p>Prepare the tuner with the configs.</p> </li> </ul> Source code in <code>src/stimulus/learner/raytune_learner.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    model_class: nn.Module,\n    data_path: str,\n    experiment_object: object,\n    max_gpus: Optional[int] = None,\n    max_cpus: Optional[int] = None,\n    max_object_store_mem: Optional[float] = None,\n    max_mem: Optional[float] = None,\n    ray_results_dir: Optional[str] = None,\n    tune_run_name: Optional[str] = None,\n    *,  # Force debug to be keyword-only\n    debug: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the TuneWrapper with the paths to the config, model, and data.\"\"\"\n    self.config = YamlRayConfigLoader(config_path).get_config()\n\n    # set all general seeds: python, numpy and torch.\n    set_general_seeds(self.config[\"seed\"])\n\n    self.config[\"model\"] = model_class\n    self.config[\"experiment\"] = experiment_object\n\n    # add the ray method for number generation to the config so it can be passed to the trainable class, that will in turn set per worker seeds in a reproducible mnanner.\n    self.config[\"ray_worker_seed\"] = tune.randint(0, 1000)\n\n    # add the data path to the config so it know where it is during tuning\n    if not os.path.exists(data_path):\n        raise ValueError(\"Data path does not exist. Given path:\" + data_path)\n    self.config[\"data_path\"] = os.path.abspath(data_path)\n\n    # build the tune config\n    self.config[\"tune\"][\"tune_params\"][\"scheduler\"] = getattr(schedulers, self.config[\"tune\"][\"scheduler\"][\"name\"])(\n        **self.config[\"tune\"][\"scheduler\"][\"params\"],\n    )\n    self.tune_config = tune.TuneConfig(**self.config[\"tune\"][\"tune_params\"])\n\n    # set ray cluster total resources (max)\n    self.max_gpus = max_gpus\n    self.max_cpus = max_cpus\n    self.max_object_store_mem = max_object_store_mem  # this is a special subset of the total usable memory that ray need for his internal work, by default is set to 30% of total memory usable\n    self.max_mem = max_mem\n\n    # build the run config\n    self.checkpoint_config = train.CheckpointConfig(checkpoint_at_end=True)  # TODO implement checkpoiting\n    # in case a custom name was not given for tune_run_name, build it like ray would do. to later pass it on the worker for the debug section.\n    if tune_run_name is None:\n        tune_run_name = \"TuneModel_\" + datetime.datetime.now(tz=datetime.timezone.utc).strftime(\"%Y-%m-%d_%H-%M-%S\")\n    self.run_config = train.RunConfig(\n        name=tune_run_name,\n        storage_path=ray_results_dir,\n        checkpoint_config=self.checkpoint_config,\n        **self.config[\"tune\"][\"run_params\"],\n    )  # TODO maybe put name into config if it was possible to retrieve from tune the name of the result subdir)\n\n    # working towards the path for the tune_run directory. if ray_results_dir None ray will put it under home so we will do the same here.\n    if ray_results_dir is None:\n        ray_results_dir = os.environ.get(\"HOME\", \"\")\n    # then we are able to pass the whole correct tune_run path to the trainable function. so it can use thaqt to place the debug dir under if needed.\n    self.config[\"tune_run_path\"] = os.path.join(ray_results_dir, tune_run_name)\n\n    # pass the debug flag to the config taken fromn tune so it can be used inside the setup of the trainable\n    self.config[\"_debug\"] = False\n    if debug:\n        self.config[\"_debug\"] = True\n\n    self.tuner = self.tuner_initialization()\n</code></pre>"},{"location":"reference/stimulus/learner/raytune_learner/#stimulus.learner.raytune_learner.TuneWrapper.tune","title":"tune","text":"<pre><code>tune() -&gt; None\n</code></pre> <p>Run the tuning process.</p> Source code in <code>src/stimulus/learner/raytune_learner.py</code> <pre><code>def tune(self) -&gt; None:\n    \"\"\"Run the tuning process.\"\"\"\n    self.tuner.fit()\n</code></pre>"},{"location":"reference/stimulus/learner/raytune_learner/#stimulus.learner.raytune_learner.TuneWrapper.tuner_initialization","title":"tuner_initialization","text":"<pre><code>tuner_initialization() -&gt; Tuner\n</code></pre> <p>Prepare the tuner with the configs.</p> Source code in <code>src/stimulus/learner/raytune_learner.py</code> <pre><code>def tuner_initialization(self) -&gt; tune.Tuner:\n    \"\"\"Prepare the tuner with the configs.\"\"\"\n    # in ray 3.0.0 the following issue is fixed. Sometimes it sees that ray is already initialized, so in that case shut it off and start anew. TODO update to ray 3.0.0\n    if is_initialized():\n        shutdown()\n\n    # initialize the ray cluster with the limiter on CPUs, GPUs or memory if needed, otherwise everything that is available. None is what ray uses to get all resources available for either CPU, GPU or memory.\n    # memory is split in two for ray. read more at ray.init documentation.\n    init(\n        num_cpus=self.max_cpus,\n        num_gpus=self.max_gpus,\n        object_store_memory=self.max_object_store_mem,\n        _memory=self.max_mem,\n    )\n\n    logging.info(f\"CLUSTER resources   -&gt;  {cluster_resources()}\")\n\n    # check if resources per trial are not exceeding maximum resources. traial = single set/combination of hyperparameter (parrallel actors maximum resources in ray tune gergon).\n    self.gpu_per_trial = self._chek_per_trial_resources(\"gpu_per_trial\", cluster_resources(), \"GPU\")\n    self.cpu_per_trial = self._chek_per_trial_resources(\"cpu_per_trial\", cluster_resources(), \"CPU\")\n\n    logging.info(f\"PER_TRIAL resources -&gt;  GPU: {self.gpu_per_trial} CPU: {self.cpu_per_trial}\")\n\n    # wrap the trainable with the allowed resources per trial\n    # also provide the training and validation data to the trainable through with_parameters\n    # this is a wrapper that passes the data as a object reference (pointer)\n    trainable = tune.with_resources(TuneModel, resources={\"cpu\": self.cpu_per_trial, \"gpu\": self.gpu_per_trial})\n    trainable = tune.with_parameters(\n        trainable,\n        training=TorchDataset(self.config[\"data_path\"], self.config[\"experiment\"], split=0),\n        validation=TorchDataset(self.config[\"data_path\"], self.config[\"experiment\"], split=1),\n    )\n\n    return tune.Tuner(trainable, tune_config=self.tune_config, param_space=self.config, run_config=self.run_config)\n</code></pre>"},{"location":"reference/stimulus/learner/raytune_parser/","title":"stimulus.learner.raytune_parser","text":""},{"location":"reference/stimulus/learner/raytune_parser/#stimulus.learner.raytune_parser","title":"raytune_parser","text":"<p>Ray Tune results parser for extracting and saving best model configurations and weights.</p> <p>Classes:</p> <ul> <li> <code>RayTuneMetrics</code>           \u2013            <p>TypedDict for storing Ray Tune metrics results.</p> </li> <li> <code>RayTuneOptimizer</code>           \u2013            <p>TypedDict for storing Ray Tune optimizer state.</p> </li> <li> <code>RayTuneResult</code>           \u2013            <p>TypedDict for storing Ray Tune optimization results.</p> </li> <li> <code>TuneParser</code>           \u2013            <p>Parser class for Ray Tune results to extract best configurations and model weights.</p> </li> </ul>"},{"location":"reference/stimulus/learner/raytune_parser/#stimulus.learner.raytune_parser.RayTuneMetrics","title":"RayTuneMetrics","text":"<p>               Bases: <code>TypedDict</code></p> <p>TypedDict for storing Ray Tune metrics results.</p>"},{"location":"reference/stimulus/learner/raytune_parser/#stimulus.learner.raytune_parser.RayTuneOptimizer","title":"RayTuneOptimizer","text":"<p>               Bases: <code>TypedDict</code></p> <p>TypedDict for storing Ray Tune optimizer state.</p>"},{"location":"reference/stimulus/learner/raytune_parser/#stimulus.learner.raytune_parser.RayTuneResult","title":"RayTuneResult","text":"<p>               Bases: <code>TypedDict</code></p> <p>TypedDict for storing Ray Tune optimization results.</p>"},{"location":"reference/stimulus/learner/raytune_parser/#stimulus.learner.raytune_parser.TuneParser","title":"TuneParser","text":"<pre><code>TuneParser(results: ExperimentAnalysis)\n</code></pre> <p>Parser class for Ray Tune results to extract best configurations and model weights.</p> <p>Methods:</p> <ul> <li> <code>fix_config_values</code>             \u2013              <p>Correct config values.</p> </li> <li> <code>get_best_config</code>             \u2013              <p>Get the best config from the results.</p> </li> <li> <code>get_best_model</code>             \u2013              <p>Get the best model weights from the results.</p> </li> <li> <code>get_best_optimizer</code>             \u2013              <p>Get the best optimizer state from the results.</p> </li> <li> <code>save_best_config</code>             \u2013              <p>Save the best config to a file.</p> </li> <li> <code>save_best_metrics_dataframe</code>             \u2013              <p>Save the dataframe with the metrics at each iteration of the best sample to a file.</p> </li> <li> <code>save_best_model</code>             \u2013              <p>Save the best model weights to a file.</p> </li> <li> <code>save_best_optimizer</code>             \u2013              <p>Save the best optimizer state to a file.</p> </li> </ul> Source code in <code>src/stimulus/learner/raytune_parser.py</code> <pre><code>def __init__(self, results: ExperimentAnalysis) -&gt; None:\n    \"\"\"`results` is the output of ray.tune.\"\"\"\n    self.results = results\n</code></pre>"},{"location":"reference/stimulus/learner/raytune_parser/#stimulus.learner.raytune_parser.TuneParser.fix_config_values","title":"fix_config_values","text":"<pre><code>fix_config_values(config: dict[str, Any]) -&gt; dict[str, Any]\n</code></pre> <p>Correct config values.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict[str, Any]</code>)           \u2013            <p>Configuration dictionary to fix</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Fixed configuration dictionary</p> </li> </ul> Source code in <code>src/stimulus/learner/raytune_parser.py</code> <pre><code>def fix_config_values(self, config: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Correct config values.\n\n    Args:\n        config: Configuration dictionary to fix\n\n    Returns:\n        Fixed configuration dictionary\n    \"\"\"\n    # fix the model and experiment values to avoid problems with serialization\n    # TODO this is a quick fix to avoid the problem with serializing class objects. maybe there is a better way.\n    config[\"model\"] = config[\"model\"].__name__\n    config[\"experiment\"] = config[\"experiment\"].__class__.__name__\n    if \"tune\" in config and \"tune_params\" in config[\"tune\"]:\n        del config[\"tune\"][\"tune_params\"][\"scheduler\"]\n    # delete miscellaneus keys, used only during debug mode for example\n    del config[\"_debug\"], config[\"tune_run_path\"]\n\n    return config\n</code></pre>"},{"location":"reference/stimulus/learner/raytune_parser/#stimulus.learner.raytune_parser.TuneParser.get_best_config","title":"get_best_config","text":"<pre><code>get_best_config() -&gt; dict[str, Any]\n</code></pre> <p>Get the best config from the results.</p> Source code in <code>src/stimulus/learner/raytune_parser.py</code> <pre><code>def get_best_config(self) -&gt; dict[str, Any]:\n    \"\"\"Get the best config from the results.\"\"\"\n    best_result = cast(RayTuneResult, self.results.best_result)\n    return best_result[\"config\"]\n</code></pre>"},{"location":"reference/stimulus/learner/raytune_parser/#stimulus.learner.raytune_parser.TuneParser.get_best_model","title":"get_best_model","text":"<pre><code>get_best_model() -&gt; dict[str, Tensor]\n</code></pre> <p>Get the best model weights from the results.</p> Source code in <code>src/stimulus/learner/raytune_parser.py</code> <pre><code>def get_best_model(self) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Get the best model weights from the results.\"\"\"\n    best_result = cast(RayTuneMetrics, self.results.best_result)\n    checkpoint_dir = best_result[\"checkpoint\"]\n    checkpoint = os.path.join(checkpoint_dir, \"model.safetensors\")\n    return safe_load_file(checkpoint)\n</code></pre>"},{"location":"reference/stimulus/learner/raytune_parser/#stimulus.learner.raytune_parser.TuneParser.get_best_optimizer","title":"get_best_optimizer","text":"<pre><code>get_best_optimizer() -&gt; dict[str, Any]\n</code></pre> <p>Get the best optimizer state from the results.</p> Source code in <code>src/stimulus/learner/raytune_parser.py</code> <pre><code>def get_best_optimizer(self) -&gt; dict[str, Any]:\n    \"\"\"Get the best optimizer state from the results.\"\"\"\n    best_result = cast(RayTuneOptimizer, self.results.best_result)\n    checkpoint_dir = best_result[\"checkpoint\"]\n    checkpoint = os.path.join(checkpoint_dir, \"optimizer.pt\")\n    return torch.load(checkpoint)\n</code></pre>"},{"location":"reference/stimulus/learner/raytune_parser/#stimulus.learner.raytune_parser.TuneParser.save_best_config","title":"save_best_config","text":"<pre><code>save_best_config(output: str) -&gt; None\n</code></pre> <p>Save the best config to a file.</p> <p>TODO maybe only save the relevant config values.</p> Source code in <code>src/stimulus/learner/raytune_parser.py</code> <pre><code>def save_best_config(self, output: str) -&gt; None:\n    \"\"\"Save the best config to a file.\n\n    TODO maybe only save the relevant config values.\n    \"\"\"\n    config = self.get_best_config()\n    config = self.fix_config_values(config)\n    with open(output, \"w\") as f:\n        json.dump(config, f, indent=4)\n</code></pre>"},{"location":"reference/stimulus/learner/raytune_parser/#stimulus.learner.raytune_parser.TuneParser.save_best_metrics_dataframe","title":"save_best_metrics_dataframe","text":"<pre><code>save_best_metrics_dataframe(output: str) -&gt; None\n</code></pre> <p>Save the dataframe with the metrics at each iteration of the best sample to a file.</p> Source code in <code>src/stimulus/learner/raytune_parser.py</code> <pre><code>def save_best_metrics_dataframe(self, output: str) -&gt; None:\n    \"\"\"Save the dataframe with the metrics at each iteration of the best sample to a file.\"\"\"\n    best_result = cast(RayTuneMetrics, self.results.best_result)\n    metrics_df = best_result[\"metrics_dataframe\"]\n    columns = [col for col in metrics_df.columns if \"config\" not in col]\n    metrics_df = metrics_df[columns]\n    metrics_df.to_csv(output, index=False)\n</code></pre>"},{"location":"reference/stimulus/learner/raytune_parser/#stimulus.learner.raytune_parser.TuneParser.save_best_model","title":"save_best_model","text":"<pre><code>save_best_model(output: str) -&gt; None\n</code></pre> <p>Save the best model weights to a file.</p> Source code in <code>src/stimulus/learner/raytune_parser.py</code> <pre><code>def save_best_model(self, output: str) -&gt; None:\n    \"\"\"Save the best model weights to a file.\"\"\"\n    safe_save_file(self.get_best_model(), output)\n</code></pre>"},{"location":"reference/stimulus/learner/raytune_parser/#stimulus.learner.raytune_parser.TuneParser.save_best_optimizer","title":"save_best_optimizer","text":"<pre><code>save_best_optimizer(output: str) -&gt; None\n</code></pre> <p>Save the best optimizer state to a file.</p> Source code in <code>src/stimulus/learner/raytune_parser.py</code> <pre><code>def save_best_optimizer(self, output: str) -&gt; None:\n    \"\"\"Save the best optimizer state to a file.\"\"\"\n    torch.save(self.get_best_optimizer(), output)\n</code></pre>"},{"location":"reference/stimulus/typing/","title":"stimulus.typing","text":""},{"location":"reference/stimulus/typing/#stimulus.typing","title":"typing","text":"<p>Typing for Stimulus Python API.</p> <p>This module contains all Stimulus types which will be used for variable typing and likely not instantiated, as well as aliases for other types to use for typing purposes.</p> <p>The aliases from this module should be used for typing purposes only.</p>"},{"location":"reference/stimulus/utils/","title":"stimulus.utils","text":""},{"location":"reference/stimulus/utils/#stimulus.utils","title":"utils","text":"<p>Utility functions package.</p> <p>Modules:</p> <ul> <li> <code>generic_utils</code>           \u2013            <p>Utility functions for general purpose operations like seed setting and tensor manipulation.</p> </li> <li> <code>launch_utils</code>           \u2013            <p>Utility functions for launching and configuring experiments and ray tuning.</p> </li> <li> <code>performance</code>           \u2013            <p>Utility module for computing various performance metrics for machine learning models.</p> </li> <li> <code>yaml_data</code>           \u2013            <p>Utility module for handling YAML configuration files and their validation.</p> </li> <li> <code>yaml_model_schema</code>           \u2013            <p>Module for handling YAML configuration files and converting them to Ray Tune format.</p> </li> </ul>"},{"location":"reference/stimulus/utils/generic_utils/","title":"stimulus.utils.generic_utils","text":""},{"location":"reference/stimulus/utils/generic_utils/#stimulus.utils.generic_utils","title":"generic_utils","text":"<p>Utility functions for general purpose operations like seed setting and tensor manipulation.</p> <p>Functions:</p> <ul> <li> <code>ensure_at_least_1d</code>             \u2013              <p>Function to make sure tensors given are not zero dimensional. if they are add one dimension.</p> </li> <li> <code>set_general_seeds</code>             \u2013              <p>Set all relevant random seeds to a given value.</p> </li> </ul>"},{"location":"reference/stimulus/utils/generic_utils/#stimulus.utils.generic_utils.ensure_at_least_1d","title":"ensure_at_least_1d","text":"<pre><code>ensure_at_least_1d(tensor: Tensor) -&gt; Tensor\n</code></pre> <p>Function to make sure tensors given are not zero dimensional. if they are add one dimension.</p> Source code in <code>src/stimulus/utils/generic_utils.py</code> <pre><code>def ensure_at_least_1d(tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Function to make sure tensors given are not zero dimensional. if they are add one dimension.\"\"\"\n    if tensor.dim() == 0:\n        tensor = tensor.unsqueeze(0)\n    return tensor\n</code></pre>"},{"location":"reference/stimulus/utils/generic_utils/#stimulus.utils.generic_utils.set_general_seeds","title":"set_general_seeds","text":"<pre><code>set_general_seeds(seed_value: Union[int, None]) -&gt; None\n</code></pre> <p>Set all relevant random seeds to a given value.</p> <p>Especially useful in case of ray.tune. Ray does not have a \"generic\" seed as far as ray 2.23.</p> Source code in <code>src/stimulus/utils/generic_utils.py</code> <pre><code>def set_general_seeds(seed_value: Union[int, None]) -&gt; None:\n    \"\"\"Set all relevant random seeds to a given value.\n\n    Especially useful in case of ray.tune. Ray does not have a \"generic\" seed as far as ray 2.23.\n    \"\"\"\n    # Set python seed\n    random.seed(seed_value)\n\n    # set numpy seed\n    np.random.seed(seed_value)\n\n    # set torch seed, diffrently from the two above torch can nopt take Noneas input value so it will not be called in that case.\n    if seed_value is not None:\n        torch.manual_seed(seed_value)\n</code></pre>"},{"location":"reference/stimulus/utils/launch_utils/","title":"stimulus.utils.launch_utils","text":""},{"location":"reference/stimulus/utils/launch_utils/#stimulus.utils.launch_utils","title":"launch_utils","text":"<p>Utility functions for launching and configuring experiments and ray tuning.</p> <p>Functions:</p> <ul> <li> <code>get_experiment</code>             \u2013              <p>Get an experiment instance by name.</p> </li> <li> <code>import_class_from_file</code>             \u2013              <p>Import and return the Model class from a specified Python file.</p> </li> <li> <code>memory_split_for_ray_init</code>             \u2013              <p>Process the input memory value into the right unit and allocates 30% for overhead and 70% for tuning.</p> </li> </ul>"},{"location":"reference/stimulus/utils/launch_utils/#stimulus.utils.launch_utils.get_experiment","title":"get_experiment","text":"<pre><code>get_experiment(experiment_name: str) -&gt; object\n</code></pre> <p>Get an experiment instance by name.</p> <p>Parameters:</p> <ul> <li> <code>experiment_name</code>               (<code>str</code>)           \u2013            <p>Name of the experiment class to instantiate.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>object</code> (              <code>object</code> )          \u2013            <p>An instance of the requested experiment class.</p> </li> </ul> Source code in <code>src/stimulus/utils/launch_utils.py</code> <pre><code>def get_experiment(experiment_name: str) -&gt; object:\n    \"\"\"Get an experiment instance by name.\n\n    Args:\n        experiment_name (str): Name of the experiment class to instantiate.\n\n    Returns:\n        object: An instance of the requested experiment class.\n    \"\"\"\n    return getattr(exp, experiment_name)()\n</code></pre>"},{"location":"reference/stimulus/utils/launch_utils/#stimulus.utils.launch_utils.import_class_from_file","title":"import_class_from_file","text":"<pre><code>import_class_from_file(file_path: str) -&gt; type\n</code></pre> <p>Import and return the Model class from a specified Python file.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>str</code>)           \u2013            <p>Path to the Python file containing the Model class.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type</code> (              <code>type</code> )          \u2013            <p>The Model class found in the file.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If no class starting with 'Model' is found in the file.</p> </li> </ul> Source code in <code>src/stimulus/utils/launch_utils.py</code> <pre><code>def import_class_from_file(file_path: str) -&gt; type:\n    \"\"\"Import and return the Model class from a specified Python file.\n\n    Args:\n        file_path (str): Path to the Python file containing the Model class.\n\n    Returns:\n        type: The Model class found in the file.\n\n    Raises:\n        ImportError: If no class starting with 'Model' is found in the file.\n    \"\"\"\n    # Extract directory path and file name\n    directory, file_name = os.path.split(file_path)\n    module_name = os.path.splitext(file_name)[0]  # Remove extension to get module name\n\n    # Create a module from the file path\n    # In summary, these three lines of code are responsible for creating a module specification based on a file location, creating a module object from that specification, and then executing the module's code to populate the module object with the definitions from the Python file.\n    spec = importlib.util.spec_from_file_location(module_name, file_path)\n    if spec is None:\n        raise ImportError(f\"Could not create module spec for {file_path}\")\n    module = importlib.util.module_from_spec(spec)\n    if spec.loader is None:\n        raise ImportError(f\"Module spec has no loader for {file_path}\")\n    spec.loader.exec_module(module)\n\n    # Find the class dynamically\n    for name in dir(module):\n        model_class = getattr(module, name)\n        if isinstance(model_class, type) and name.startswith(\"Model\"):\n            return model_class\n\n    # Class not found\n    raise ImportError(\"No class starting with 'Model' found in the file.\")\n</code></pre>"},{"location":"reference/stimulus/utils/launch_utils/#stimulus.utils.launch_utils.memory_split_for_ray_init","title":"memory_split_for_ray_init","text":"<pre><code>memory_split_for_ray_init(\n    memory_str: Union[str, None],\n) -&gt; tuple[float, float]\n</code></pre> <p>Process the input memory value into the right unit and allocates 30% for overhead and 70% for tuning.</p> <p>Useful in case ray detects them wrongly. Memory is split in two for ray: for store_object memory and the other actual memory for tuning. The following function takes the total possible usable/allocated memory as a string parameter and returns in bytes the values for store_memory (30% as default in ray) and memory (70%).</p> <p>Parameters:</p> <ul> <li> <code>memory_str</code>               (<code>Union[str, None]</code>)           \u2013            <p>Memory string in format like \"8G\", \"16GB\", etc.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[float, float]</code>           \u2013            <p>tuple[float, float]: A tuple containing (store_memory, memory) in bytes.</p> </li> </ul> Source code in <code>src/stimulus/utils/launch_utils.py</code> <pre><code>def memory_split_for_ray_init(memory_str: Union[str, None]) -&gt; tuple[float, float]:\n    \"\"\"Process the input memory value into the right unit and allocates 30% for overhead and 70% for tuning.\n\n    Useful in case ray detects them wrongly. Memory is split in two for ray: for store_object memory\n    and the other actual memory for tuning. The following function takes the total possible\n    usable/allocated memory as a string parameter and returns in bytes the values for store_memory\n    (30% as default in ray) and memory (70%).\n\n    Args:\n        memory_str (Union[str, None]): Memory string in format like \"8G\", \"16GB\", etc.\n\n    Returns:\n        tuple[float, float]: A tuple containing (store_memory, memory) in bytes.\n    \"\"\"\n    if memory_str is None:\n        return 0.0, 0.0\n\n    units = {\"B\": 1, \"K\": 2**10, \"M\": 2**20, \"G\": 2**30, \"T\": 2**40, \"P\": 2**50}\n\n    # Extract the numerical part and the unit\n    value_str = \"\"\n    unit = \"\"\n\n    for char in memory_str:\n        if char.isdigit() or char == \".\":\n            value_str += char\n        elif char.isalpha():\n            unit += char.upper()\n\n    value = float(value_str)\n\n    # Normalize the unit (to handle cases like Gi, GB, Mi, etc.)\n    if unit.endswith((\"I\", \"i\", \"B\", \"b\")):\n        unit = unit[:-1]\n\n    if unit not in units:\n        raise ValueError(f\"Unknown unit: {unit}\")\n\n    bytes_value = value * units[unit]\n\n    # Calculate 30% and 70%\n    thirty_percent = math.floor(bytes_value * 0.30)\n    seventy_percent = math.floor(bytes_value * 0.70)\n\n    return float(thirty_percent), float(seventy_percent)\n</code></pre>"},{"location":"reference/stimulus/utils/performance/","title":"stimulus.utils.performance","text":""},{"location":"reference/stimulus/utils/performance/#stimulus.utils.performance","title":"performance","text":"<p>Utility module for computing various performance metrics for machine learning models.</p> <p>Classes:</p> <ul> <li> <code>Performance</code>           \u2013            <p>Returns the value of a given metric.</p> </li> </ul>"},{"location":"reference/stimulus/utils/performance/#stimulus.utils.performance.Performance","title":"Performance","text":"<pre><code>Performance(\n    labels: Any, predictions: Any, metric: str = \"rocauc\"\n)\n</code></pre> <p>Returns the value of a given metric.</p>"},{"location":"reference/stimulus/utils/performance/#stimulus.utils.performance.Performance--parameters","title":"Parameters","text":"<p>labels (np.array) : labels predictions (np.array) : predictions metric (str) : the metric to compute</p>"},{"location":"reference/stimulus/utils/performance/#stimulus.utils.performance.Performance--returns","title":"Returns:","text":"<p>value (float) : the value of the metric</p> <p>TODO we can add more metrics here</p> <p>TODO currently for classification  metrics like precision, recall, f1score and mcc, we are using a threshold of 0.5 to convert the probabilities to binary predictions. However for models with imbalanced predictions, where the meaningful threshold is not located at 0.5, one can end up with full of 0s or 1s, and thus meaningless performance metrics.</p> <p>Parameters:</p> <ul> <li> <code>labels</code>               (<code>Any</code>)           \u2013            <p>Ground truth labels</p> </li> <li> <code>predictions</code>               (<code>Any</code>)           \u2013            <p>Model predictions</p> </li> <li> <code>metric</code>               (<code>str</code>, default:                   <code>'rocauc'</code> )           \u2013            <p>Type of metric to compute (default: \"rocauc\")</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>data2array</code>             \u2013              <p>Convert input data to numpy array.</p> </li> <li> <code>f1score</code>             \u2013              <p>Compute F1 score.</p> </li> <li> <code>handle_multiclass</code>             \u2013              <p>Handle the case of multiclass classification.</p> </li> <li> <code>mcc</code>             \u2013              <p>Compute Matthews Correlation Coefficient.</p> </li> <li> <code>prauc</code>             \u2013              <p>Compute PR AUC score.</p> </li> <li> <code>precision</code>             \u2013              <p>Compute precision score.</p> </li> <li> <code>recall</code>             \u2013              <p>Compute recall score.</p> </li> <li> <code>rocauc</code>             \u2013              <p>Compute ROC AUC score.</p> </li> <li> <code>spearmanr</code>             \u2013              <p>Compute Spearman correlation coefficient.</p> </li> </ul> Source code in <code>src/stimulus/utils/performance.py</code> <pre><code>def __init__(self, labels: Any, predictions: Any, metric: str = \"rocauc\") -&gt; None:\n    \"\"\"Initialize Performance class with labels, predictions and metric type.\n\n    Args:\n        labels: Ground truth labels\n        predictions: Model predictions\n        metric: Type of metric to compute (default: \"rocauc\")\n    \"\"\"\n    labels_arr = self.data2array(labels)\n    predictions_arr = self.data2array(predictions)\n    labels_arr, predictions_arr = self.handle_multiclass(labels_arr, predictions_arr)\n    if labels_arr.shape != predictions_arr.shape:\n        raise ValueError(\n            f\"The labels have shape {labels_arr.shape} whereas predictions have shape {predictions_arr.shape}.\",\n        )\n    function = getattr(self, metric)\n    self.val = function(labels_arr, predictions_arr)\n</code></pre>"},{"location":"reference/stimulus/utils/performance/#stimulus.utils.performance.Performance.data2array","title":"data2array","text":"<pre><code>data2array(data: Any) -&gt; NDArray[float64]\n</code></pre> <p>Convert input data to numpy array.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Any</code>)           \u2013            <p>Input data in various formats</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray[float64]</code>           \u2013            <p>NDArray[np.float64]: Converted numpy array</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If input data type is not supported</p> </li> </ul> Source code in <code>src/stimulus/utils/performance.py</code> <pre><code>def data2array(self, data: Any) -&gt; NDArray[np.float64]:\n    \"\"\"Convert input data to numpy array.\n\n    Args:\n        data: Input data in various formats\n\n    Returns:\n        NDArray[np.float64]: Converted numpy array\n\n    Raises:\n        ValueError: If input data type is not supported\n    \"\"\"\n    if isinstance(data, list):\n        return np.array(data, dtype=np.float64)\n    if isinstance(data, np.ndarray):\n        return data.astype(np.float64)\n    if isinstance(data, torch.Tensor):\n        return data.detach().cpu().numpy().astype(np.float64)\n    if isinstance(data, (int, float)):\n        return np.array([data], dtype=np.float64)\n    raise ValueError(f\"The data must be a list, np.array, torch.Tensor, int or float. Instead it is {type(data)}\")\n</code></pre>"},{"location":"reference/stimulus/utils/performance/#stimulus.utils.performance.Performance.f1score","title":"f1score","text":"<pre><code>f1score(\n    labels: NDArray[float64], predictions: NDArray[float64]\n) -&gt; float\n</code></pre> <p>Compute F1 score.</p> Source code in <code>src/stimulus/utils/performance.py</code> <pre><code>def f1score(self, labels: NDArray[np.float64], predictions: NDArray[np.float64]) -&gt; float:\n    \"\"\"Compute F1 score.\"\"\"\n    predictions_binary = np.array([1 if p &gt; BINARY_THRESHOLD else 0 for p in predictions])\n    return float(f1_score(labels, predictions_binary))\n</code></pre>"},{"location":"reference/stimulus/utils/performance/#stimulus.utils.performance.Performance.handle_multiclass","title":"handle_multiclass","text":"<pre><code>handle_multiclass(\n    labels: NDArray[float64], predictions: NDArray[float64]\n) -&gt; tuple[NDArray[float64], NDArray[float64]]\n</code></pre> <p>Handle the case of multiclass classification.</p> <p>TODO currently only two class predictions are handled. Needs to handle the other scenarios.</p> Source code in <code>src/stimulus/utils/performance.py</code> <pre><code>def handle_multiclass(\n    self,\n    labels: NDArray[np.float64],\n    predictions: NDArray[np.float64],\n) -&gt; tuple[NDArray[np.float64], NDArray[np.float64]]:\n    \"\"\"Handle the case of multiclass classification.\n\n    TODO currently only two class predictions are handled. Needs to handle the other scenarios.\n    \"\"\"\n    # if only one columns for labels and predictions\n    if (len(labels.shape) == 1) and (len(predictions.shape) == 1):\n        return labels, predictions\n\n    # if one columns for labels, but two columns for predictions\n    if (len(labels.shape) == 1) and (predictions.shape[1] == BINARY_CLASS_COUNT):\n        predictions = predictions[:, 1]  # assumes the second column is the positive class\n        return labels, predictions\n\n    # other scenarios not implemented yet\n    raise ValueError(f\"Labels have shape {labels.shape} and predictions have shape {predictions.shape}.\")\n</code></pre>"},{"location":"reference/stimulus/utils/performance/#stimulus.utils.performance.Performance.mcc","title":"mcc","text":"<pre><code>mcc(\n    labels: NDArray[float64], predictions: NDArray[float64]\n) -&gt; float\n</code></pre> <p>Compute Matthews Correlation Coefficient.</p> Source code in <code>src/stimulus/utils/performance.py</code> <pre><code>def mcc(self, labels: NDArray[np.float64], predictions: NDArray[np.float64]) -&gt; float:\n    \"\"\"Compute Matthews Correlation Coefficient.\"\"\"\n    predictions_binary = np.array([1 if p &gt; BINARY_THRESHOLD else 0 for p in predictions])\n    return float(matthews_corrcoef(labels, predictions_binary))\n</code></pre>"},{"location":"reference/stimulus/utils/performance/#stimulus.utils.performance.Performance.prauc","title":"prauc","text":"<pre><code>prauc(\n    labels: NDArray[float64], predictions: NDArray[float64]\n) -&gt; float\n</code></pre> <p>Compute PR AUC score.</p> Source code in <code>src/stimulus/utils/performance.py</code> <pre><code>def prauc(self, labels: NDArray[np.float64], predictions: NDArray[np.float64]) -&gt; float:\n    \"\"\"Compute PR AUC score.\"\"\"\n    return float(average_precision_score(labels, predictions))\n</code></pre>"},{"location":"reference/stimulus/utils/performance/#stimulus.utils.performance.Performance.precision","title":"precision","text":"<pre><code>precision(\n    labels: NDArray[float64], predictions: NDArray[float64]\n) -&gt; float\n</code></pre> <p>Compute precision score.</p> Source code in <code>src/stimulus/utils/performance.py</code> <pre><code>def precision(self, labels: NDArray[np.float64], predictions: NDArray[np.float64]) -&gt; float:\n    \"\"\"Compute precision score.\"\"\"\n    predictions_binary = np.array([1 if p &gt; BINARY_THRESHOLD else 0 for p in predictions])\n    return float(precision_score(labels, predictions_binary))\n</code></pre>"},{"location":"reference/stimulus/utils/performance/#stimulus.utils.performance.Performance.recall","title":"recall","text":"<pre><code>recall(\n    labels: NDArray[float64], predictions: NDArray[float64]\n) -&gt; float\n</code></pre> <p>Compute recall score.</p> Source code in <code>src/stimulus/utils/performance.py</code> <pre><code>def recall(self, labels: NDArray[np.float64], predictions: NDArray[np.float64]) -&gt; float:\n    \"\"\"Compute recall score.\"\"\"\n    predictions_binary = np.array([1 if p &gt; BINARY_THRESHOLD else 0 for p in predictions])\n    return float(recall_score(labels, predictions_binary))\n</code></pre>"},{"location":"reference/stimulus/utils/performance/#stimulus.utils.performance.Performance.rocauc","title":"rocauc","text":"<pre><code>rocauc(\n    labels: NDArray[float64], predictions: NDArray[float64]\n) -&gt; float\n</code></pre> <p>Compute ROC AUC score.</p> Source code in <code>src/stimulus/utils/performance.py</code> <pre><code>def rocauc(self, labels: NDArray[np.float64], predictions: NDArray[np.float64]) -&gt; float:\n    \"\"\"Compute ROC AUC score.\"\"\"\n    return float(roc_auc_score(labels, predictions))\n</code></pre>"},{"location":"reference/stimulus/utils/performance/#stimulus.utils.performance.Performance.spearmanr","title":"spearmanr","text":"<pre><code>spearmanr(\n    labels: NDArray[float64], predictions: NDArray[float64]\n) -&gt; float\n</code></pre> <p>Compute Spearman correlation coefficient.</p> Source code in <code>src/stimulus/utils/performance.py</code> <pre><code>def spearmanr(self, labels: NDArray[np.float64], predictions: NDArray[np.float64]) -&gt; float:\n    \"\"\"Compute Spearman correlation coefficient.\"\"\"\n    return float(spearmanr(labels, predictions)[0])\n</code></pre>"},{"location":"reference/stimulus/utils/yaml_data/","title":"stimulus.utils.yaml_data","text":""},{"location":"reference/stimulus/utils/yaml_data/#stimulus.utils.yaml_data","title":"yaml_data","text":"<p>Utility module for handling YAML configuration files and their validation.</p> <p>Classes:</p> <ul> <li> <code>YamlColumns</code>           \u2013            <p>Model for column configuration.</p> </li> <li> <code>YamlColumnsEncoder</code>           \u2013            <p>Model for column encoder configuration.</p> </li> <li> <code>YamlConfigDict</code>           \u2013            <p>Model for main YAML configuration.</p> </li> <li> <code>YamlGlobalParams</code>           \u2013            <p>Model for global parameters in YAML configuration.</p> </li> <li> <code>YamlSchema</code>           \u2013            <p>Model for validating YAML schema.</p> </li> <li> <code>YamlSplit</code>           \u2013            <p>Model for split configuration.</p> </li> <li> <code>YamlSubConfigDict</code>           \u2013            <p>Model for sub-configuration generated from main config.</p> </li> <li> <code>YamlTransform</code>           \u2013            <p>Model for transform configuration.</p> </li> <li> <code>YamlTransformColumns</code>           \u2013            <p>Model for transform columns configuration.</p> </li> <li> <code>YamlTransformColumnsTransformation</code>           \u2013            <p>Model for column transformation configuration.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>check_yaml_schema</code>             \u2013              <p>Validate YAML configuration fields have correct types.</p> </li> <li> <code>dump_yaml_list_into_files</code>             \u2013              <p>Dumps a list of YAML configurations into separate files with custom formatting.</p> </li> <li> <code>expand_transform_list_combinations</code>             \u2013              <p>Expands a list of transforms into all possible parameter combinations.</p> </li> <li> <code>expand_transform_parameter_combinations</code>             \u2013              <p>Get all possible transforms by extracting parameters at each valid index.</p> </li> <li> <code>extract_transform_parameters_at_index</code>             \u2013              <p>Get a transform with parameters at the specified index.</p> </li> <li> <code>generate_data_configs</code>             \u2013              <p>Generates all possible data configurations from a YAML config.</p> </li> </ul>"},{"location":"reference/stimulus/utils/yaml_data/#stimulus.utils.yaml_data.YamlColumns","title":"YamlColumns","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for column configuration.</p>"},{"location":"reference/stimulus/utils/yaml_data/#stimulus.utils.yaml_data.YamlColumnsEncoder","title":"YamlColumnsEncoder","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for column encoder configuration.</p>"},{"location":"reference/stimulus/utils/yaml_data/#stimulus.utils.yaml_data.YamlConfigDict","title":"YamlConfigDict","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for main YAML configuration.</p>"},{"location":"reference/stimulus/utils/yaml_data/#stimulus.utils.yaml_data.YamlGlobalParams","title":"YamlGlobalParams","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for global parameters in YAML configuration.</p>"},{"location":"reference/stimulus/utils/yaml_data/#stimulus.utils.yaml_data.YamlSchema","title":"YamlSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for validating YAML schema.</p>"},{"location":"reference/stimulus/utils/yaml_data/#stimulus.utils.yaml_data.YamlSplit","title":"YamlSplit","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for split configuration.</p>"},{"location":"reference/stimulus/utils/yaml_data/#stimulus.utils.yaml_data.YamlSubConfigDict","title":"YamlSubConfigDict","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for sub-configuration generated from main config.</p>"},{"location":"reference/stimulus/utils/yaml_data/#stimulus.utils.yaml_data.YamlTransform","title":"YamlTransform","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for transform configuration.</p> <p>Methods:</p> <ul> <li> <code>validate_param_lists_across_columns</code>             \u2013              <p>Validate that parameter lists across columns have consistent lengths.</p> </li> </ul>"},{"location":"reference/stimulus/utils/yaml_data/#stimulus.utils.yaml_data.YamlTransform.validate_param_lists_across_columns","title":"validate_param_lists_across_columns  <code>classmethod</code>","text":"<pre><code>validate_param_lists_across_columns(\n    columns: list[YamlTransformColumns],\n) -&gt; list[YamlTransformColumns]\n</code></pre> <p>Validate that parameter lists across columns have consistent lengths.</p> <p>Parameters:</p> <ul> <li> <code>columns</code>               (<code>list[YamlTransformColumns]</code>)           \u2013            <p>List of transform columns to validate</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[YamlTransformColumns]</code>           \u2013            <p>The validated columns list</p> </li> </ul> Source code in <code>src/stimulus/utils/yaml_data.py</code> <pre><code>@field_validator(\"columns\")\n@classmethod\ndef validate_param_lists_across_columns(cls, columns: list[YamlTransformColumns]) -&gt; list[YamlTransformColumns]:\n    \"\"\"Validate that parameter lists across columns have consistent lengths.\n\n    Args:\n        columns: List of transform columns to validate\n\n    Returns:\n        The validated columns list\n    \"\"\"\n    # Get all parameter list lengths across all columns and transformations\n    all_list_lengths: set[int] = set()\n\n    for column in columns:\n        for transformation in column.transformations:\n            if transformation.params and any(\n                isinstance(param_value, list) and len(param_value) &gt; 0\n                for param_value in transformation.params.values()\n            ):\n                all_list_lengths.update(\n                    len(param_value)\n                    for param_value in transformation.params.values()\n                    if isinstance(param_value, list) and len(param_value) &gt; 0\n                )\n\n    # Skip validation if no lists found\n    if not all_list_lengths:\n        return columns\n\n    # Check if all lists either have length 1, or all have the same length\n    all_list_lengths.discard(1)  # Remove length 1 as it's always valid\n    if len(all_list_lengths) &gt; 1:  # Multiple different lengths found\n        raise ValueError(\n            \"All parameter lists across columns must either contain one element or have the same length\",\n        )\n\n    return columns\n</code></pre>"},{"location":"reference/stimulus/utils/yaml_data/#stimulus.utils.yaml_data.YamlTransformColumns","title":"YamlTransformColumns","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for transform columns configuration.</p>"},{"location":"reference/stimulus/utils/yaml_data/#stimulus.utils.yaml_data.YamlTransformColumnsTransformation","title":"YamlTransformColumnsTransformation","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for column transformation configuration.</p>"},{"location":"reference/stimulus/utils/yaml_data/#stimulus.utils.yaml_data.check_yaml_schema","title":"check_yaml_schema","text":"<pre><code>check_yaml_schema(config_yaml: YamlConfigDict) -&gt; str\n</code></pre> <p>Validate YAML configuration fields have correct types.</p> <p>If the children field is specific to a parent, the children fields class is hosted in the parent fields class. If any field in not the right type, the function prints an error message explaining the problem and exits the python code.</p> <p>Parameters:</p> <ul> <li> <code>config_yaml</code>               (<code>YamlConfigDict</code>)           \u2013            <p>The YamlConfigDict containing the fields of the yaml configuration file</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>Empty string if validation succeeds</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If validation fails</p> </li> </ul> Source code in <code>src/stimulus/utils/yaml_data.py</code> <pre><code>def check_yaml_schema(config_yaml: YamlConfigDict) -&gt; str:\n    \"\"\"Validate YAML configuration fields have correct types.\n\n    If the children field is specific to a parent, the children fields class is hosted in the parent fields class.\n    If any field in not the right type, the function prints an error message explaining the problem and exits the python code.\n\n    Args:\n        config_yaml: The YamlConfigDict containing the fields of the yaml configuration file\n\n    Returns:\n        str: Empty string if validation succeeds\n\n    Raises:\n        ValueError: If validation fails\n    \"\"\"\n    try:\n        YamlSchema(yaml_conf=config_yaml)\n    except ValidationError as e:\n        # Use logging instead of print for error handling\n        raise ValueError(\"Wrong type on a field, see the pydantic report above\") from e\n    return \"\"\n</code></pre>"},{"location":"reference/stimulus/utils/yaml_data/#stimulus.utils.yaml_data.dump_yaml_list_into_files","title":"dump_yaml_list_into_files","text":"<pre><code>dump_yaml_list_into_files(\n    yaml_list: list[YamlSubConfigDict],\n    directory_path: str,\n    base_name: str,\n) -&gt; None\n</code></pre> <p>Dumps a list of YAML configurations into separate files with custom formatting.</p> Source code in <code>src/stimulus/utils/yaml_data.py</code> <pre><code>def dump_yaml_list_into_files(\n    yaml_list: list[YamlSubConfigDict],\n    directory_path: str,\n    base_name: str,\n) -&gt; None:\n    \"\"\"Dumps a list of YAML configurations into separate files with custom formatting.\"\"\"\n    # Create a new class attribute rather than assigning to the method\n    # Remove this line since we'll add ignore_aliases to CustomDumper instead\n\n    def represent_none(dumper: yaml.Dumper, _: Any) -&gt; yaml.Node:\n        \"\"\"Custom representer to format None values as empty strings in YAML output.\"\"\"\n        return dumper.represent_scalar(\"tag:yaml.org,2002:null\", \"\")\n\n    def custom_representer(dumper: yaml.Dumper, data: Any) -&gt; yaml.Node:\n        \"\"\"Custom representer to handle different types of lists with appropriate formatting.\"\"\"\n        if isinstance(data, list):\n            if len(data) == 0:\n                return dumper.represent_scalar(\"tag:yaml.org,2002:null\", \"\")\n            if isinstance(data[0], dict):\n                return dumper.represent_sequence(\"tag:yaml.org,2002:seq\", data, flow_style=False)\n            if isinstance(data[0], list):\n                return dumper.represent_sequence(\"tag:yaml.org,2002:seq\", data, flow_style=True)\n        return dumper.represent_sequence(\"tag:yaml.org,2002:seq\", data, flow_style=True)\n\n    class CustomDumper(yaml.Dumper):\n        \"\"\"Custom YAML dumper that adds extra formatting controls.\"\"\"\n\n        def ignore_aliases(self, _data: Any) -&gt; bool:\n            \"\"\"Ignore aliases in the YAML output.\"\"\"\n            return True\n\n        def write_line_break(self, _data: Any = None) -&gt; None:\n            \"\"\"Add extra newline after root-level elements.\"\"\"\n            super().write_line_break(_data)\n            if len(self.indents) &lt;= 1:  # At root level\n                super().write_line_break(_data)\n\n        def increase_indent(self, *, flow: bool = False, indentless: bool = False) -&gt; None:  # type: ignore[override]\n            \"\"\"Ensure consistent indentation by preventing indentless sequences.\"\"\"\n            return super().increase_indent(\n                flow=flow,\n                indentless=indentless,\n            )  # Force indentless to False for better formatting\n\n    # Register the custom representers with our dumper\n    yaml.add_representer(type(None), represent_none, Dumper=CustomDumper)\n    yaml.add_representer(list, custom_representer, Dumper=CustomDumper)\n\n    for i, yaml_dict in enumerate(yaml_list):\n        dict_data = yaml_dict.model_dump(exclude_none=True)\n\n        def fix_params(input_dict: dict[str, Any]) -&gt; dict[str, Any]:\n            \"\"\"Recursively process dictionary to properly handle params fields.\"\"\"\n            if isinstance(input_dict, dict):\n                processed_dict: dict[str, Any] = {}\n                for key, value in input_dict.items():\n                    if key == \"encoder\" and isinstance(value, list):\n                        processed_dict[key] = []\n                        for encoder in value:\n                            processed_encoder = dict(encoder)\n                            if \"params\" not in processed_encoder or not processed_encoder[\"params\"]:\n                                processed_encoder[\"params\"] = {}\n                            processed_dict[key].append(processed_encoder)\n                    elif key == \"transformations\" and isinstance(value, list):\n                        processed_dict[key] = []\n                        for transformation in value:\n                            processed_transformation = dict(transformation)\n                            if \"params\" not in processed_transformation or not processed_transformation[\"params\"]:\n                                processed_transformation[\"params\"] = {}\n                            processed_dict[key].append(processed_transformation)\n                    elif isinstance(value, dict):\n                        processed_dict[key] = fix_params(value)\n                    elif isinstance(value, list):\n                        processed_dict[key] = [\n                            fix_params(list_item) if isinstance(list_item, dict) else list_item for list_item in value\n                        ]\n                    else:\n                        processed_dict[key] = value\n                return processed_dict\n            return input_dict\n\n        dict_data = fix_params(dict_data)\n\n        with open(f\"{directory_path}/{base_name}_{i}.yaml\", \"w\") as f:\n            yaml.dump(\n                dict_data,\n                f,\n                Dumper=CustomDumper,\n                sort_keys=False,\n                default_flow_style=False,\n                indent=2,\n                width=float(\"inf\"),  # Prevent line wrapping\n            )\n</code></pre>"},{"location":"reference/stimulus/utils/yaml_data/#stimulus.utils.yaml_data.expand_transform_list_combinations","title":"expand_transform_list_combinations","text":"<pre><code>expand_transform_list_combinations(\n    transform_list: list[YamlTransform],\n) -&gt; list[YamlTransform]\n</code></pre> <p>Expands a list of transforms into all possible parameter combinations.</p> <p>Takes a list of transforms where each transform may contain parameter lists, and expands them into separate transforms with single parameter values. For example, if a transform has parameters [0.1, 0.2] and [1, 2], this will create two transforms: one with 0.1/1 and another with 0.2/2.</p> <p>Parameters:</p> <ul> <li> <code>transform_list</code>               (<code>list[YamlTransform]</code>)           \u2013            <p>A list of YamlTransform objects containing parameter lists that need to be expanded into individual transforms.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[YamlTransform]</code>           \u2013            <p>list[YamlTransform]: A flattened list of transforms where each transform has single parameter values instead of parameter lists. The length of the returned list will be the sum of the number of parameter combinations for each input transform.</p> </li> </ul> Source code in <code>src/stimulus/utils/yaml_data.py</code> <pre><code>def expand_transform_list_combinations(transform_list: list[YamlTransform]) -&gt; list[YamlTransform]:\n    \"\"\"Expands a list of transforms into all possible parameter combinations.\n\n    Takes a list of transforms where each transform may contain parameter lists,\n    and expands them into separate transforms with single parameter values.\n    For example, if a transform has parameters [0.1, 0.2] and [1, 2], this will\n    create two transforms: one with 0.1/1 and another with 0.2/2.\n\n    Args:\n        transform_list: A list of YamlTransform objects containing parameter lists\n            that need to be expanded into individual transforms.\n\n    Returns:\n        list[YamlTransform]: A flattened list of transforms where each transform\n            has single parameter values instead of parameter lists. The length of\n            the returned list will be the sum of the number of parameter combinations\n            for each input transform.\n    \"\"\"\n    sub_transforms = []\n    for transform in transform_list:\n        sub_transforms.extend(expand_transform_parameter_combinations(transform))\n    return sub_transforms\n</code></pre>"},{"location":"reference/stimulus/utils/yaml_data/#stimulus.utils.yaml_data.expand_transform_parameter_combinations","title":"expand_transform_parameter_combinations","text":"<pre><code>expand_transform_parameter_combinations(\n    transform: YamlTransform,\n) -&gt; list[YamlTransform]\n</code></pre> <p>Get all possible transforms by extracting parameters at each valid index.</p> <p>For a transform with parameter lists, creates multiple new transforms, each containing single parameter values from the corresponding indices of the parameter lists.</p> <p>Parameters:</p> <ul> <li> <code>transform</code>               (<code>YamlTransform</code>)           \u2013            <p>The original transform containing parameter lists</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[YamlTransform]</code>           \u2013            <p>A list of transforms, each with single parameter values from sequential indices</p> </li> </ul> Source code in <code>src/stimulus/utils/yaml_data.py</code> <pre><code>def expand_transform_parameter_combinations(transform: YamlTransform) -&gt; list[YamlTransform]:\n    \"\"\"Get all possible transforms by extracting parameters at each valid index.\n\n    For a transform with parameter lists, creates multiple new transforms, each containing\n    single parameter values from the corresponding indices of the parameter lists.\n\n    Args:\n        transform: The original transform containing parameter lists\n\n    Returns:\n        A list of transforms, each with single parameter values from sequential indices\n    \"\"\"\n    # Find the length of parameter lists - we only need to check the first list we find\n    # since all lists must have the same length (enforced by pydantic validator)\n    max_length = 1\n    for column in transform.columns:\n        for transformation in column.transformations:\n            if transformation.params:\n                list_lengths = [len(v) for v in transformation.params.values() if isinstance(v, list) and len(v) &gt; 1]\n                if list_lengths:\n                    max_length = list_lengths[0]  # All lists have same length due to validator\n                    break\n\n    # Generate a transform for each index\n    transforms = []\n    for i in range(max_length):\n        transforms.append(extract_transform_parameters_at_index(transform, i))\n\n    return transforms\n</code></pre>"},{"location":"reference/stimulus/utils/yaml_data/#stimulus.utils.yaml_data.extract_transform_parameters_at_index","title":"extract_transform_parameters_at_index","text":"<pre><code>extract_transform_parameters_at_index(\n    transform: YamlTransform, index: int = 0\n) -&gt; YamlTransform\n</code></pre> <p>Get a transform with parameters at the specified index.</p> <p>Parameters:</p> <ul> <li> <code>transform</code>               (<code>YamlTransform</code>)           \u2013            <p>The original transform containing parameter lists</p> </li> <li> <code>index</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Index to extract parameters from (default 0)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>YamlTransform</code>           \u2013            <p>A new transform with single parameter values at the specified index</p> </li> </ul> Source code in <code>src/stimulus/utils/yaml_data.py</code> <pre><code>def extract_transform_parameters_at_index(transform: YamlTransform, index: int = 0) -&gt; YamlTransform:\n    \"\"\"Get a transform with parameters at the specified index.\n\n    Args:\n        transform: The original transform containing parameter lists\n        index: Index to extract parameters from (default 0)\n\n    Returns:\n        A new transform with single parameter values at the specified index\n    \"\"\"\n    # Create a copy of the transform\n    new_transform = YamlTransform(**transform.model_dump())\n\n    # Process each column and transformation\n    for column in new_transform.columns:\n        for transformation in column.transformations:\n            if transformation.params:\n                # Convert each parameter list to single value at index\n                new_params = {}\n                for param_name, param_value in transformation.params.items():\n                    if isinstance(param_value, list):\n                        new_params[param_name] = param_value[index]\n                    else:\n                        new_params[param_name] = param_value\n                transformation.params = new_params\n\n    return new_transform\n</code></pre>"},{"location":"reference/stimulus/utils/yaml_data/#stimulus.utils.yaml_data.generate_data_configs","title":"generate_data_configs","text":"<pre><code>generate_data_configs(\n    yaml_config: YamlConfigDict,\n) -&gt; list[YamlSubConfigDict]\n</code></pre> <p>Generates all possible data configurations from a YAML config.</p> <p>Takes a YAML configuration that may contain parameter lists and splits, and generates all possible combinations of parameters and splits into separate data configurations.</p> <p>For example, if the config has: - A transform with parameters [0.1, 0.2] - Two splits [0.7/0.3] and [0.8/0.2] This will generate 4 configs, 2 for each split.</p> <p>Parameters:</p> <ul> <li> <code>yaml_config</code>               (<code>YamlConfigDict</code>)           \u2013            <p>The source YAML configuration containing transforms with parameter lists and multiple splits.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[YamlSubConfigDict]</code>           \u2013            <p>list[YamlSubConfigDict]: A list of data configurations, where each config has single parameter values and one split configuration. The length will be the product of the number of parameter combinations and the number of splits.</p> </li> </ul> Source code in <code>src/stimulus/utils/yaml_data.py</code> <pre><code>def generate_data_configs(yaml_config: YamlConfigDict) -&gt; list[YamlSubConfigDict]:\n    \"\"\"Generates all possible data configurations from a YAML config.\n\n    Takes a YAML configuration that may contain parameter lists and splits,\n    and generates all possible combinations of parameters and splits into\n    separate data configurations.\n\n    For example, if the config has:\n    - A transform with parameters [0.1, 0.2]\n    - Two splits [0.7/0.3] and [0.8/0.2]\n    This will generate 4 configs, 2 for each split.\n\n    Args:\n        yaml_config: The source YAML configuration containing transforms with\n            parameter lists and multiple splits.\n\n    Returns:\n        list[YamlSubConfigDict]: A list of data configurations, where each\n            config has single parameter values and one split configuration. The\n            length will be the product of the number of parameter combinations\n            and the number of splits.\n    \"\"\"\n    if isinstance(yaml_config, dict) and not isinstance(yaml_config, YamlConfigDict):\n        raise TypeError(\"Input must be a YamlConfigDict object\")\n\n    sub_transforms = expand_transform_list_combinations(yaml_config.transforms)\n    sub_splits = yaml_config.split\n    sub_configs = []\n    for split in sub_splits:\n        for transform in sub_transforms:\n            sub_configs.append(\n                YamlSubConfigDict(\n                    global_params=yaml_config.global_params,\n                    columns=yaml_config.columns,\n                    transforms=transform,\n                    split=split,\n                ),\n            )\n    return sub_configs\n</code></pre>"},{"location":"reference/stimulus/utils/yaml_model_schema/","title":"stimulus.utils.yaml_model_schema","text":""},{"location":"reference/stimulus/utils/yaml_model_schema/#stimulus.utils.yaml_model_schema","title":"yaml_model_schema","text":"<p>Module for handling YAML configuration files and converting them to Ray Tune format.</p> <p>Classes:</p> <ul> <li> <code>CustomTunableParameter</code>           \u2013            <p>Custom tunable parameter.</p> </li> <li> <code>Data</code>           \u2013            <p>Data parameters.</p> </li> <li> <code>Loss</code>           \u2013            <p>Loss parameters.</p> </li> <li> <code>Model</code>           \u2013            <p>Model configuration.</p> </li> <li> <code>RayTuneModel</code>           \u2013            <p>Ray Tune compatible model configuration.</p> </li> <li> <code>RunParams</code>           \u2013            <p>Run parameters.</p> </li> <li> <code>Scheduler</code>           \u2013            <p>Scheduler parameters.</p> </li> <li> <code>TunableParameter</code>           \u2013            <p>Tunable parameter.</p> </li> <li> <code>Tune</code>           \u2013            <p>Tune parameters.</p> </li> <li> <code>TuneParams</code>           \u2013            <p>Tune parameters.</p> </li> <li> <code>YamlRayConfigLoader</code>           \u2013            <p>Load and convert YAML configurations to Ray Tune format.</p> </li> </ul>"},{"location":"reference/stimulus/utils/yaml_model_schema/#stimulus.utils.yaml_model_schema.CustomTunableParameter","title":"CustomTunableParameter","text":"<p>               Bases: <code>BaseModel</code></p> <p>Custom tunable parameter.</p>"},{"location":"reference/stimulus/utils/yaml_model_schema/#stimulus.utils.yaml_model_schema.Data","title":"Data","text":"<p>               Bases: <code>BaseModel</code></p> <p>Data parameters.</p>"},{"location":"reference/stimulus/utils/yaml_model_schema/#stimulus.utils.yaml_model_schema.Loss","title":"Loss","text":"<p>               Bases: <code>BaseModel</code></p> <p>Loss parameters.</p>"},{"location":"reference/stimulus/utils/yaml_model_schema/#stimulus.utils.yaml_model_schema.Model","title":"Model","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model configuration.</p>"},{"location":"reference/stimulus/utils/yaml_model_schema/#stimulus.utils.yaml_model_schema.RayTuneModel","title":"RayTuneModel","text":"<p>               Bases: <code>BaseModel</code></p> <p>Ray Tune compatible model configuration.</p>"},{"location":"reference/stimulus/utils/yaml_model_schema/#stimulus.utils.yaml_model_schema.RunParams","title":"RunParams","text":"<p>               Bases: <code>BaseModel</code></p> <p>Run parameters.</p>"},{"location":"reference/stimulus/utils/yaml_model_schema/#stimulus.utils.yaml_model_schema.Scheduler","title":"Scheduler","text":"<p>               Bases: <code>BaseModel</code></p> <p>Scheduler parameters.</p>"},{"location":"reference/stimulus/utils/yaml_model_schema/#stimulus.utils.yaml_model_schema.TunableParameter","title":"TunableParameter","text":"<p>               Bases: <code>BaseModel</code></p> <p>Tunable parameter.</p> <p>Methods:</p> <ul> <li> <code>validate_mode</code>             \u2013              <p>Validate that mode is supported by Ray Tune.</p> </li> </ul>"},{"location":"reference/stimulus/utils/yaml_model_schema/#stimulus.utils.yaml_model_schema.TunableParameter.validate_mode","title":"validate_mode","text":"<pre><code>validate_mode() -&gt; TunableParameter\n</code></pre> <p>Validate that mode is supported by Ray Tune.</p> Source code in <code>src/stimulus/utils/yaml_model_schema.py</code> <pre><code>@pydantic.model_validator(mode=\"after\")\ndef validate_mode(self) -&gt; \"TunableParameter\":\n    \"\"\"Validate that mode is supported by Ray Tune.\"\"\"\n    if not hasattr(tune, self.mode):\n        raise AttributeError(\n            f\"Mode {self.mode} not recognized, check the ray.tune documentation at https://docs.ray.io/en/master/tune/api_docs/suggestion.html\",\n        )\n\n    mode = getattr(tune, self.mode)\n    if mode.__name__ not in [\n        \"choice\",\n        \"uniform\",\n        \"loguniform\",\n        \"quniform\",\n        \"qloguniform\",\n        \"qnormal\",\n        \"randint\",\n        \"sample_from\",\n    ]:\n        raise NotImplementedError(f\"Mode {mode.__name__} not implemented yet\")\n\n    return self\n</code></pre>"},{"location":"reference/stimulus/utils/yaml_model_schema/#stimulus.utils.yaml_model_schema.Tune","title":"Tune","text":"<p>               Bases: <code>BaseModel</code></p> <p>Tune parameters.</p>"},{"location":"reference/stimulus/utils/yaml_model_schema/#stimulus.utils.yaml_model_schema.TuneParams","title":"TuneParams","text":"<p>               Bases: <code>BaseModel</code></p> <p>Tune parameters.</p>"},{"location":"reference/stimulus/utils/yaml_model_schema/#stimulus.utils.yaml_model_schema.YamlRayConfigLoader","title":"YamlRayConfigLoader","text":"<pre><code>YamlRayConfigLoader(model: Model)\n</code></pre> <p>Load and convert YAML configurations to Ray Tune format.</p> <p>This class handles loading model configurations and converting them into formats compatible with Ray Tune's hyperparameter search spaces.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Model</code>)           \u2013            <p>Pydantic Model instance containing configuration</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>convert_config_to_ray</code>             \u2013              <p>Convert Model configuration to Ray Tune format.</p> </li> <li> <code>convert_raytune</code>             \u2013              <p>Convert parameter configuration to Ray Tune format.</p> </li> <li> <code>get_config</code>             \u2013              <p>Return the current configuration.</p> </li> <li> <code>raytune_sample_from</code>             \u2013              <p>Apply tune.sample_from to a given custom sampling function.</p> </li> <li> <code>raytune_space_selector</code>             \u2013              <p>Convert space parameters to Ray Tune format based on the mode.</p> </li> <li> <code>sampint</code>             \u2013              <p>Return a list of n random samples from the sample_space.</p> </li> </ul> Source code in <code>src/stimulus/utils/yaml_model_schema.py</code> <pre><code>def __init__(self, model: Model) -&gt; None:\n    \"\"\"Initialize the config loader with a Model instance.\n\n    Args:\n        model: Pydantic Model instance containing configuration\n    \"\"\"\n    self.model = model\n    self.ray_model = self.convert_config_to_ray(model)\n</code></pre>"},{"location":"reference/stimulus/utils/yaml_model_schema/#stimulus.utils.yaml_model_schema.YamlRayConfigLoader.convert_config_to_ray","title":"convert_config_to_ray","text":"<pre><code>convert_config_to_ray(model: Model) -&gt; RayTuneModel\n</code></pre> <p>Convert Model configuration to Ray Tune format.</p> <p>Converts parameters in network_params and optimizer_params to Ray Tune search spaces.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Model</code>)           \u2013            <p>Model configuration</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RayTuneModel</code>           \u2013            <p>Ray Tune compatible model configuration</p> </li> </ul> Source code in <code>src/stimulus/utils/yaml_model_schema.py</code> <pre><code>def convert_config_to_ray(self, model: Model) -&gt; RayTuneModel:\n    \"\"\"Convert Model configuration to Ray Tune format.\n\n    Converts parameters in network_params and optimizer_params to Ray Tune search spaces.\n\n    Args:\n        model: Model configuration\n\n    Returns:\n        Ray Tune compatible model configuration\n    \"\"\"\n    return RayTuneModel(\n        network_params={k: self.convert_raytune(v) for k, v in model.network_params.items()},\n        optimizer_params={k: self.convert_raytune(v) for k, v in model.optimizer_params.items()},\n        loss_params={k: self.convert_raytune(v) for k, v in model.loss_params},\n        data_params={k: self.convert_raytune(v) for k, v in model.data_params},\n        tune=model.tune,\n    )\n</code></pre>"},{"location":"reference/stimulus/utils/yaml_model_schema/#stimulus.utils.yaml_model_schema.YamlRayConfigLoader.convert_raytune","title":"convert_raytune","text":"<pre><code>convert_raytune(\n    param: TunableParameter | CustomTunableParameter,\n) -&gt; Any\n</code></pre> <p>Convert parameter configuration to Ray Tune format.</p> <p>Parameters:</p> <ul> <li> <code>param</code>               (<code>TunableParameter | CustomTunableParameter</code>)           \u2013            <p>Parameter configuration</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>Ray Tune compatible parameter configuration</p> </li> </ul> Source code in <code>src/stimulus/utils/yaml_model_schema.py</code> <pre><code>def convert_raytune(self, param: TunableParameter | CustomTunableParameter) -&gt; Any:\n    \"\"\"Convert parameter configuration to Ray Tune format.\n\n    Args:\n        param: Parameter configuration\n\n    Returns:\n        Ray Tune compatible parameter configuration\n    \"\"\"\n    mode = getattr(tune, param.mode)\n\n    if isinstance(param, TunableParameter):\n        return self.raytune_space_selector(mode, param.space)\n    return self.raytune_sample_from(mode, param)\n</code></pre>"},{"location":"reference/stimulus/utils/yaml_model_schema/#stimulus.utils.yaml_model_schema.YamlRayConfigLoader.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; dict\n</code></pre> <p>Return the current configuration.</p> <p>Returns:</p> <ul> <li> <code>dict</code>           \u2013            <p>Current configuration dictionary</p> </li> </ul> Source code in <code>src/stimulus/utils/yaml_model_schema.py</code> <pre><code>def get_config(self) -&gt; dict:\n    \"\"\"Return the current configuration.\n\n    Returns:\n        Current configuration dictionary\n    \"\"\"\n    return self.ray_model.model_dump()\n</code></pre>"},{"location":"reference/stimulus/utils/yaml_model_schema/#stimulus.utils.yaml_model_schema.YamlRayConfigLoader.raytune_sample_from","title":"raytune_sample_from","text":"<pre><code>raytune_sample_from(\n    mode: Callable, param: CustomTunableParameter\n) -&gt; Any\n</code></pre> <p>Apply tune.sample_from to a given custom sampling function.</p> <p>Parameters:</p> <ul> <li> <code>mode</code>               (<code>Callable</code>)           \u2013            <p>Ray Tune sampling function</p> </li> <li> <code>param</code>               (<code>CustomTunableParameter</code>)           \u2013            <p>TunableParameter containing sampling parameters</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>Configured sampling function</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>If the sampling function is not supported</p> </li> </ul> Source code in <code>src/stimulus/utils/yaml_model_schema.py</code> <pre><code>def raytune_sample_from(self, mode: Callable, param: CustomTunableParameter) -&gt; Any:\n    \"\"\"Apply tune.sample_from to a given custom sampling function.\n\n    Args:\n        mode: Ray Tune sampling function\n        param: TunableParameter containing sampling parameters\n\n    Returns:\n        Configured sampling function\n\n    Raises:\n        NotImplementedError: If the sampling function is not supported\n    \"\"\"\n    if param.function == \"sampint\":\n        return mode(lambda _: self.sampint(param.sample_space, param.n_space))\n\n    raise NotImplementedError(f\"Function {param.function} not implemented yet\")\n</code></pre>"},{"location":"reference/stimulus/utils/yaml_model_schema/#stimulus.utils.yaml_model_schema.YamlRayConfigLoader.raytune_space_selector","title":"raytune_space_selector","text":"<pre><code>raytune_space_selector(mode: Callable, space: list) -&gt; Any\n</code></pre> <p>Convert space parameters to Ray Tune format based on the mode.</p> <p>Parameters:</p> <ul> <li> <code>mode</code>               (<code>Callable</code>)           \u2013            <p>Ray Tune search space function (e.g., tune.choice, tune.uniform)</p> </li> <li> <code>space</code>               (<code>list</code>)           \u2013            <p>List of parameters defining the search space</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>Configured Ray Tune search space</p> </li> </ul> Source code in <code>src/stimulus/utils/yaml_model_schema.py</code> <pre><code>def raytune_space_selector(self, mode: Callable, space: list) -&gt; Any:\n    \"\"\"Convert space parameters to Ray Tune format based on the mode.\n\n    Args:\n        mode: Ray Tune search space function (e.g., tune.choice, tune.uniform)\n        space: List of parameters defining the search space\n\n    Returns:\n        Configured Ray Tune search space\n    \"\"\"\n    if mode.__name__ == \"choice\":\n        return mode(space)\n\n    return mode(*tuple(space))\n</code></pre>"},{"location":"reference/stimulus/utils/yaml_model_schema/#stimulus.utils.yaml_model_schema.YamlRayConfigLoader.sampint","title":"sampint  <code>staticmethod</code>","text":"<pre><code>sampint(sample_space: list, n_space: list) -&gt; list[int]\n</code></pre> <p>Return a list of n random samples from the sample_space.</p> <p>This function is useful for sampling different numbers of layers, each with different numbers of neurons.</p> <p>Parameters:</p> <ul> <li> <code>sample_space</code>               (<code>list</code>)           \u2013            <p>List [min, max] defining range of values to sample from</p> </li> <li> <code>n_space</code>               (<code>list</code>)           \u2013            <p>List [min, max] defining range for number of samples</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[int]</code>           \u2013            <p>List of randomly sampled integers</p> </li> </ul> Note <p>Uses Python's random module which is not cryptographically secure. This is acceptable for hyperparameter sampling but should not be used for security-critical purposes (S311 fails when linting).</p> Source code in <code>src/stimulus/utils/yaml_model_schema.py</code> <pre><code>@staticmethod\ndef sampint(sample_space: list, n_space: list) -&gt; list[int]:\n    \"\"\"Return a list of n random samples from the sample_space.\n\n    This function is useful for sampling different numbers of layers,\n    each with different numbers of neurons.\n\n    Args:\n        sample_space: List [min, max] defining range of values to sample from\n        n_space: List [min, max] defining range for number of samples\n\n    Returns:\n        List of randomly sampled integers\n\n    Note:\n        Uses Python's random module which is not cryptographically secure.\n        This is acceptable for hyperparameter sampling but should not be\n        used for security-critical purposes (S311 fails when linting).\n    \"\"\"\n    sample_space_list = list(range(sample_space[0], sample_space[1] + 1))\n    n_space_list = list(range(n_space[0], n_space[1] + 1))\n    n = random.choice(n_space_list)  # noqa: S311\n    return random.sample(sample_space_list, n)\n</code></pre>"},{"location":"coverage/","title":"Coverage report","text":""}]}